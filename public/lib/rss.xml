<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Sarang's DevOps Notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://notes.sarangwandile.xyz/</link><image><url>https://notes.sarangwandile.xyz/lib/media/favicon.png</url><title>Sarang's DevOps Notes</title><link>https://notes.sarangwandile.xyz/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 30 Dec 2024 16:01:53 GMT</lastBuildDate><atom:link href="https://notes.sarangwandile.xyz/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 30 Dec 2024 16:00:46 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Authentication vs. Authorization]]></title><description><![CDATA[ 
 <br><br>While often used interchangeably,&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate" target="_blank">authentication</a>&nbsp;and authorization represent fundamentally different functions. In this article, we compare and contrast the two to show how they protect applications in complementary ways.<br><br>In simple terms, authentication is the process of verifying who a user is, while authorization is the process of verifying what they have access to.<br>Comparing these processes to a real-world example, when you go through security in an airport, you show your ID to authenticate your identity. Then, when you arrive at the gate, you present your boarding pass to the flight attendant, so they can authorize you to board your flight and allow access to the plane.<br><br>Here's a quick overview of the differences between authentication and authorization:<br><br>In short, access to a resource is protected by both authentication and authorization. If you can't prove your identity, you won't be allowed into a resource. And even if you can prove your identity, if you are not authorized for that resource, you will still be denied access.<br>Auth0 has products and services for authentication, like&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate/passwordless/passwordless-with-universal-login" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate/passwordless/passwordless-with-universal-login" target="_blank">passwordless</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/secure/multi-factor-authentication" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/secure/multi-factor-authentication" target="_blank">multi-factor authentication</a>&nbsp;(MFA), and&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate/single-sign-on" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate/single-sign-on" target="_blank">Single-Sign On (SSO)</a>&nbsp;you can configure using&nbsp;Auth0 Dashboard&nbsp;or&nbsp;Management API. For authorization, Auth0 offers&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/manage-users/access-control/rbac" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/manage-users/access-control/rbac" target="_blank">role-based access control</a>&nbsp;(RBAC) or&nbsp;<a data-tooltip-position="top" aria-label="https://docs.fga.dev/fga" rel="noopener nofollow" class="external-link" href="https://docs.fga.dev/fga" target="_blank">fine grained authorization</a>&nbsp;FGA).]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/authorized-vs-authonticated.html</link><guid isPermaLink="false">AWS/For Reading/Authorized vs Authonticated.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[AWS EC2 Instance Types Grouped by Family]]></title><description><![CDATA[ 
 <br><br><br>
<br>M1 (e.g., m1.small, m1.medium, m1.large)

<br>Balanced CPU, memory, and network resources
<br>Suitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications


<br>M3 (e.g., m3.large, m3.xlarge, m3.2xlarge)

<br>Higher CPU performance and larger instance sizes
<br>Recommended for general-purpose instances with demanding CPU requirements


<br>M6g (e.g., m6g.medium, m6g.large, m6g.xlarge, m6g.2xlarge)

<br>Latest generation of general-purpose instances
<br>Provides a balance of CPU, memory, and network resources
<br>Suitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications


<br><br>
<br>C1 (e.g., c1.xlarge, c1.2xlarge, c1.4xlarge)

<br>High CPU-to-memory ratio
<br>Ideal for CPU-bound scale-out applications, such as front-end fleets for high-traffic web sites, on-demand batch processing, distributed analytics, web servers, video encoding, and high-performance science and engineering applications


<br>CC2 (e.g., cc2.8xlarge)

<br>Latest generation of compute-optimized instances
<br>Provides the lowest cost for CPU performance among all EC2 instance types
<br>Supports cluster networking and high core count (32 vCPUs)


<br><br>
<br>M2 (e.g., m2.xlarge, m2.2xlarge, m2.4xlarge)

<br>High memory-to-CPU ratio
<br>Suitable for memory-bound applications, such as high-performance databases, distributed cache, genome assembly, and larger deployments of SAP, Microsoft SharePoint, and other enterprise applications


<br>CR1 (e.g., cr1.16xlarge)

<br>Latest generation of memory-optimized instances
<br>Provides more memory (244 GiB), faster CPU (Intel Xeon E5-2670), and supports cluster networking


<br><br>
<br>HI1 (e.g., hi1.4xlarge)

<br>Optimized for very high random I/O performance and low cost per IOPS
<br>Ideal for transactional applications, such as large deployments of NoSQL databases like Cassandra and MongoDB


<br>HS1 (e.g., hs1.8xlarge)

<br>Optimized for very high storage density, low storage cost, and high sequential I/O performance
<br>Suitable for large-scale data warehouses, large always-on Hadoop clusters, and cluster file systems


<br><br>
<br>CG1 (e.g., cg1.4xlarge)

<br>High-performance computing instances with NVIDIA Tesla V100 GPUs
<br>Suitable for applications that require intense graphics processing, such as video encoding, scientific simulations, and machine learning


<br><br>
<br>T1 (e.g., t1.micro)

<br>Very low-cost instance option with a small amount of CPU resources
<br>May opportunistically increase CPU capacity in short bursts when additional cycles are available
<br>Suitable for lower-throughput applications, such as bastion hosts or administrative applications, or for low-traffic websites that require additional compute cycles from time to time


<br><br>AWS instance types support high-throughput workloads like video transcoding and scientific simulations&nbsp;through various combinations of CPU, memory, storage, and networking capacity. Here are some key features and instance types that cater to these workloads:<br>
<br>Compute-Optimized Instances: M5zn, C6g, C6gn, and Hpc6id instances are designed for applications that require extremely high single-thread performance and high throughput, such as:

<br>Gaming
<br>High Performance Computing (HPC)
<br>Simulation modeling for industries like automotive, aerospace, energy, and telecommunications


<br>Storage-Optimized Instances: I3en, H1, and I2 instances are ideal for big data processing clusters, MapReduce workloads, and distributed file systems, making them suitable for:

<br>Batch processing workloads
<br>Media transcoding
<br>High-performance web servers
<br>Scientific modeling


<br>High-Performance Computing (HPC) Instances: Hpc6a and Hpc6id instances are designed for tight-coupled, compute-intensive, and high-performance computing jobs, including:

<br>Computational fluid dynamics
<br>Weather forecasting
<br>Molecular dynamics
<br>Complex simulations and deep learning workloads


<br>GPU-Powered Instances: Vt1 instances, powered by NVIDIA V100 or T4 GPUs, are optimized for:

<br>Live transcoding video at up to 4K UHD resolutions
<br>Machine learning inference
<br>Graphics rendering
<br>Scientific simulations


<br>Network-Optimized Instances: R instances, with Enhanced Networking and EBS optimization, are suitable for workloads that require high networking throughput, such as:

<br>High-performance file systems
<br>Distributed web scale in-memory caches
<br>Real-time big data analytics
<br>Telco applications like 5G User Plane Function (UPF)


<br>AWS instance types offer a range of configurations to support high-throughput workloads, allowing you to choose the optimal instance for your specific needs. By leveraging these instance types, you can efficiently run demanding workloads like video transcoding and scientific simulations in the cloud.<br><br>When you launch an instance, the&nbsp;instance type&nbsp;that you specify determines the hardware of the host computer used for your instance. Each instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities. Select an instance type based on the requirements of the application or software that you plan to run on your instance.&nbsp;For more information about features and use cases, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance Types Details</a>.<br>Amazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such as the network and the disk subsystem, among instances. If each instance on a host computer tries to use as much of one of these shared resources as possible, each receives an equal share of that resource. However, when a resource is underused, an instance can consume a higher share of that resource while it's available.<br>Each instance type provides higher or lower minimum performance from a shared resource. For example, instance types with high I/O performance have a larger allocation of shared resources. Allocating a larger share of shared resources also reduces the variance of I/O performance. For most applications, moderate I/O performance is more than enough. However, for applications that require greater or more consistent I/O performance, consider an instance type with higher I/O performance.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#AvailableInstanceTypes" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#AvailableInstanceTypes" target="_blank">Available instance types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-specs" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-specs" target="_blank">Hardware specifications</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hypervisor-type" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hypervisor-type" target="_blank">Hypervisor type</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-virtualization-type" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-virtualization-type" target="_blank">AMI virtualization types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-types-processors" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-types-processors" target="_blank">Processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" target="_blank">Find an Amazon EC2 instance type</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-ec2-instance-type-recommendations.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-ec2-instance-type-recommendations.html" target="_blank">Get recommendations from EC2 instance type finder</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recommendations.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recommendations.html" target="_blank">Get EC2 instance recommendations from Compute Optimizer</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" target="_blank">Amazon EC2 instance type changes</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html" target="_blank">Burstable performance instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configure-gpu-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configure-gpu-instances.html" target="_blank">Performance acceleration with GPU instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html" target="_blank">Amazon EC2 Mac instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html" target="_blank">Amazon EBS-optimized instance types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html" target="_blank">CPU options for Amazon EC2 instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" target="_blank">AMD SEV-SNP for Amazon EC2 instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/processor_state_control.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/processor_state_control.html" target="_blank">Processor state control for Amazon EC2 Linux instances</a>
<br><br>Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.<br><br>Names are based on instance family, generation, processor family, capabilities, and size. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html" target="_blank">Naming conventions</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>To determine which instance types meet your requirements, such as supported Regions, compute resources, or storage resources, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" target="_blank">Find an Amazon EC2 instance type</a>&nbsp;and&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" target="_blank">Amazon EC2 instance type specifications</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>For detailed instance type specifications, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" target="_blank">Specifications</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide. For pricing information, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/pricing/on-demand/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/pricing/on-demand/" target="_blank">Amazon EC2 On-Demand Pricing</a>.<br>To determine which instance type best meets your needs, we recommend that you launch an instance and use your own benchmark application. Because you pay by the instance second, it's convenient and inexpensive to test multiple instance types before making a decision. If your needs change, even after you make a decision, you can change the instance type later. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" target="_blank">Amazon EC2 instance type changes</a>.<br><br>Amazon EC2 supports the following hypervisors: Xen and Nitro.<br><br>
<br>General purpose:&nbsp;M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8g | T3 | T3a | T4g<br>

<br>Compute optimized:&nbsp;C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g<br>

<br>Memory optimized:&nbsp;R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6idn | R6in | R6id | R7a | R7g | R7gd | R7i | R7iz | R8g | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | X2gd | X2idn | X2iedn | X2iezn | X8g | z1d<br>

<br>Storage optimized:&nbsp;D3 | D3en | I3en | I4g | I4i | Im4gn | Is4gen<br>

<br>Accelerated computing:&nbsp;DL1 | DL2q | G4ad | G4dn | G5 | G5g | G6 | G6e | Gr6 | Inf1 | Inf2 | P3dn | P4d | P4de | P5 | P5e | Trn1 | Trn1n | VT1<br>

<br>High-performance computing:&nbsp;Hpc6a | Hpc6id | Hpc7a | Hpc7g<br>

<br>Previous generation:&nbsp;A1<br>

<br>For more information about the supported versions of Nitro hypervisor, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html#nitro-version-network-features" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html#nitro-version-network-features" target="_blank">Network feature support</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>
<br>General purpose: M1 | M2 | M3 | M4 | T1 | T2<br>

<br>Compute optimized: C1 | C3 | C4<br>

<br>Memory optimized: R3 | R4 | X1 | X1e<br>

<br>Storage optimized: D2 | H1 | I2 | I3<br>

<br>Accelerated computing: F1 | G3 | P2 | P3<br>

<br><br>The virtualization type of your instance is determined by the AMI that you use to launch it. Current generation instance types support hardware virtual machine (HVM) only. Some previous generation instance types support paravirtual (PV) and some AWS Regions support PV instances. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#virtualization_types" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#virtualization_types" target="_blank">Virtualization types</a>.<br>For best performance, we recommend that you use an HVM AMI. In addition, HVM AMIs are required to take advantage of enhanced networking. HVM virtualization uses hardware-assist technology provided by the AWS platform. With HVM virtualization, the guest VM runs as if it were on a native hardware platform, except that it still uses PV network and storage drivers for improved performance.<br><br>EC2 instances support a variety of processors.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-processors" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-processors" target="_blank">Intel processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#amd-epyc-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#amd-epyc-instances" target="_blank">AMD processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" target="_blank">AWS Graviton processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" target="_blank">AWS Trainium</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-inferentia-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-inferentia-instances" target="_blank">AWS Inferentia</a>
<br><br>Amazon EC2 instances that run on Intel processors might include the following processor features. Not all instances that run on Intel processors support all of these processor features. For information about which features are available for each instance type, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance types</a>.<br>
<br>Intel AES New Instructions (AES-NI)&nbsp;— Intel AES-NI encryption instruction set improves upon the original Advanced Encryption Standard (AES) algorithm to provide faster data protection and greater security. All current generation EC2 instances support this processor feature.<br>

<br>Intel Advanced Vector Extensions (Intel AVX, Intel AVX2, and Intel AVX-512)&nbsp;— Intel AVX and Intel AVX2 are 256-bit, and Intel AVX-512 is a 512-bit instruction set extension designed for applications that are Floating Point (FP) intensive. Intel AVX instructions improve performance for applications like image and audio/video processing, scientific simulations, financial analytics, and 3D modeling and analysis. These features are only available on instances launched with HVM AMIs.<br>

<br>Intel Turbo Boost Technology&nbsp;— Intel Turbo Boost Technology processors automatically run cores faster than the base operating frequency.<br>

<br>Intel Deep Learning Boost (Intel DL Boost)&nbsp;— Accelerates AI deep learning use cases. The 2nd Gen Intel Xeon Scalable processors extend Intel AVX-512 with a new Vector Neural Network Instruction (VNNI/INT8) that significantly increases deep learning inference performance over previous generation Intel Xeon Scalable processors (with FP32) for image recognition/segmentation, object detection, speech recognition, language translation, recommendation systems, reinforcement learning, and more. VNNI may not be compatible with all Linux distributions.
  The following instances support VNNI:&nbsp;M5n,&nbsp;R5n,&nbsp;M5dn,&nbsp;M5zn,&nbsp;R5b,&nbsp;R5dn,&nbsp;D3,&nbsp;D3en, and&nbsp;C6i.&nbsp;C5&nbsp;and&nbsp;C5d&nbsp;instances support VNNI for only&nbsp;12xlarge,&nbsp;24xlarge, and&nbsp;metal&nbsp;instances.<br>

<br>Confusion can result from industry naming conventions for 64-bit CPUs. Chip manufacturer Advanced Micro Devices (AMD) introduced the first commercially successful 64-bit architecture based on the Intel x86 instruction set. Consequently, the architecture is widely referred to as AMD64 regardless of the chip manufacturer. Windows and several Linux distributions follow this practice. This explains why the internal system information on an instance running Ubuntu or Windows displays the CPU architecture as AMD64 even though the instances are running on Intel hardware.<br><br>Amazon EC2 instances that run on&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/amd/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/amd/" target="_blank">AMD EPYC</a>&nbsp;processors can help you optimize both cost and performance for your workloads. These instances might support the following processor features. Not all instances that run on AMD processors support all of these processor features. For information about which features are available for each instance type, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance types</a>.<br>
<br>AMD Secure Memory Encryption (SME)<br>

<br>AMD Transparent Single Key Memory Encryption (TSME)<br>

<br>AMD Advanced Vector Extensions (AVX)<br>

<br>AMD Secure Encrypted Virtualization-Secure Nested Paging (<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" target="_blank">SEV-SNP</a>)<br>

<br>Vector Neural Network Instructions (VNNI)<br>

<br>BFloat16<br>

<br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/graviton/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/graviton/" target="_blank">AWS Graviton</a>&nbsp;is a family of processors designed to deliver the best price performance for your workloads running on Amazon EC2 instances.<br>For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/graviton/getting-started" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/graviton/getting-started" target="_blank">Getting started with Graviton</a>.<br><br>Instances powered by&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/machine-learning/trainium/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/machine-learning/trainium/" target="_blank">AWS Trainium</a>&nbsp;are purpose built for high-performance, cost-effective deep learning training. You can use these instances to train natural language processing, computer vision, and recommender models used across a broad set of applications, such as speech recognition, recommendation, fraud detection, and image and video classification. Use your existing workflows in popular ML frameworks, such as PyTorch and TensorFlow.<br><br>Instances powered by&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/machine-learning/inferentia/" target="_blank">AWS Inferentia</a>&nbsp;are designed to accelerate machine learning. They provide high performance and low latency machine learning inference. These instances are optimized for deploying deep learning (DL) models for applications, such as natural language processing, object detection and classification, content personalization and filtering, and speech recognition.<br>There are a variety of ways that you can get started:<br>
<br>Use SageMaker, a fully-managed service that is the easiest way to get started with machine learning models. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html" target="_blank">Get Started with SageMaker</a>&nbsp;in the&nbsp;Amazon SageMaker Developer Guide.<br>

<br>Launch an Inf1 or Inf2 instance using the Deep Learning AMI. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia.html" target="_blank">AWS Inferentia with DLAMI</a>&nbsp;in the&nbsp;AWS Deep Learning AMIs Developer Guide.<br>

<br>Launch an Inf1 or Inf2 instance using your own AMI and install the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/aws/aws-neuron-sdk" rel="noopener nofollow" class="external-link" href="https://github.com/aws/aws-neuron-sdk" target="_blank">AWS Neuron SDK</a>, which enables you to compile, run, and profile deep learning models for AWS Inferentia.<br>

<br>Launch a container instance using an Inf1 or Inf2 instance and an Amazon ECS-optimized AMI. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html" target="_blank">Amazon Linux 2 (Inferentia) AMIs</a>&nbsp;in the&nbsp;Amazon Elastic Container Service Developer Guide.<br>

<br>Create an Amazon EKS cluster with nodes running Inf1 instances. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html" target="_blank">Inferentia support</a>&nbsp;in the&nbsp;Amazon EKS User Guide.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/aws-instance-types.html</link><guid isPermaLink="false">AWS/For Reading/AWS Instance Types.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[AWS Load Balancer Types]]></title><description><![CDATA[ 
 <br><br>AWS offers four types of load balancers:<br>]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/difference-between-load-balancers.html</link><guid isPermaLink="false">AWS/For Reading/Difference between Load balancers.md</guid><pubDate>Sun, 29 Dec 2024 14:19:08 GMT</pubDate></item><item><title><![CDATA[Host Tomcat Vanshit's Version]]></title><description><![CDATA[<a class="tag" href="https://notes.sarangwandile.xyz/?query=tag:git" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#git</a> 
 <br>.............................................Deploying Application..........................<br>1.Create an instance by using ec2-service and create database by RDS service.<br>
2.Create custom security group by adding SSH, custom tcp 8080 , for maria db add a port custom tcp 3306.<br>
3.Add this Security group to EC2 instance and Database instance.<br>
4.Login to instance.<br>
5.Search tomcat 9 on google and Download its zip file.<br>
6.Use curl -O <a rel="noopener nofollow" class="external-link" href="https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip" target="_blank">https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip</a>.<br>
7.unzip it<br>
8. Install java-17 package:-<br>
yum install java-17 -y<br>
9.Change directory :- cd apache-tomcat-9.0.97/bin<br>
10.bash catalina.sh start<br>
11.After this steps search with your instance public ip (For ex, 192.25.26.244:8080)<br>
12.Install git :- yum install git -y<br>
13.Run command :- <a href="https://notes.sarangwandile.xyz?query=tag:git" class="tag" target="_blank" rel="noopener nofollow">#git</a> clone <a rel="noopener nofollow" class="external-link" href="https://github.com/Pritam-Khergade/student-ui" target="_blank">https://github.com/Pritam-Khergade/student-ui</a><br>
14. 2 folders will be downloaded in bin dir.<br>
15.cd student-ui<br>
16.Install maven :- yum install maven -y<br>
17.mvn clean package.<br>
18.After cleaning package u should see build success notification.<br>
19.cd target<br>
20.Rename file :- mv studentapp-2.2-SNAPSHOT.war studentapp.war<br>
21.Move this folder in tomcat/webapps dir :-<br>
mv studentapp.war /root/apache-tomcat-9.0.97/webapps/<br>
22.Search with public ip u should see a registeration form :- 192.25.26.244:8080/studentapp<br>.................................................Database part......................................................<br>1.Install maria db in instance :-  yum install mariadb105 -y<br>
2.Connect to database :- MySQL -h van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com(endpoint) -u  -p <br>
3.After this u will get into database.<br>
4.Create DATABASE with name studentapp and tables,<br>CREATE DATABASE studentapp;<br>
use studentapp;<br>CREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,
	student_name VARCHAR(100) NOT NULL,
    student_addr VARCHAR(100) NOT NULL,
	student_age VARCHAR(3) NOT NULL,
	student_qual VARCHAR(20) NOT NULL,
	student_percent VARCHAR(10) NOT NULL,
	student_year_passed VARCHAR(10) NOT NULL,
	PRIMARY KEY (student_id)
);
<br>5.After this exit from database.<br>
6.vim /apache-tomcat-9.0.97/conf/context.xml<br>
7.Write this configuration :- <br>&lt;context&gt;
&lt;Resource name="jdbc/TestDB" auth="Container" type="javax.sql.DataSource"
           maxTotal="500" maxIdle="30" maxWaitMillis="1000"
           username="admin" password="Remote#123" driverClassName="com.mysql.jdbc.Driver"
           url="jdbc:mysql://van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com:3306/studentapp?useUnicode=yes&amp;amp;characterEncoding=utf8"/&gt;
&lt;context&gt;
<br>8.save and exit.<br>
9.After this download mysqlconnector.jar file from this link:-<br>
<a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar</a><br>
10.cd apache-tomcat-9.0.97/lib/<br>
11.Download this file in lib folder :- curl -O <a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar</a><br>
12.After this the data u entered in your form should be stored in this database.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/host-tomcat-vanshit's-version.html</link><guid isPermaLink="false">AWS/For Reading/Host Tomcat Vanshit's Version.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[How DNS Works]]></title><description><![CDATA[ 
 <br><br>The Domain Name System (DNS) is the phonebook of the Internet. Humans access information online through&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name/" target="_blank">domain names</a>, like nytimes.com or espn.com. Web browsers interact through&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/network-layer/internet-protocol/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/network-layer/internet-protocol/" target="_blank">Internet Protocol (IP)</a>&nbsp;addresses. DNS translates domain names to&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/what-is-my-ip-address/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/what-is-my-ip-address/" target="_blank">IP addresses</a>&nbsp;so browsers can load Internet resources.<br>Each device connected to the Internet has a unique IP address which other machines use to find the device. DNS servers eliminate the need for humans to memorize IP addresses such as 192.168.1.1 (in IPv4), or more complex newer alphanumeric IP addresses such as 2400:cb00:2048:1::c629:d7a2 (in IPv6).<br><img title="What is DNS?" alt="DNS - magnifying glass examines IP addresses, finds www.example.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png" referrerpolicy="no-referrer"><br><br>The process of DNS resolution involves converting a hostname (such as <a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a>) into a computer-friendly IP address (such as 192.168.1.1). An IP address is given to each device on the Internet, and that address is necessary to find the appropriate Internet device - like a street address is used to find a particular home. When a user wants to load a webpage, a translation must occur between what a user types into their web browser (example.com) and the machine-friendly address necessary to locate the example.com webpage.<br>In order to understand the process behind the DNS resolution, it’s important to learn about the different hardware components a DNS query must pass between. For the web browser, the DNS lookup occurs "behind the scenes" and requires no interaction from the user’s computer apart from the initial request.<br>Report<br>2023 GigaOm Radar for DNS Security<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/lp/gigaom-radar-dns-security/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/lp/gigaom-radar-dns-security/" target="_blank">Get the report</a><br>Report<br>Read the Q4 2023 DDoS Threat Landscape Report<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/lp/ddos-threat-report-2023-q4/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/lp/ddos-threat-report-2023-q4/" target="_blank">Get the report</a><br><br>
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">DNS recursor</a>&nbsp;- The recursor can be thought of as a librarian who is asked to go find a particular book somewhere in a library. The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client’s DNS query.
<br>Root nameserver&nbsp;- The&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/dns-root-server/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/dns-root-server/" target="_blank">root server</a>&nbsp;is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations.
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">TLD nameserver</a>&nbsp;- The top level domain server (<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/top-level-domain/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/top-level-domain/" target="_blank">TLD</a>) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is “com”).
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">Authoritative nameserver</a>&nbsp;- This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.
<br>Fast &amp; Secure DNS<br>Free DNS included with any Cloudflare plan<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/plans/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/plans/" target="_blank">Start for free</a><br><br>Both concepts refer to servers (groups of servers) that are integral to the DNS infrastructure, but each performs a different role and lives in different locations inside the pipeline of a DNS query. One way to think about the difference is the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" target="_blank">recursive</a>&nbsp;resolver is at the beginning of the DNS query and the authoritative nameserver is at the end.<br><br>The recursive resolver is the computer that responds to a recursive request from a client and takes the time to track down the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/" target="_blank">DNS record</a>. It does this by making a series of requests until it reaches the authoritative DNS nameserver for the requested record (or times out or returns an error if no record is found). Luckily, recursive DNS resolvers do not always need to make multiple requests in order to track down the records needed to respond to a client;&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/what-is-caching/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/what-is-caching/" target="_blank">caching</a>&nbsp;is a data persistence process that helps short-circuit the necessary requests by serving the requested resource record earlier in the DNS lookup.<br><img title="DNS Record Request Sequence - Recursive Resolver" alt="DNS Record Request Sequence - DNS Recursive Resolver gets request from client" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3NOmAzkfPG8FTA8zLc7Li8/8efda230b212c0de2d3bbcb408507b1e/dns_record_request_sequence_recursive_resolver.png" referrerpolicy="no-referrer"><br><br>Put simply, an authoritative DNS server is a server that actually holds, and is responsible for, DNS resource records. This is the server at the bottom of the DNS lookup chain that will respond with the queried resource record, ultimately allowing the web browser making the request to reach the IP address needed to access a website or other web resources. An authoritative nameserver can satisfy queries from its own data without needing to query another source, as it is the final source of truth for certain DNS records.<br><img title="DNS Record Request Sequence - Authoritative Nameserver" alt="DNS Record Request Sequence - DNS query reaches authoritative nameserver for cloudflare.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6Cxvsc4NOvmU4pPkKbkDmP/a7588a4c8a3c187e9175a40fa1b3d548/dns_record_request_sequence_authoritative_nameserver.png" referrerpolicy="no-referrer"><br>It’s worth mentioning that in instances where the query is for a subdomain such as foo.example.com or&nbsp;<a data-tooltip-position="top" aria-label="https://blog.cloudflare.com/" rel="noopener nofollow" class="external-link" href="https://blog.cloudflare.com/" target="_blank">blog.cloudflare.com</a>, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/" target="_blank">CNAME record</a>.<br><img title="DNS Record Request Sequence - CNAME record for subdomain" alt="DNS Record Request Sequence - DNS query to CNAME record for subdomain blog.cloudflare.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1O1o3jhs0ztWsD00k8RLIJ/f33c1793a7e21cb92678c1f35ef1b245/dns_record_request_sequence_cname_subdomain.png" referrerpolicy="no-referrer"><br>There is a key difference between many DNS services and the one that Cloudflare provides. Different DNS recursive resolvers such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/cloudflare-vs-google-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/cloudflare-vs-google-dns/" target="_blank">Google DNS</a>, OpenDNS, and providers like Comcast all maintain data center installations of DNS recursive resolvers. These resolvers allow for quick and easy queries through optimized clusters of DNS-optimized computer systems, but they are fundamentally different than the nameservers hosted by Cloudflare.<br>Cloudflare maintains infrastructure-level nameservers that are integral to the functioning of the Internet. One key example is the&nbsp;<a data-tooltip-position="top" aria-label="https://blog.cloudflare.com/f-root/" rel="noopener nofollow" class="external-link" href="https://blog.cloudflare.com/f-root/" target="_blank">f-root server network</a>&nbsp;which Cloudflare is partially responsible for hosting. The F-root is one of the root level DNS nameserver infrastructure components responsible for the billions of Internet requests per day. Our&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/glossary/anycast-network/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/glossary/anycast-network/" target="_blank">Anycast network</a>&nbsp;puts us in a unique position to handle large volumes of DNS traffic without service interruption.<br><br>For most situations, DNS is concerned with a domain name being translated into the appropriate IP address. To learn how this process works, it helps to follow the path of a DNS lookup as it travels from a web browser, through the DNS lookup process, and back again. Let's take a look at the steps.<br>Note: Often DNS lookup information will be cached either locally inside the querying computer or remotely in the DNS infrastructure. There are typically 8 steps in a DNS lookup. When DNS information is cached, steps are skipped from the DNS lookup process which makes it quicker. The example below outlines all 8 steps when nothing is cached.<br><br>
<br>A user types ‘example.com’ into a web browser and the query travels into the Internet and is received by a DNS recursive resolver.
<br>The resolver then queries a DNS root nameserver (.).
<br>The root server then responds to the resolver with the address of a Top Level Domain (TLD) DNS server (such as .com or .net), which stores the information for its domains. When searching for example.com, our request is pointed toward the .com TLD.
<br>The resolver then makes a request to the .com TLD.
<br>The TLD server then responds with the IP address of the domain’s nameserver, example.com.
<br>Lastly, the recursive resolver sends a query to the domain’s nameserver.
<br>The IP address for example.com is then returned to the resolver from the nameserver.
<br>The DNS resolver then responds to the web browser with the IP address of the domain requested initially.
<br>Once the 8 steps of the DNS lookup have returned the IP address for example.com, the browser is able to make the request for the web page:<br>
<br>The browser makes a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/" target="_blank">HTTP</a>&nbsp;request to the IP address.
<br>The server at that IP returns the webpage to be rendered in the browser (step 10).
<br><img title="Complete DNS Lookup and Webpage Query" alt="Complete DNS Lookup and Webpage Query - 10 steps" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1NzaAqpEFGjqTZPAS02oNv/bf7b3f305d9c35bde5c5b93a519ba6d5/what_is_a_dns_server_dns_lookup.png" referrerpolicy="no-referrer"><br><br>The DNS resolver is the first stop in the DNS lookup, and it is responsible for dealing with the client that made the initial request. The resolver starts the sequence of queries that ultimately leads to a URL being translated into the necessary IP address.<br>Note: A typical uncached DNS lookup will involve both recursive and iterative queries.<br>It's important to differentiate between a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" target="_blank">recursive DNS</a>&nbsp;query and a recursive DNS resolver. The query refers to the request made to a DNS resolver requiring the resolution of the query. A DNS recursive resolver is the computer that accepts a recursive query and processes the response by making the necessary requests.<br><img title="DNS Recursive Query" alt="DNS recursive query goes from DNS client to DNS recursive resolver" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/rOXBgctX2gaXNDqP5ktek/7086a97e00525159c6bd9318819c2287/dns_recursive_query.png" referrerpolicy="no-referrer"><br><br>In a typical DNS lookup three types of queries occur. By using a combination of these queries, an optimized process for DNS resolution can result in a reduction of distance traveled. In an ideal situation cached record data will be available, allowing a DNS name server to return a non-recursive query.<br><br>
<br>Recursive query&nbsp;- In a recursive query, a DNS client requires that a DNS server (typically a DNS recursive resolver) will respond to the client with either the requested resource record or an error message if the resolver can't find the record.
<br>Iterative query&nbsp;- in this situation the DNS client will allow a DNS server to return the best answer it can. If the queried DNS server does not have a match for the query name, it will return a referral to a DNS server authoritative for a lower level of the domain namespace. The DNS client will then make a query to the referral address. This process continues with additional DNS servers down the query chain until either an error or timeout occurs.
<br>Non-recursive query&nbsp;- typically this will occur when a DNS resolver client queries a DNS server for a record that it has access to either because it's authoritative for the record or the record exists inside of its cache. Typically, a DNS server will cache DNS records to prevent additional bandwidth consumption and load on upstream servers.
<br><br>The purpose of caching is to temporarily stored data in a location that results in improvements in performance and reliability for data requests. DNS caching involves storing data closer to the requesting client so that the DNS query can be resolved earlier and additional queries further down the DNS lookup chain can be avoided, thereby improving load times and reducing bandwidth/CPU consumption. DNS data can be cached in a variety of locations, each of which will store DNS records for a set amount of time determined by a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/" target="_blank">time-to-live (TTL)</a>.<br><br>Modern web browsers are designed by default to cache DNS records for a set amount of time. The purpose here is obvious; the closer the DNS caching occurs to the web browser, the fewer processing steps must be taken in order to check the cache and make the correct requests to an IP address. When a request is made for a DNS record, the browser cache is the first location checked for the requested record.<br>In Chrome, you can see the status of your DNS cache by going to chrome://net-internals/#dns.<br><br>The operating system level DNS resolver is the second and last local stop before a DNS query leaves your machine. The process inside your operating system that is designed to handle this query is commonly called a “stub resolver” or DNS client. When a stub resolver gets a request from an application, it first checks its own cache to see if it has the record. If it does not, it then sends a DNS query (with a recursive flag set), outside the local network to a DNS recursive resolver inside the Internet service provider (ISP).<br>When the recursive resolver inside the ISP receives a DNS query, like all previous steps, it will also check to see if the requested host-to-IP-address translation is already stored inside its local persistence layer.<br>The recursive resolver also has additional functionality depending on the types of records it has in its cache:<br>
<br>If the resolver does not have the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/" target="_blank">A records</a>, but does have the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-ns-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-ns-record/" target="_blank">NS records</a>&nbsp;for the authoritative nameservers, it will query those name servers directly, bypassing several steps in the DNS query. This shortcut prevents lookups from the root and .com nameservers (in our search for example.com) and helps the resolution of the DNS query occur more quickly.
<br>If the resolver does not have the NS records, it will send a query to the TLD servers (.com in our case), skipping the root server.
<br>In the unlikely event that the resolver does not have records pointing to the TLD servers, it will then query the root servers. This event typically occurs after a DNS cache has been purged.
<br>Learn about what differentiates&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/dns/" target="_blank">Cloudflare DNS</a>&nbsp;from other DNS providers.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/how-dns-works.html</link><guid isPermaLink="false">AWS/For Reading/How DNS Works.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[IAM Policies]]></title><description><![CDATA[ 
 <br><br><br><br>Based on the provided search results, IAM (Identity and Access Management) in AWS offers four types of policies:<br>
<br>Identity-Based Policies: Attached to an identity (user, group, or role), these policies define the permissions for that specific identity. They are used to control access to AWS resources and services.
<br>Resource-Based Policies: Attached to a specific resource (e.g., S3 bucket, EC2 instance), these policies define the permissions for that resource. They specify which identities have access to the resource and under what conditions.
<br>Service Control Policies (SCPs): These policies control the actions that can be performed by IAM users and roles across AWS services. They are used to enforce organizational compliance and governance.
<br>Permissions Boundary Policies: These policies define the maximum possible permissions that an IAM role can have. They are used to restrict the permissions granted to roles created by CI/CD pipelines or other automated processes.
<br>Here's a summary of the policies and their uses:<br><br>Note that Managed Policies and Customer Managed Policies are not separate policy types, but rather categories of policies. Managed Policies are created and managed by AWS, while Customer Managed Policies are created and managed by users in their AWS accounts. Inline Policies are embedded within an identity or resource, but are not a distinct policy type.<br>For more information, refer to the AWS documentation and the provided search results.<br><br>Based on the provided search results, AWS Identity and Access Management (IAM) supports the following types of users:<br>
<br>IAM User: An IAM user is an entity you create in AWS. This user is an identity within your AWS account that’s granted specific custom permissions. IAM users have a name and password used to sign in to the AWS Management Console.
<br>Root User: The root user is the initial sign-in identity for an AWS account. It has complete access to all AWS services and resources in the account. However, AWS strongly recommends not using the root user for everyday tasks and instead creating IAM users with limited permissions.
<br>IAM Identity Center User: IAM Identity Center users are members of AWS Organizations and can be granted access to multiple AWS accounts and applications through the AWS access portal. They can use their corporate credentials to sign in if their company has integrated Active Directory or another identity provider with IAM Identity Center.
<br>Federated Identity: With web identity federation, you can receive an authentication token and exchange it for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Federated identities sign in using an external identity provider.
<br>AWS Builder ID User: AWS Builder ID users are a type of user that represents an individual and allows access to AWS services and tools without an AWS account. They sign in to specific AWS services or tools using their AWS Builder ID profile.
<br>Note that these types of users are not mutually exclusive, and you may use a combination of them in your AWS environment. For example, you might create IAM users for your workforce and use federated identities for external partners or customers.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/iam-policies.html</link><guid isPermaLink="false">AWS/For Reading/IAM Policies.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Load Balancer]]></title><description><![CDATA[ 
 <br>Last Updated :&nbsp;04 Oct, 2024<br>When a website becomes extremely popular, the traffic on that website increases, and the load on a single server also increases. The concurrent traffic overloads the single server and the website becomes slower for the users. To meet the request of these high volumes of data and to return the correct response in a fast and reliable manner, we need to scale the server. This can be done by adding more servers to the network and distributing all the requests across these servers.&nbsp;<br><img alt="load-balancer-copy-(1)" src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp" referrerpolicy="no-referrer"><br>Important Topics for the Load Balancer – System Design Interview Question<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-is-a-load-balancer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-is-a-load-balancer" target="_blank">What is a Load Balancer?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-will-happen-if-there-is-no-load-balancer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-will-happen-if-there-is-no-load-balancer" target="_blank">What will happen if there is No Load Balancer?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-load-balancer-works" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-load-balancer-works" target="_blank">How Load Balancer Works?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#where-are-load-balancers-typically-placed" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#where-are-load-balancers-typically-placed" target="_blank">Where Are Load Balancers Typically Placed?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#types-of-load-balancers" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#types-of-load-balancers" target="_blank">Types of Load Balancers</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#load-balancing-algorithms" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#load-balancing-algorithms" target="_blank">Load Balancing Algorithms</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-to-use-load-balancing-during-system-design-interviews" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-to-use-load-balancing-during-system-design-interviews" target="_blank">How to Use Load Balancing During System Design Interviews?</a>
<br><br>A load balancer is a networking device or software application that distributes and balances the incoming traffic among the servers to provide high availability, efficient utilization of servers, and high performance.<br>This article outlines various strategies for implementing load balancing and its importance in high-availability systems. To build a solid foundation in system design, the&nbsp;<a data-tooltip-position="top" aria-label="https://gfgcdn.com/tu/S4c/" rel="noopener nofollow" class="external-link" href="https://gfgcdn.com/tu/S4c/" target="_blank">*<strong></strong></a>*System Design Course**&nbsp;will help you understand the underlying principles and techniques for designing robust and scalable systems.<br>
<br>Load balancers are highly used in cloud computing domains, data centers, and large-scale web applications where traffic flow needs to be managed.&nbsp;
<br>The primary goal of using a load balancer is, not to overburden with huge incoming traffic which may lead to server crashes or high latency.
<br><br>Before understanding how a load balancer works, let’s understand what problem will occur without the load balancer through an example.<br>
Consider a scenario where an application is running on a single server and the client connects to that server directly without load balancing.<br>

<br><img alt="lb1" src="https://media.geeksforgeeks.org/wp-content/uploads/20240213114428/lb1.webp" referrerpolicy="no-referrer"><br>There are two main problems with this model:<br>
<br>**Single Point of Failure:**&nbsp;

<br>If the server goes down or something happens to the server the whole application will be interrupted and it will become unavailable for the users for a certain period. It will create a bad experience for users which is unacceptable for service providers.


<br>**Overloaded Servers:**&nbsp;

<br>There will be a limitation on the number of requests that a web server can handle. If the business grows and the number of requests increases the server will be overloaded.&nbsp;
<br>To solve the increasing number of requests we need to add a few more servers and we need to distribute the requests to the cluster of servers.&nbsp;


<br><br>Lets understand how Load Balancer works through the above discussed example:<br>To solve the above issue and to distribute the number of requests we can add a load balancer in front of the web servers and allow our services to handle any number of requests by adding any number of web servers in the network.&nbsp;<br>
<br>We can spread the request across multiple servers.&nbsp;
<br>For some reason, if one of the servers goes offline the service will be continued.
<br>Also, the latency on each request will go down because each server is not bottlenecked on RAM/Disk/CPU anymore.
<br><img alt="lb2" src="https://media.geeksforgeeks.org/wp-content/uploads/20240213114444/lb2.webp" referrerpolicy="no-referrer"><br>Load balancers minimize server response time and maximize throughput. Load balancer ensures high availability and reliability by sending requests only to online servers Load balancers do continuous health checks to monitor the server’s capability of handling the request. Depending on the number of requests or demand load balancers add or remove the number of servers.<br><br>Below is the image where a load balancer can be placed…<br><img alt="Where-are-Load-Balancer-placed-copy-(1)" src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140708/Where-are-Load-Balancer-placed-copy-(1).webp" referrerpolicy="no-referrer"><br>
<br>In between the client application/user and the server
<br>In between the server and the application/job servers
<br>In between the application servers and the cache servers
<br>In between the cache servers the database servers
<br><br><br>There are mainly three typers of load balancers based on configurations:<br><br>Software load balancers are applications or components that run on general-purpose servers. They are implemented in software, making them flexible and adaptable to various environments.<br><br>As the name suggests we use a physical appliance to distribute the traffic across the cluster of network servers. These load balancers are also known as Layer 4-7 Routers and these are capable of handling all kinds of HTTP, HTTPS, TCP, and UDP traffic.<br>
<br>These load balancers are expensive to acquire and configure, which is the reason a lot of service providers use them only as the first entry point for user requests.
<br>Later the internal software load balancers are used to redirect the data behind the infrastructure wall.&nbsp;
<br><br>A virtual load balancer is a type of load balancing solution implemented as a virtual machine (VM) or software instance within a virtualized environment ,such as data centers utilizing virtualization technologies like VMware, Hyper-V, or KVM.. It plays a crucial role in distributing incoming network traffic across multiple servers or resources to ensure efficient utilization of resources, improve response times, and prevent server overload.<br><br>There are mainly three typers of load balancers based on functions:<br><br>Layer-4 load balancers operate at the transport layer of the OSI model. They make forwarding decisions based on information available in network layer protocols (such as IP addresses and port numbers).&nbsp;<br><br>Layer-7 load balancers operate at the application layer of the OSI model. They can make load balancing decisions based on content, including information such as URLs, HTTP headers, or cookies.&nbsp;&nbsp;<br><br>GSLB stands for Global Server Load Balancer.&nbsp;This type of load balancer goes beyond the traditional local load balancing and is designed for distributing traffic across multiple data centers or geographically distributed servers. It takes into account factors such as server proximity, server health, and geographic location to intelligently distribute traffic across multiple locations.<br>**Further Read:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/layer-4l4-layer-7l7-and-gslb-load-balancers/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/layer-4l4-layer-7l7-and-gslb-load-balancers/" target="_blank">Layer-4 vs. Layer-7 vs. GSLB</a><br><br>We need a load-balancing algorithm to decide which request should be redirected to which backend server. The different system uses different ways to select the servers from the load balancer. Companies use varieties of load-balancing algorithm techniques depending on the configuration. Some of the common load-balancing algorithms are given below:<br><br>The Round Robin algorithm is a simple static load balancing approach in which requests are distributed across the servers in a sequential or rotational manner.&nbsp;It is easy to implement but it doesn’t consider the load already on a server so there is a risk that one of the servers receives a lot of requests and becomes overloaded.<br><br>The Weighted Round Robin algorithm is also a static load balancing approach which is much similar to the round-robin technique. The only difference is, that each of the resources in a list is provided a weighted score.&nbsp;Depending on the weighted score the request is distributed to these servers.&nbsp;<br><br>The Source IP Hash cLoad Balancing Algorithm is a static method used in network load balancing to distribute incoming requests among a set of servers based on the hash value of the source IP address. This algorithm aims to ensure that requests originating from the same source IP address are consistently directed to the same server.<br><br>The Least Connections algorithm is a dynamic load balancing approach that assigns new requests to the server with the fewest active connections. The idea is to distribute incoming workloads in a way that minimizes the current load on each server, aiming for a balanced distribution of connections across all available resources.&nbsp;<br><br>The Least Response method is a dynamic load balancing approach that aims to minimize response times by directing new requests to the server with the quickest response time.&nbsp;<br><br>In your system design interview, you’ll be asked some sort of scalability question where you’ll have to explain how load balancers help distribute the traffic and how it ensures scalability and availability of services in your application. The overall concept that you need to keep in mind from this article is…<br>
<br>A load balancer enables elastic scalability which improves the performance and throughput of data. It allows you to keep many copies of data (redundancy) to ensure the availability of the system. In case a server goes down or fails you’ll have the backup to restore the services.&nbsp;
<br>Load balancers can be placed at any software layer.
<br>Many companies use both hardware and software to implement load balancers, depending on the different scale points in their system.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/load-balancer.html</link><guid isPermaLink="false">AWS/For Reading/Load Balancer.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Managed policies and inline policies]]></title><description><![CDATA[ 
 <br><br>When you set the permissions for an identity in IAM, you must decide whether to use an AWS managed policy, a customer managed policy, or an inline policy. The following topics provide more information about each of the types of identity-based policies and when to use them.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies" target="_blank">AWS managed policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies" target="_blank">Customer managed policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#inline-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#inline-policies" target="_blank">Inline policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-choosing-managed-or-inline.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-choosing-managed-or-inline.html" target="_blank">Choose between managed policies and inline policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-convert-inline-to-managed.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-convert-inline-to-managed.html" target="_blank">Convert an inline policy to a managed policy</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-deprecated.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-deprecated.html" target="_blank">Deprecated AWS managed policies</a>
<br><br>An&nbsp;AWS managed policy&nbsp;is a standalone policy that is created and administered by AWS. A&nbsp;standalone policy&nbsp;means that the policy has its own Amazon Resource Name (ARN) that includes the policy name. For example,&nbsp;arn:aws:iam::aws:policy/IAMReadOnlyAccess&nbsp;is an AWS managed policy. For more information about ARNs, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" target="_blank">IAM ARNs</a>. For a list of AWS managed policies for AWS services, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/policy-list.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/policy-list.html" target="_blank">AWS managed policies</a>.<br>AWS managed policies make it convenient for you to assign appropriate permissions to users, IAM groups, and roles. It is faster than writing the policies yourself, and includes permissions for many common use cases.<br>You cannot change the permissions defined in AWS managed policies. AWS occasionally updates the permissions defined in an AWS managed policy. When AWS does this, the update affects all principal entities (IAM users, IAM groups, and IAM roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API calls become available for existing services. For example, the AWS managed policy called&nbsp;ReadOnlyAccess&nbsp;provides read-only access to all AWS services and resources. When AWS launches a new service, AWS updates the&nbsp;ReadOnlyAccess&nbsp;policy to add read-only permissions for the new service. The updated permissions are applied to all principal entities that the policy is attached to.<br>Full access AWS managed policies: These define permissions for service administrators by granting full access to a service. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonDynamoDBFullAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonDynamoDBFullAccess.html" target="_blank">AmazonDynamoDBFullAccess</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/IAMFullAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/IAMFullAccess.html" target="_blank">IAMFullAccess</a><br>

<br>Power-user AWS managed policies: These provide full access to AWS services and resources, but do not allow managing users and IAM groups. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSCodeCommitPowerUser.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSCodeCommitPowerUser.html" target="_blank">AWSCodeCommitPowerUser</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSKeyManagementServicePowerUser.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSKeyManagementServicePowerUser.html" target="_blank">AWSKeyManagementServicePowerUser</a><br>

<br>Partial-access AWS managed policies: These provide specific levels of access to AWS services without allowing&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_understand-policy-summary-access-level-summaries.html#access_policies_access-level" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_understand-policy-summary-access-level-summaries.html#access_policies_access-level" target="_blank">permissions management</a>&nbsp;access level permissions. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonMobileAnalyticsWriteOnlyAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonMobileAnalyticsWriteOnlyAccess.html" target="_blank">AmazonMobileAnalyticsWriteOnlyAccess</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEC2ReadOnlyAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEC2ReadOnlyAccess.html" target="_blank">AmazonEC2ReadOnlyAccess</a><br>

<br>Job function AWS managed policies: These policies align closely with commonly used job functions in the IT industry and facilitate granting permissions for these job functions. One key advantage of using job function policies is that they are maintained and updated by AWS as new services and API operations are introduced. For example, the&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AdministratorAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AdministratorAccess.html" target="_blank">AdministratorAccess</a>&nbsp;job function provides full access and permissions delegation to every service and resource in AWS. We recommend that you use this policy only for the account administrator. For power users that require full access to every service except limited access to IAM and Organizations, use the&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/PowerUserAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/PowerUserAccess.html" target="_blank">PowerUserAccess</a>&nbsp;job function. For a list and descriptions of the job function policies, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html" target="_blank">AWS managed policies for job functions</a>.<br>The following diagram illustrates AWS managed policies. The diagram shows three AWS managed policies:&nbsp;AdministratorAccess,&nbsp;PowerUserAccess, and&nbsp;AWSCloudTrail_ReadOnlyAccess. Notice that a single AWS managed policy can be attached to principal entities in different AWS accounts, and to different principal entities in a single AWS account.<br><img alt="Diagram of AWS managed policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png" referrerpolicy="no-referrer"><br><br>You can create standalone policies in your own AWS account that you can attach to principal entities (IAM users, IAM groups, and IAM roles). You create these&nbsp;customer managed policies&nbsp;for your specific use cases, and you can change and update them as often as you like. Like AWS managed policies, when you attach a policy to a principal entity, you give the entity the permissions that are defined in the policy. When you update permissions in the policy, the changes are applied to all principal entities that the policy is attached to.<br>A great way to create a customer managed policy is to start by copying an existing AWS managed policy. That way you know that the policy is correct at the beginning and all you need to do is customize it to your environment.<br>The following diagram illustrates customer managed policies. Each policy is an entity in IAM with its own&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" target="_blank">Amazon Resource Name (ARN)</a>&nbsp;that includes the policy name. Notice that the same policy can be attached to multiple principal entities—for example, the same&nbsp;DynamoDB-books-app&nbsp;policy is attached to two different IAM roles.<br>For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html" target="_blank">Define custom IAM permissions with customer managed policies</a><br><img alt="Diagram of customer managed policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-customer-managed-policies.diagram.png" referrerpolicy="no-referrer"><br><br>An inline policy is a policy created for a single IAM identity (a user, user group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity. You can create a policy and embed it in an identity, either when you create the identity or later. If a policy could apply to more than one entity, it’s better to use a managed policy.<br>The following diagram illustrates inline policies. Each policy is an inherent part of the user, group, or role. Notice that two roles include the same policy (the&nbsp;DynamoDB-books-app&nbsp;policy), but they are not sharing a single policy. Each role has its own copy of the policy.<br><img alt="Diagram of inline policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-inline-policies.diagram.png" referrerpolicy="no-referrer"><br>]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/managed-policies-and-inline-policies.html</link><guid isPermaLink="false">AWS/For Reading/Managed policies and inline policies.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Types of Network Protocols and Their Uses]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;22 May, 2024<br>Network protocols are a set of rules that are responsible for the communication of data between various devices in the network. These protocols define guidelines and conventions for transmitting and receiving data, ensuring efficient and reliable data communication.<br><br>A network protocol is a set of rules that govern data communication between different devices in the network. It determines what is being communicated, how it is being communicated, and when it is being communicated. It permits connected devices to communicate with each other, irrespective of internal and structural differences.<br><br>It is essential to understand how devices communicate over a network by recognizing network protocols. The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/open-systems-interconnection-model-osi/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/open-systems-interconnection-model-osi/" target="_blank">Open Systems Interconnection (OSI),</a>&nbsp;the most widely used model, illustrates how computer systems interact with one another over a network. The communication mechanism between two network devices is shown by seven different layers in the OSI model. Every layer in the OSI model works based on different network protocols. At every layer, one or more protocols are there for network communication. To enable network-to-network connections, the Internet Protocol (IP), for instance, routes data by controlling information like the source and destination addresses of data packets. It is known as a network layer protocol.<br><br>In most cases, communication across a network like the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/internet-and-its-services/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/internet-and-its-services/" target="_blank">Internet</a>&nbsp;uses the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/layers-of-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/layers-of-osi-model/" target="_blank">OSI model</a>. The OSI model has a total of seven layers. Secured connections, network management, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-and-communication/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-and-communication/" target="_blank">network communication</a>&nbsp;are the three main tasks that the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/elements-of-network-protocol/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/elements-of-network-protocol/" target="_blank">network protocol</a>&nbsp;performs. The purpose of protocols is to link different devices.<br>The protocols can be broadly classified into three major categories:<br>
<br>Network Communication
<br>Network Management
<br>Network Security
<br><br>Communication protocols are really important for the functioning of a network. They are so crucial that it is not possible to have computer networks without them. These protocols formally set out the rules and formats through which data is transferred. These protocols handle syntax, semantics, error detection, synchronization, and authentication. Below mentioned are some network communication protocol:<br><br>It is a layer 7 protocol that is designed for transferring a hypertext between two or more systems.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/http-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/http-full-form/" target="_blank">HTTP</a>&nbsp;works on a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/client-server-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/client-server-model/" target="_blank">client-server model</a>, most of the data sharing over the web is done through using HTTP.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/tcp-ip-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/tcp-ip-model/" target="_blank">TCP</a>&nbsp;layouts a reliable stream delivery by using sequenced acknowledgment. It is a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-oriented-service/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-oriented-service/" target="_blank">connection-oriented</a>&nbsp;protocol i.e., it establishes a connection between applications before sending any&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-data/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-data/" target="_blank">data</a>. It is used for communicating over a network. It has many applications such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/email-protocols/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/email-protocols/" target="_blank">emails</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" target="_blank">FTP</a>, streaming media, etc.<br><br>It is a connectionless protocol that lay-out a basic but unreliable message service. It adds no&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/flow-control-in-data-link-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/flow-control-in-data-link-layer/" target="_blank">flow control</a>, reliability, or&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-error-recovery/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-error-recovery/" target="_blank">error-recovery</a>&nbsp;functions.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" target="_blank">UPD</a>&nbsp;is functional in cases where reliability is not required. It is used when we want faster transmission, for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-broadcast-and-multicast/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-broadcast-and-multicast/" target="_blank">multicasting and broadcasting</a>&nbsp;connections, etc.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/border-gateway-protocol-bgp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/border-gateway-protocol-bgp/" target="_blank">BGP</a>&nbsp;is a routing protocol that controls how packets pass through the router in an independent system one or more networks run by a single organization and connect to different networks. It connects the endpoints of a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/lan-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/lan-full-form/" target="_blank">LAN</a>&nbsp;with other LANs and it also connects endpoints in different LANs to one another.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works/" target="_blank">ARP</a>&nbsp;is a protocol that helps in mapping logical addresses to the physical addresses acknowledged in a local network. For mapping and maintaining a correlation between these logical and physical addresses a table known as ARP cache is used.<br><br>It is a protocol through which data is sent from one host to another over the internet. It is used for addressing and routing data packets so that they can reach their destination.<br><br>it’s a protocol for network management and it’s used for the method of automating the process of configuring devices on IP networks. A&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/dynamic-host-configuration-protocol-dhcp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/dynamic-host-configuration-protocol-dhcp/" target="_blank">DHCP</a>&nbsp;server automatically assigns an&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-an-ip-address/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-an-ip-address/" target="_blank">IP address</a>&nbsp;and various other configurational changes to devices on a network so they can communicate with other IP networks. it also allows devices to use various services such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-time-protocol-ntp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-time-protocol-ntp/" target="_blank">NTP,</a>&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" target="_blank">DNS</a>, or any other protocol based on&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/differences-between-tcp-and-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/differences-between-tcp-and-udp/" target="_blank">TCP or UDP</a>.<br><br>These protocols assist in describing the procedures and policies that are used in monitoring, maintaining, and managing the computer network. These protocols also help in communicating these requirements across the network to ensure stable communication. Network management protocols can also be used for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-to-troubleshoot-common-http-error-codes/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-to-troubleshoot-common-http-error-codes/" target="_blank">troubleshooting</a>&nbsp;connections between a host and a client.<br><br>It is a layer 3 protocol that is used by network devices to forward operational information and error messages.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/internet-control-message-protocol-icmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/internet-control-message-protocol-icmp/" target="_blank">ICMP</a>&nbsp;is used for reporting congestions, network errors, diagnostic purposes, and timeouts.<br><br>It is a layer 7 protocol that is used for managing nodes on an IP network. There are three main components in the SNMP protocol i.e.,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" target="_blank">SNMP</a>&nbsp;agent, SNMP manager, and managed device. SNMP agent has the local knowledge of management details, it translates those details into a form that is compatible with the SNMP manager. The manager presents data acquired from SNMP agents, thus helping in monitoring network glitches, and network performance, and troubleshooting them.<br><br>It is a type of file retrieval protocol that provides downloadable files with some description for easy management, retrieving, and searching of files. All the files are arranged on a remote computer in a stratified manner. Gopher is an old protocol and it is not much used nowadays.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" target="_blank">FTP</a>&nbsp;is a Client/server protocol that is used for moving files to or from a host computer, it allows users to download&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-program-and-file/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-program-and-file/" target="_blank">files, programs</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/web-pages/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/web-pages/" target="_blank">web pages</a>, and other things that are available on other services.<br><br>It is a protocol that a local mail client uses to get email messages from a remote email server over a TCP/IP connection. Email servers hosted by ISPs also use the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-pop3-post-office-protocol-version-3/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-pop3-post-office-protocol-version-3/" target="_blank">POP3</a>&nbsp;protocol to hold and receive emails intended for their users. Eventually, these users will use email client software to look at their mailbox on the remote server and to download their emails. After the email client downloads the emails, they are generally deleted from the servers.<br><br>It is a protocol that allows the user to connect to a remote computer program and to use it i.e., it is designed for remote connectivity.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-to-telnet/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-to-telnet/" target="_blank">Telnet</a>&nbsp;creates a connection between a host machine and a remote endpoint to enable a remote session.<br><br>These protocols secure the data in passage over a network. These protocols also determine how the network secures data from any unauthorized attempts to extract or review data. These protocols make sure that no unauthorized devices, users, or services can access the network data. Primarily, these protocols depend on encryption to secure data.<br><br>It is a network security protocol mainly used for protecting sensitive data and securing internet connections. SSL allows both server-to-server and client-to-server communication. All the data transferred through&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/secure-socket-layer-ssl/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/secure-socket-layer-ssl/" target="_blank">SSL</a>&nbsp;is encrypted thus stopping any unauthorized person from accessing it.<br><br>It is the secured version of HTTP. this protocol ensures secure communication between two computers where one sends the request through the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/browser-developer-tools/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/browser-developer-tools/" target="_blank">browser</a>&nbsp;and the other fetches the data from the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/web-server-and-its-type/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/web-server-and-its-type/" target="_blank">web server</a>.<br><br>It is a security protocol designed for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/data-security/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/data-security/" target="_blank">data security</a>&nbsp;and privacy over the internet, its functionality is encryption, checking the integrity of data i.e., whether it has been tampered with or not, and authentication. It is generally used for encrypted communication between servers and web apps, like a web browser loading a website, it can also be used for encryption of messages, emails, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/voice-over-internet-protocol-voip/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/voice-over-internet-protocol-voip/" target="_blank">VoIP</a>.<br><br><br>
<br>ICMP protocol is used to retrieve message from the mail server. By using ICMP mail user can view and manage mails on his system.
<br><br>
<br>SIP is used in video, voice, and messaging application. This protocol is used to initiating, Managing, Terminating the session between two users while they are communicating.
<br><br>
<br>This protocol is used to forward audio, video over IP network. This protocol is used with SIP protocol to send audio, video at real-time.
<br><br>
<br>RAP is used in network management. It helps to user for accessing the nearest router for communication. RAP is less efficient as compared to&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" target="_blank">SNMP</a>.
<br><br>
<br>It is used to implement VPN ( Virtual Private Network ). PPTP protocol append PPP frame in IP datagram for transmission through IP based network.
<br><br>
<br>TFTP is the simplified version of FTP. TFTP is also used to transfer file over internet
<br><br>
<br>RLP is used to assign the resource such as server, printer, or other devices over the internet to the user. It is used to locate the resource to the client for broadcast query.
<br><br><br>
Network protocol is a set of rules that shows how data is transferred between various devices connected to the same network.
<br><br>
The protocol used while accessing the internet are TCP and UDP.
<br><br>
IP Multicasting is defined as the types of group communication in which data is sent simultaneously to multiple computers.
<br><br>
Important protocols of transport layer include-

<br>Transmission Control Protocol (TCP).
<br>User Datagram Protocol (UDP).
<br>Stream Control Transmission Protocol (SCTP).

<br><br>
Some important protocols of Application Layer include-

<br>Hyper Text Transfer Protocol (HTTP).
<br>File transfer Protocol (FTP).
<br>Simple Mail Transfer protocol (SMTP).
<br>Domain Name System (DNS).

<br><br>
Full form of DHCP is Dynamic Host Configuration Protocol.
<br><br>
Function of DHCP is to assign IP address to device on a network automatically.
<br><br>
A <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/virtual-lan-vlan/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/virtual-lan-vlan/" target="_blank">virtual local area network (VLAN)</a> is a virtualized link that unites various network nodes and devices from several LANs into a single logical network.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/network-protocols.html</link><guid isPermaLink="false">AWS/For Reading/Network Protocols.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[What is OSI Model? – Layers of OSI Model]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;11 Nov, 2024<br>The&nbsp;**OSI (Open Systems Interconnection)**&nbsp;Model is a set of rules that explains how different computer systems communicate over a network. OSI Model was developed by the&nbsp;**International Organization for Standardization (ISO)**. The OSI Model consists of 7 layers and each layer has specific functions and responsibilities.<br>This layered approach makes it easier for different devices and technologies to work together. OSI Model provides a clear structure for data transmission and managing network issues. The OSI Model is widely used as a reference to understand how network systems function.<br>In this article, we will discuss the OSI Model and each layer of the OSI Model in detail. We will also discuss the flow of data in the OSI Model and how the&nbsp;**OSI Model**&nbsp;is different from the&nbsp;**TCP/IP Model.**<br><img alt="OSI-Model" src="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif" referrerpolicy="no-referrer"><br>OSI Model<br>For those preparing for competitive exams like GATE, a strong understanding of networking concepts, including the OSI model, is crucial. To deepen your knowledge in this area and other key computer science topics, consider enrolling in the&nbsp;<a data-tooltip-position="top" aria-label="https://gfgcdn.com/tu/R89/" rel="noopener nofollow" class="external-link" href="https://gfgcdn.com/tu/R89/" target="_blank">*<strong></strong></a>*GATE CS Self-Paced course**&nbsp;. This course offers comprehensive coverage of the syllabus, helping you build a solid foundation for your exam preparation.<br><br>There are 7 layers in the OSI Model and each layer has its specific role in handling data. All the layers are mentioned below:<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/physical-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/physical-layer-in-osi-model" target="_blank">Physical Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/data-link-layer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/data-link-layer" target="_blank">Data Link Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-layer-in-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-layer-in-osi-model/" target="_blank">Network Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/transport-layer-in-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/transport-layer-in-osi-model/" target="_blank">Transport Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/session-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/session-layer-in-osi-model" target="_blank">Session Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/presentation-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/presentation-layer-in-osi-model" target="_blank">Presentation Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/application-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/application-layer-in-osi-model" target="_blank">Application Layer</a>
<br><br>The lowest layer of the OSI reference model is the&nbsp;**Physical Layer**. It is responsible for the actual physical connection between the devices. The physical layer contains information in the form of&nbsp;**bits.**&nbsp;Physical Layer is responsible for transmitting individual bits from one node to the next. When receiving data, this layer will get the signal received and convert it into 0s and 1s and send them to the Data Link layer, which will put the frame back together. Common physical layer devices are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-network-hub-and-how-it-works/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-network-hub-and-how-it-works/" target="_blank">Hub</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/repeaters-in-computer-network/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/repeaters-in-computer-network/" target="_blank">Repeater</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-modem-and-router/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-modem-and-router/" target="_blank">Modem</a>, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/types-of-ethernet-cable/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/types-of-ethernet-cable/" target="_blank">Cables</a>.<br><img alt="physical-layer-in-OSI" src="https://media.geeksforgeeks.org/wp-content/uploads/20241015103017414021/physical-layer-768.png" referrerpolicy="no-referrer"><br>Physical Layer<br><br>
<br>**Bit Synchronization:**&nbsp;The physical layer provides the synchronization of the bits by providing a clock. This clock controls both sender and receiver thus providing synchronization at the bit level.
<br>**Bit Rate Control:**&nbsp;The Physical layer also defines the transmission rate i.e. the number of bits sent per second.
<br>**Physical Topologies:**&nbsp;Physical layer specifies how the different, devices/nodes are arranged in a network i.e.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-bus-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-bus-topology/" target="_blank">bus topology</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-star-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-star-topology/" target="_blank">star topology</a>, or&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantage-and-disadvantage-of-mesh-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantage-and-disadvantage-of-mesh-topology/" target="_blank">mesh topology</a>.
<br>**Transmission Mode:**&nbsp;Physical layer also defines how the data flows between the two connected devices. The various transmission modes possible are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-simplex-half-duplex-and-full-duplex-transmission-modes/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-simplex-half-duplex-and-full-duplex-transmission-modes/" target="_blank">Simplex, half-duplex and full-duplex</a>.
<br><br>The data link layer is responsible for the node-to-node delivery of the message. The main function of this layer is to make sure data transfer is error-free from one node to another, over the physical layer. When a packet arrives in a network, it is the responsibility of the DLL to transmit it to the Host using its&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/mac-address-in-computer-network" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/mac-address-in-computer-network" target="_blank">MAC address</a>. Packet in the Data Link layer is referred to as&nbsp;**Frame.**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-switch-and-bridge/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-switch-and-bridge/" target="_blank">Switches and Bridges</a>&nbsp;are common Data Link Layer devices.<br>The Data Link Layer is divided into two sublayers:<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/logical-link-control-llc-protocol-data-unit" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/logical-link-control-llc-protocol-data-unit" target="_blank">Logical Link Control (LLC)</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-of-mac-address-in-computer-network" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-of-mac-address-in-computer-network" target="_blank">Media Access Control (MAC)</a>
<br>The packet received from the Network layer is further divided into frames depending on the frame size of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/nic-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/nic-full-form/" target="_blank">*<strong></strong></a>*NIC(Network Interface Card)**. DLL also encapsulates Sender and Receiver’s MAC address in the header.<br>The Receiver’s MAC address is obtained by placing an&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works" target="_blank">ARP(Address Resolution Protocol)</a>&nbsp;request onto the wire asking “Who has that IP address?” and the destination host will reply with its MAC address.<br><br>
<br>**Framing:**&nbsp;Framing is a function of the data link layer. It provides a way for a sender to transmit a set of bits that are meaningful to the receiver. This can be accomplished by attaching special bit patterns to the beginning and end of the frame.
<br>**Physical Addressing:**&nbsp;After creating frames, the Data link layer adds physical addresses (**MAC addresses**) of the sender and/or receiver in the header of each frame.
<br>**Error Control:**&nbsp;The data link layer provides the mechanism of error control in which it detects and retransmits damaged or lost frames.
<br>**Flow Control:**&nbsp;The data rate must be constant on both sides else the data may get corrupted thus, flow control coordinates the amount of data that can be sent before receiving an acknowledgment.
<br>**Access Control:**&nbsp;When a single communication channel is shared by multiple devices, the MAC sub-layer of the data link layer helps to determine which device has control over the channel at a given time.
<br><br>The network layer works for the transmission of data from one host to the other located in different networks. It also takes care of packet routing i.e. selection of the shortest path to transmit the packet, from the number of routes available. The sender and receiver’s&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-an-ip-address" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-an-ip-address" target="_blank">IP address</a>&nbsp;are placed in the header by the network layer. Segment in the Network layer is referred to as&nbsp;**Packet.**&nbsp;Network layer is implemented by networking devices such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-router-and-switch/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-router-and-switch/" target="_blank">routers and switches</a>.<br><br>
<br>**Routing:**&nbsp;The network layer protocols determine which route is suitable from source to destination. This function of the network layer is known as routing.
<br>**Logical Addressing:**&nbsp;To identify each device inter-network uniquely, the network layer defines an addressing scheme. The sender and receiver’s IP addresses are placed in the header by the network layer. Such an address distinguishes each device uniquely and universally.
<br><br>The transport layer provides services to the application layer and takes services from the network layer. The data in the transport layer is referred to as&nbsp;**Segments**. It is responsible for the end-to-end delivery of the complete message. The transport layer also provides the acknowledgment of the successful data transmission and re-transmits the data if an error is found. Protocols used in Transport Layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-transmission-control-protocol-tcp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-transmission-control-protocol-tcp/" target="_blank">TCP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" target="_blank">UDP</a>&nbsp;&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-netbios-enumeration/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-netbios-enumeration/" target="_blank">NetBIOS</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/pptp-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/pptp-full-form/" target="_blank">PPTP</a>.<br>**At the sender’s side**, the transport layer receives the formatted data from the upper layers, performs&nbsp;**Segmentation**, and also implements&nbsp;**Flow and error control**&nbsp;to ensure proper data transmission. It also adds Source and Destination&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-ports-in-networking" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-ports-in-networking" target="_blank">port number</a>&nbsp;in its header and forwards the segmented data to the Network Layer.<br>
<br>Generally, this destination port number is configured, either by default or manually. For example, when a web application requests a web server, it typically uses port number 80, because this is the default port assigned to web applications. Many applications have default ports assigned.
<br>**At the Receiver’s side,**&nbsp;Transport Layer reads the port number from its header and forwards the Data which it has received to the respective application. It also performs sequencing and reassembling of the segmented data.<br><br>
<br>**Segmentation and Reassembly:**&nbsp;This layer accepts the message from the (session) layer, and breaks the message into smaller units. Each of the segments produced has a header associated with it. The transport layer at the destination station reassembles the message.
<br>**Service Point Addressing:**&nbsp;To deliver the message to the correct process, the transport layer header includes a type of address called service point address or port address. Thus by specifying this address, the transport layer makes sure that the message is delivered to the correct process.
<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-oriented-service" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-oriented-service" target="_blank">Connection-Oriented Service</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-less-service" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-less-service" target="_blank">Connectionless Service</a>
<br><br>Session Layer in the OSI Model is responsible for the establishment of connections, management of connections, terminations of sessions between two devices. It also provides authentication and security. Protocols used in the Session Layer are NetBIOS, PPTP.<br><br>
<br>**Session Establishment, Maintenance, and Termination:**&nbsp;The layer allows the two processes to establish, use, and terminate a connection.
<br>**Synchronization:**&nbsp;This layer allows a process to add checkpoints that are considered synchronization points in the data. These synchronization points help to identify the error so that the data is re-synchronized properly, and ends of the messages are not cut prematurely and data loss is avoided.
<br>**Dialog Controller:**&nbsp;The session layer allows two systems to start communication with each other in half-duplex or full-duplex.
<br>**Example**<br>Let us consider a scenario where a user wants to send a message through some Messenger application running in their browser. The “**Messenger**” here acts as the application layer which provides the user with an interface to create the data. This message or so-called&nbsp;**Data**&nbsp;is compressed, optionally encrypted (if the data is sensitive), and converted into bits (0’s and 1’s) so that it can be transmitted.<br><img alt="Communication in Session Layer" src="https://media.geeksforgeeks.org/wp-content/uploads/20230405124947/communication.jpg" referrerpolicy="no-referrer"><br>Communication in Session Layer<br><br>The presentation layer is also called the&nbsp;**Translation layer**. The data from the application layer is extracted here and manipulated as per the required format to transmit over the network. Protocols used in the Presentation Layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-jpeg-and-png/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-jpeg-and-png/" target="_blank">JPEG</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/mpeg-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/mpeg-full-form/" target="_blank">MPEG</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-a-gif-file/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-a-gif-file/" target="_blank">GIF</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-secure-socket-layer-ssl-and-transport-layer-security-tls/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-secure-socket-layer-ssl-and-transport-layer-security-tls/" target="_blank">TLS/SSL</a>, etc.<br><br>
<br>**Translation:**&nbsp;For example,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-ascii-and-ebcdic" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-ascii-and-ebcdic" target="_blank">ASCII to EBCDIC</a>.
<br>**Encryption/ Decryption:**&nbsp;Data encryption translates the data into another form or code. The encrypted data is known as the ciphertext and the decrypted data is known as plain text. A key value is used for encrypting as well as decrypting data.
<br>**Compression:**&nbsp;Reduces the number of bits that need to be transmitted on the network.
<br><br>At the very top of the OSI Reference Model stack of layers, we find the Application layer which is implemented by the network applications. These applications produce the data to be transferred over the network. This layer also serves as a window for the application services to access the network and for displaying the received information to the user. Protocols used in the Application layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-mail-transfer-protocol-smtp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-mail-transfer-protocol-smtp/" target="_blank">SMTP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp-in-application-layer/" target="_blank">FTP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" target="_blank">DNS</a>, etc.<br><img alt="application-layer-in-OSI" src="https://media.geeksforgeeks.org/wp-content/uploads/20241015103102290937/application-layer.gif" referrerpolicy="no-referrer"><br>Application Layer<br><br>The main functions of the application layer are given below.<br>
<br>**Network Virtual Terminal(NVT):**&nbsp;It allows a user to log on to a remote host.
<br>**File Transfer Access and Management(FTAM):**&nbsp;This application allows a user to access files in a remote host, retrieve files in a remote host, and manage or control files from a remote computer.
<br>**Mail Services:**&nbsp;Provide email service.
<br>**Directory Services:**&nbsp;This application provides distributed database sources and access for global information about various objects and services.
<br><br>When we transfer information from one device to another, it travels through 7 layers of OSI model. First data travels down through 7 layers from the sender’s end and then climbs back 7 layers on the receiver’s end.<br>Data flows through the OSI model in a step-by-step process:<br>
<br>**Application Layer:**&nbsp;Applications create the data.
<br>**Presentation Layer:**&nbsp;Data is formatted and encrypted.
<br>**Session Layer:**&nbsp;Connections are established and managed.
<br>**Transport Layer:**&nbsp;Data is broken into segments for reliable delivery.
<br>**Network Layer**: Segments are packaged into packets and routed.
<br>**Data Link Layer:**&nbsp;Packets are framed and sent to the next device.
<br>**Physical Layer:**&nbsp;Frames are converted into bits and transmitted physically.
<br>Each layer adds specific information to ensure the data reaches its destination correctly, and these steps are reversed upon arrival.<br><img alt="Data Flow in OSI model" src="https://media.geeksforgeeks.org/wp-content/uploads/20210220204638/cn1.png" referrerpolicy="no-referrer"><br>We can understand how data flows through OSI Model with the help of an example mentioned below.<br>Let us suppose,&nbsp;**Person A**&nbsp;sends an e-mail to his friend&nbsp;**Person B**.<br>**Step 1: Person A**&nbsp;interacts with e-mail application like&nbsp;**Gmail**,&nbsp;**outlook**, etc. Writes his email to send. (This happens at&nbsp;**Application Layer**).<br>**Step 2: At Presentation Layer,**&nbsp;Mail application prepares for data transmission like encrypting data and formatting it for transmission.<br>**Step 3: At Session Layer,**&nbsp;There is a connection established between the sender and receiver on the internet.<br>**Step 4: At Transport Layer**, Email data is broken into smaller segments. It adds sequence number and error-checking information to maintain the reliability of the information.<br>**Step 5: At Network Layer,**&nbsp;Addressing of packets is done in order to find the best route for transfer.<br>**Step 6: At Data Link Layer, d**ata packets are encapsulated into frames, then MAC address is added for local devices and then it checks for error using error detection.<br>**Step 7: At Physical Layer,**&nbsp;Frames are transmitted in the form of electrical/ optical signals over a physical network medium like ethernet cable or WiFi.<br>After the email reaches the receiver i.e.&nbsp;**Person B**, the process will reverse and decrypt the e-mail content. At last, the email will be shown on&nbsp;**Person B**&nbsp;email client.<br><br><br><br>The OSI Model matters because it provides the user a clear structure of “how the data moves in the network?”. As the OSI Model consists of 7 layers, each layer has its specific role, and due to which it helps in understanding, identifying and solving the complex network problems easily by focusing on one of the layers not the entire network.<br>As the modern Internet does not prefer the OSI Model, but still, the OSI Model is still very helpful for solving network problems. It helps people understanding network concepts very easily.<br><br><br><img alt="OSI-vs-TCP/IP" src="https://media.geeksforgeeks.org/wp-content/uploads/20240615134832/OSI-vs-TCP.png" referrerpolicy="no-referrer"><br>OSI vs TCP/IP<br><br>The OSI Model defines the communication of a computing system into 7 different layers. Its advantages include:<br>
<br>It divides network communication into 7 layers which makes it easier to understand and troubleshoot.
<br>It standardizes network communications, as each layer has fixed functions and protocols.
<br>Diagnosing network problems is easier with the&nbsp;**OSI model.**
<br>It is easier to improve with advancements as each layer can get updates separately.
<br><br>
<br>The OSI Model has seven layers, which can be complicated and hard to understand for beginners.
<br>In real-life networking, most systems use a simpler model called the Internet protocol suite (TCP/IP), so the OSI Model is not always directly applicable.
<br>Each layer in the OSI Model adds its own set of rules and operations, which can make the process more time-consuming and less efficient.
<br>The OSI Model is more of a theoretical framework, meaning it’s great for understanding concepts but not always practical for implementation.
<br><br>In conclusion, the OSI (Open Systems Interconnection) model helps us understand how data moves in networks. It consists of seven distinct layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. Each layer has specific responsibilities and interacts with the layers directly above and below it. Since it is a conceptual model, but the OSI framework is still widely used to troubleshoot and understand networking issues.<br><br><br>
No, OSI layers do not work independently. Each layer depends on the services provided by the layer below it and, in turn, provides services to the layer above it. This layered approach ensures that data is transmitted smoothly from the source to the destination.
<br><br>
By breaking down communication into layers, the OSI Model helps network administrators isolate problems more easily.
<br><br>
If a particular OSI layer fails, data transmission may be disrupted or fail entirely. Network administrator will check layer by layer to identify and resolve the issue, make sure that each layer is functioning correctly or not.
<br><br>
The Domain Name System (DNS) operates at Layer 7 (Application Layer). It translates domain names into IP addresses, facilitating communication between users and services across the network.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/osi-model.html</link><guid isPermaLink="false">AWS/For Reading/OSI Model.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Software Development Life Cycle (SDLC)]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;20 Nov, 2024<br>**Software development life cycle (SDLC) is a structured process that is used to design, develop, and test good-quality software.**&nbsp;SDLC, or software development life cycle, is a methodology that defines the entire procedure of software development step-by-step. The&nbsp;**goal of the SDLC life cycle model**&nbsp;is to deliver high-quality, maintainable software that meets the user’s requirements. SDLC in software engineering models outlines the plan for each stage so that each stage of the software development model can perform its task efficiently to deliver the software at a low cost within a given time frame that meets users requirements. In this article we will see Software Development Life Cycle (SDLC) in detail.<br><br>**SDLC is a process followed for software building within a software organization.**&nbsp;SDLC consists of a precise plan that describes how to develop, maintain, replace, and enhance specific software. The life cycle defines a method for improving the quality of software and the all-around development process. &nbsp;<br><img alt="SDLC" src="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg" referrerpolicy="no-referrer"><br><br>SDLC specifies the task(s) to be performed at various stages by a software engineer or developer. It ensures that the end product is able to meet the customer’s expectations and fits within the overall budget. Hence, it’s vital for a software developer to have prior knowledge of this software development process.&nbsp;SDLC is a collection of these six stages, and the stages of SDLC are as follows:<br><img alt="Stages of the Software Development Life Cycle Model SDLC" src="https://media.geeksforgeeks.org/wp-content/uploads/20231220112830/6-Stages-of-Software-Development-Life-Cycle.jpg" referrerpolicy="no-referrer"><br>Software Development Life Cycle Model SDLC Stages<br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" target="_blank">*<strong></strong></a>*SDLC Model**&nbsp;**involves six phases or stages**&nbsp;while developing any software.<br><br>Planning is a crucial step in everything, just as in&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-development" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-development" target="_blank">software development</a>. In this same stage,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/activities-involved-in-software-requirement-analysis" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/activities-involved-in-software-requirement-analysis" target="_blank">requirement analysis</a>&nbsp;is also performed by the developers of the organization. This is attained from customer inputs, and sales department/market surveys.&nbsp;<br>The information from this analysis forms the building blocks of a basic project. The quality of the project is a result of planning. Thus, in this stage, the basic project is designed with all the available information.<br><img alt="Stage 1" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094307/1.jpg" referrerpolicy="no-referrer"><br>Stage-1 : Planning and Requirement Analysis<br><br>In this stage, all the requirements for the target software are specified. These requirements get approval from customers, market analysts, and stakeholders.&nbsp;<br>
This is fulfilled by utilizing SRS (Software Requirement Specification). This is a sort of document that specifies all those things that need to be defined and created during the entire project cycle.&nbsp;<br><img alt="Stage-2: Defining Requirements" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094400/2.jpg" referrerpolicy="no-referrer"><br>Stage-2 : Defining Requirements<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-requirement-specification-srs-format" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-requirement-specification-srs-format" target="_blank">SRS</a>&nbsp;is a reference for software designers to come up with the best architecture for the software. Hence, with the requirements defined in SRS, multiple designs for the product architecture are present in the Design Document Specification (DDS).&nbsp;<br>This DDS is assessed by market analysts and stakeholders. After evaluating all the possible factors, the most practical and logical design is chosen for development.<br><img alt="Stage-3: Designing Architecture" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094458/3.jpg" referrerpolicy="no-referrer"><br>Stage 3: Design<br><br>At this stage, the fundamental development of the product starts. For this, developers use a specific programming code as per the design in the DDS. Hence, it is important for the coders to follow the protocols set by the association. Conventional programming tools like compilers, interpreters, debuggers, etc. are also put into use at this stage. Some popular languages like C/C++, Python, Java, etc. are put into use as per the software regulations.&nbsp;<br><img alt="Stage-4: Developing Product" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094538/4.jpg" referrerpolicy="no-referrer"><br>Stage 4: Development<br><br>After the development of the product, testing of the software is necessary to ensure its smooth execution. Although, minimal testing is conducted at every stage of SDLC.&nbsp;Therefore, at this stage, all the probable flaws are tracked, fixed, and retested. This ensures that the product confronts the quality requirements of SRS.&nbsp;<br>**Documentation, Training, and Support:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/overview-software-documentation" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/overview-software-documentation" target="_blank">Software documentation</a>&nbsp;is an essential part of the software development life cycle. A well-written document acts as a tool and means to information repository necessary to know about software processes, functions, and maintenance. Documentation also provides information about how to use the product. Training in an attempt to improve the current or future employee performance by increasing an employee’s ability to work through learning, usually by changing his attitude and developing his skills and understanding.&nbsp;<br><img alt="Stage-5: Product Testing and Integration" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094631/5.jpg" referrerpolicy="no-referrer"><br>Stage 5: Testing<br><br>After detailed testing, the conclusive product is released in phases as per the organization’s strategy. Then it is tested in a real industrial environment. It is important to ensure its smooth performance. If it performs well, the organization sends out the product as a whole. After retrieving beneficial feedback, the company releases it as it is or with auxiliary improvements to make it further helpful for the customers. However, this alone is not enough. Therefore, along with the deployment, the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/product-management" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/product-management" target="_blank">product’s supervision</a>.&nbsp;<br><img alt="Stage-6: Deployment and Maintenance of Products" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094709/6.jpg" referrerpolicy="no-referrer"><br>Stage 6: Deployment and Maintenance<br><br>To this day, we have&nbsp;**more than 50**&nbsp;recognized&nbsp;**SDLC models**&nbsp;in use. But&nbsp;**None of them is perfect**, and each brings its favorable aspects and disadvantages for a specific software development project or a team.<br>Here, we have listed the&nbsp;**top five**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/top-8-software-development-models-used-in-industry/?ref=" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/top-8-software-development-models-used-in-industry/?ref=" target="_blank">*<strong></strong></a>*most popular SDLC models**:<br><br>It is the fundamental model of the software development life cycle. This is a very simple model. The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-classical-waterfall-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-classical-waterfall-model" target="_blank">*<strong></strong></a>*waterfall model**&nbsp;is not in practice anymore, but it is the basis for all other SDLC models. Because of its simple structure, the waterfall model is easier to use and provides a tangible output. In the waterfall model, once a phase seems to be completed, it cannot be changed, and due to this less flexible nature, the waterfall model is not in practice anymore.&nbsp;<br><br>The agile model in SDLC was mainly designed to adapt to changing requests quickly. The main goal of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-agile-development-models" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-agile-development-models" target="_blank">*<strong></strong></a>*Agile model**&nbsp;is to facilitate quick project completion. The agile model refers to a group of development processes. These processes have some similar characteristics but also possess certain subtle differences among themselves.<br><br>In the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-iterative-waterfall-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-iterative-waterfall-model" target="_blank">*<strong></strong></a>*Iterative model**&nbsp;**in SDLC**, each cycle results in a semi-developed but deployable version; with each cycle, some requirements are added to the software, and the final cycle results in the software with the complete requirement specification.&nbsp;<br><br>The spiral model in SDLC is one of the most crucial SDLC models that provides support for risk handling. It has various spirals in its diagrammatic representation; the number of spirals depends upon the type of project. Each loop in the spiral structure indicates the Phases of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-spiral-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-spiral-model" target="_blank">*<strong></strong></a>*Spiral model****.**&nbsp;&nbsp;<br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-sdlc-v-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-sdlc-v-model" target="_blank">*<strong></strong></a>*V-shaped model**&nbsp;**in SDLC**&nbsp;is executed in a sequential manner in V-shape. Each stage or phase of this model is integrated with a testing phase. After every development phase, a testing phase is associated with it, and the next phase will start once the previous phase is completed, i.e., development &amp; testing. It is also known as the verification or validation model.&nbsp;<br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/overview-of-big-bang-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/overview-of-big-bang-model" target="_blank">Big Bang model</a>&nbsp;in SDLC is a term used to describe an informal and unstructured approach to software development, where there is no specific planning, documentation, or well-defined phases.<br><br>SDLC is a method, approach, or process that is followed by a software development organization while developing any software.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" target="_blank">SDLC models</a>&nbsp;were introduced to follow a disciplined and systematic method while designing software. With the software development life cycle, the process of software design is divided into small parts, which makes the problem more understandable and easier to solve. SDLC comprises a detailed description or step-by-step plan for designing, developing, testing, and maintaining the software.<br>
Follow the project <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/library-management-system" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/library-management-system" target="_blank">*<strong></strong></a>*Library Management System** or <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/portfolio-website-project-software-development" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/portfolio-website-project-software-development" target="_blank">*<strong></strong></a>*E Portfolio Website** to see the use of Software Development Life Cycle in a Software Projects.
<br><br>A frequent issue in software development is the delay of security-related tasks until the testing phase, which occurs late in the software development life cycle (SDLC) and occurs after the majority of crucial design and implementation has been finished. During the testing phase, security checks may be minimal and restricted to scanning and penetration testing, which may fail to identify more complicated security flaws.<br>Security issue can be address in SDLC by following DevOps. Security is integrated throughout the whole SDLC, from build to production, through the use of DevSecOps. Everyone involved in the DevOps value chain have responsibility for security under DevSecOps.<br><br>**Developing a banking application using SDLC:**<br>
<br>**Planning and Analysis:**&nbsp;During this stage, business stakeholders’ requirements about the functionality and features of banking application will be gathered by program managers and business analysts. Detailed SRS (<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-requirement-specification-srs-format/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-requirement-specification-srs-format/" target="_blank">Software Requirement Specification</a>) documentation will be produced by them. Together with business stakeholders, business analysts will analyse and approve the SRS document.
<br>**Design:**&nbsp;Developers will receive SRS documentation. Developers will read over the documentation and comprehend the specifications. Web pages will be designed by designers. High level system architecture will be prepared by developers.
<br>**Development:**&nbsp;During this stage, development will code. They will create the web pages and APIs needed to put the feature into practice.
<br>**Testing:**&nbsp;Comprehensive functional testing will be carried out. They will guarantee that the banking platform is glitch-free and operating properly.
<br>**Deployment and Maintenance:**&nbsp;The code will be made available to customers and deployed. Following this deployment, the customer can access the online banking. The same methodology will be used to create any additional features.
<br><br>Choosing the right SDLC (Software Development Life Cycle) model is essential for project success. Here are the key factors to consider:<br>
<br>**Project Requirements:**

<br>**Clear Requirements:**&nbsp;Use&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/waterfall-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/waterfall-model/" target="_blank">*<strong></strong></a>*Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if requirements are well-defined and unlikely to change.
<br>**Changing Requirements:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Iterative**&nbsp;models if requirements are unclear or likely to evolve.


<br>**Project Size and Complexity:**

<br>**Small Projects:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**RAD**&nbsp;for small, simple projects.
<br>**Large Projects:**&nbsp;Use&nbsp;**Agile**,&nbsp;**Spiral**, or&nbsp;**DevOps**&nbsp;for large, complex projects that need flexibility.


<br>**Team Expertise:**

<br>**Experienced Teams:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Scrum**&nbsp;if the team is familiar with iterative development.
<br>**Less Experienced Teams:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;for teams needing structured guidance.


<br>**Client Involvement:**

<br>**Frequent Client Feedback:**&nbsp;Use&nbsp;**Agile**,&nbsp;**Scrum**, or&nbsp;**RAD**&nbsp;if regular client interaction is needed.
<br>**Minimal Client Involvement:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if client involvement is low after initial planning.


<br>**Time and Budget Constraints:**

<br>**Fixed Time and Budget:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if you have strict time and budget limits.
<br>**Flexible Time and Budget:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Spiral**&nbsp;if you can adjust time and budget as needed.


<br>**Risk Management:**

<br>**High-Risk Projects:**&nbsp;Use&nbsp;**Spiral**&nbsp;for projects with significant risks and uncertainties.
<br>**Low-Risk Projects:**&nbsp;Use&nbsp;**Waterfall**&nbsp;for projects with minimal risks.


<br>**Product Release Timeline:**

<br>**Quick Release Needed:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**RAD**&nbsp;to deliver products quickly.
<br>**Longer Development Time:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;for projects with no urgent deadlines.


<br>**Maintenance and Support:**

<br>**Long-Term Maintenance:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**DevOps**&nbsp;for projects needing continuous updates and support.
<br>**Minimal Maintenance:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if little future maintenance is expected.


<br>**Stakeholder Expectations:**

<br>**High Stakeholder Engagement:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Scrum**&nbsp;if stakeholders want ongoing involvement.
<br>**Low Stakeholder Engagement:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if stakeholders prefer involvement only at major milestones.


<br>**Note:**<br>

<br>**Waterfall**: Best for clear, stable projects with minimal changes.
<br>**V-Model**: Good for projects with clear requirements and a strong focus on testing.
<br>**Agile/Scrum**: Ideal for projects with changing requirements and frequent client interaction.
<br>**Spiral**: Suitable for high-risk projects with evolving requirements.
<br>**RAD**: Useful for projects needing rapid development.
<br>**DevOps**: Best for continuous integration and ongoing support

<br><br>In conclusion, we now know that the&nbsp;**Software Development Life Cycle (SDLC) in software engineering is an important framework for the better and more structured development of optimized software programs.**&nbsp;In a world full of rapid evolution in technology, SDLC phases plays a crucial role in enabling some good and innovative solutions for helping users and organizations. Also, it’s better to adapt SDLC principles to achieve software development goals effectively.<br><br><br>(A) Spiral model<br>(B) Prototyping model<br>(C) Waterfall model<br>(D) Capability maturity model<br>
**Solution:**&nbsp;The correct Answer is&nbsp;**(D)**.
<br><br><br>(A) P-3, Q-2, R-4, S-1<br>(B) P-2, Q-3, R-1, S-4<br>(C) P-3, Q-2, R-1, S-4<br>(D) P-2, Q-3, R-4, S-1<br>
**Solution:**&nbsp;The correct Answer is&nbsp;**(B)**.
<br><br><br>
The SDLC involves planning the project, gathering requirements, designing the system, coding the software, testing it for issues, deploying it to users, and maintaining it post-release. Each phase ensures the software meets user needs and functions correctly, from start to finish.
<br><br>
The main phases of SDLC include Requirements, Design, Implementation (Coding), Testing, Deployment, and Maintenance. These phases represent the stages a software project goes through from initiation to completion.
<br><br>
SDLC ensures a structured and organized approach to software development, leading to the creation of reliable and high-quality software. It helps manage resources efficiently, reduces development time, and minimizes the risk of project failure.
<br><br>
The key objectives of SDLC include delivering a high-quality product, meeting customer requirements, managing project resources effectively, minimizing risks, and providing a clear and transparent development process.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/sdlc-software-developement-lifecycle.html</link><guid isPermaLink="false">AWS/For Reading/SDLC - Software Developement LifeCycle.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Some talks about git in reddit 1]]></title><description><![CDATA[ 
 <br><br>[reddit user]<br>
git&nbsp;is a program that is used to track changes in code. It's very useful and allows you to "go back in time" if something is broken and see what's changed which might have caused it to break. It's also very handy when several people work on the same code together, they can share their changes easily.
GitHub is a website, now owned by Microsoft, that you can use with git. There are other websites like it, e.g. GitLab. All of them allow you to put your changes on their servers to make it easier to share them with other people. They may also have other features like more advanced permissions so you can only share your code with specific people, or build platforms where you can automatically build your program after you've pushed new code to them.
<br>[reddit user]<br>
Just in case that is still a bit dense for you or your current computer knowledge a simpler version is:
Linus Torvalds, the man who Linux is named after (Linus + Unix = Linux) made a manager for managing the revisions of computer programming he was doing making the Linux Kernel. Every different thing he tried to do to solve for how to make something one way, or another or the third way was saved. if he had to go back and change from the third way back to the second or first as all saved unlike usual computer coding which wouldn't save that.
Making fun of himself that he needed this helpful feature he called it "git" after the British Slang for someone being a dummy. Now some people prefer to think of it as "Go-back In Time" instead of thinking of the creator of the Linux Kernel as 'dumb'.
Now Git was so useful, and like Linux, Open Source and Free, and the more it was developed by the community the more powerful, and to some people, seen as a required tool for doing programming projects.
What was shared versions of work stored locally, called versioning, would be stored in folders of single coders work, or multiple coders putting their proposed version(s) for a piece of code, or multiple people sharing and collaborating on the same code. The folders for these were called repositories. Later the finished/published pieces of code would be published to a folder which was also called a repository. When you install software on Linux or just using package managers in general, they largely use collections of published repositories either by pointing at all those final version repositories or by collecting copies of them on a central server called itself a repository, as in a repository of repositories.
Now this is where GitHub and its siblings/cousins come in. They can act as an online collection of not just the published version of the repositories, but a central place for people to collaborate with Git online, store the work away from the person's local computer alone, preventing loss of data, and even act as a place users can come to learn about and install and get help with the published program/code.
In short you use Git to help you develop code and track all the versions of your work over time.<br>
Git is so ubiquitous that the format it finishes off the code in can be used to distribute to code and update it later.
GitHub (and GitLab, SourceForge, etc.) are places where code project can be worked on and collaborated with that also distribute and help creators support users that is done with Git and Git related software.
Repositories is just a Git related Jargon for the folders that hold the different versions of a Git Project, but when you are talking Package Managers or Code Sites like GitHub, they are talking about the stored published versions of the code that people can download, install, and use.
To you the end user just think of repositories as "Folders of Code" be it programs or extensions or mods or even collections of settings and that it is just jargon.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/some-talks-about-git-in-reddit-1.html</link><guid isPermaLink="false">AWS/For Reading/Some talks about git in reddit 1.md</guid><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[ssl certificate]]></title><description><![CDATA[ 
 <br><br>An SSL certificate is a digital certificate that authenticates a website's identity and enables an encrypted connection. SSL stands for Secure Sockets Layer, a security protocol that creates an encrypted link between a web server and a web browser.<br>Companies and organizations need to add SSL certificates to their websites to secure online transactions and keep customer information private and secure.<br>In short: SSL keeps internet connections secure and prevents criminals from reading or modifying information transferred between two systems. When you see a padlock icon next to the URL in the address bar, that means SSL protects the website you are visiting.<br>Since its inception about 25 years ago, there have been several versions of SSL protocol, all of which at some point ran into security troubles. A revamped and renamed version followed — TLS (Transport Layer Security), which is still in use today. However, the initials SSL stuck, so the new version of the protocol is still usually called by the old name.<br><br>SSL works by ensuring that any data transferred between users and websites, or between two systems, remains impossible to read. It uses encryption algorithms to scramble data in transit, which prevents hackers from reading it as it is sent over the connection. This data includes potentially sensitive information such as names, addresses, credit card numbers, or other financial details.<br>The process works like this:<br>
<br>A browser or server attempts to connect to a website (i.e., a web server) secured with SSL.
<br>The browser or server requests that the web server identifies itself.
<br>The web server sends the browser or server a copy of its SSL certificate in response.
<br>The browser or server checks to see whether it trusts the SSL certificate. If it does, it signals this to the webserver.
<br>The web server then returns a digitally signed acknowledgment to start an SSL encrypted session.
<br>Encrypted data is shared between the browser or server and the webserver.
<br>This process is sometimes referred to as an "SSL handshake." While it sounds like a lengthy process, it takes place in milliseconds.<br>When a website is secured by an SSL certificate, the acronym HTTPS (which stands for HyperText Transfer Protocol Secure) appears in the URL. Without an SSL certificate, only the letters HTTP – i.e., without the S for Secure – will appear. A padlock icon will also display in the URL address bar. This signals trust and provides reassurance to those visiting the website.<br>To view an SSL certificate's details, you can click on the padlock symbol located within the browser bar. Details typically included within SSL certificates include:<br>
<br>The domain name that the certificate was issued for
<br>Which person, organization, or device it was issued to
<br>Which Certificate Authority issued it
<br>The Certificate Authority's digital signature
<br>Associated subdomains
<br>Issue date of the certificate
<br>The expiry date of the certificate
<br>The public key (the private key is not revealed)
<br><br>Websites need SSL certificates to keep user data secure, verify ownership of the website, prevent attackers from creating a fake version of the site, and convey trust to users.<br>If a website is asking users to sign in, enter personal details such as their credit card numbers, or view confidential information such as health benefits or financial information, then it is essential to keep the data confidential. SSL certificates help keep online interactions private and assure users that the website is authentic and safe to share private information with.<br>More relevant to businesses is the fact that an SSL certificate is required for an HTTPS web address. HTTPS is the secure form of HTTP, which means that HTTPS websites have their traffic encrypted by SSL. Most browsers tag HTTP sites – those without SSL certificates – as "not secure." This sends a clear signal to users that the site may not be trustworthy – incentivizing businesses who have not done so to migrate to HTTPS.<br>An SSL certificate helps to secure information such as:<br>
<br>Login credentials
<br>Credit card transactions or bank account information
<br>Personally identifiable information — such as full name, address, date of birth, or telephone number
<br>Legal documents and contracts
<br>Medical records
<br>Proprietary information
<br><br>There are different types of SSL certificates with different validation levels. The six main types are:<br>
<br>Extended Validation certificates (EV SSL)
<br>Organization Validated certificates (OV SSL)
<br>Domain Validated certificates (DV SSL)
<br>Wildcard SSL certificates
<br>Multi-Domain SSL certificates (MDC)
<br>Unified Communications Certificates (UCC)
<br><br>This is the highest-ranking and most expensive type of SSL certificate. It tends to be used for high profile websites which collect data and involve online payments. When installed, this SSL certificate displays the padlock, HTTPS, name of the business, and the country on the browser address bar. Displaying the website owner's information in the address bar helps distinguish the site from malicious sites. To set up an EV SSL certificate, the website owner must go through a standardized identity verification process to confirm they are authorized legally to the exclusive rights to the domain.<br><br>This version of SSL certificate has a similar assurance similar level to the EV SSL certificate since to obtain one; the website owner needs to complete a substantial validation process. This type of certificate also displays the website owner's information in the address bar to distinguish from malicious sites. OV SSL certificates tend to be the second most expensive (after EV SSLs), and their primary purpose is to encrypt the user's sensitive information during transactions. Commercial or public-facing websites must install an OV SSL certificate to ensure that any customer information shared remains confidential.<br><br>The validation process to obtain this SSL certificate type is minimal, and as a result, Domain Validation SSL certificates provide lower assurance and minimal encryption. They tend to be used for blogs or informational websites – i.e., which do not involve data collection or online payments. This SSL certificate type is one of the least expensive and quickest to obtain. The validation process only requires website owners to prove domain ownership by responding to an email or phone call. The browser address bar only displays HTTPS and a padlock with no business name displayed.<br><br>Wildcard SSL certificates allow you to secure a base domain and unlimited sub-domains on a single certificate. If you have multiple sub-domains to secure, then a Wildcard SSL certificate purchase is&nbsp;much&nbsp;less expensive than buying individual SSL certificates for each of them. Wildcard SSL certificates have an asterisk  as part of the common name, where the asterisk represents any valid sub-domains that have the same base domain. For example, a single Wildcard certificate for website can be used to secure:<br>
<br>payments.yourdomain.com
<br>login.yourdomain.com
<br>mail.yourdomain.com
<br>download.yourdomain.com
<br>anything.yourdomain.com
<br><br>A Multi-Domain certificate can be used to secure many domains and/or sub-domain names. This includes the combination of completely unique domains and sub-domains with different TLDs (Top-Level Domains) except for local/internal ones.<br>For example:<br>
<br><a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a>
<br>example.org
<br>mail.this-domain.net
<br>example.anything.com.au
<br>checkout.example.com
<br>secure.example.org
<br>Multi-Domain certificates do not support sub-domains by default. If you need to secure both <a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a> and example.com with one Multi-Domain certificate, then both hostnames should be specified when obtaining the certificate.<br><br>Unified Communications Certificates (UCC) are also considered Multi-Domain SSL certificates. UCCs were initially designed to secure Microsoft Exchange and Live Communications servers. Today, any website owner can use these certificates to allow multiple domain names to be secured on a single certificate. UCC Certificates are organizationally validated and display a padlock on a browser. UCCs can be used as EV SSL certificates to give website visitors the highest assurance through the green address bar.<br>It is essential to be familiar with the different types of SSL certificates to obtain the right type of certificate for your website.<br><br>SSL certificates can be obtained directly from a Certificate Authority (CA). Certificate Authorities – sometimes also referred to as Certification Authorities – issue millions of SSL certificates each year. They play a critical role in how the internet operates and how transparent, trusted interactions can occur online.<br>The cost of an SSL certificate can range from free to hundreds of dollars, depending on the level of security you require. Once you decide on the type of certificate you require, you can then look for Certificate Issuers, which offer SSLs at the level you require.<br>Obtaining your SSL involves the following steps:<br>
<br>Prepare by getting your server set up and ensuring your&nbsp;<a data-tooltip-position="top" aria-label="https://www.whois.com/" rel="noopener nofollow" class="external-link" href="https://www.whois.com/" target="_blank">WHOIS</a>&nbsp;record is updated and matches what you are submitting to the Certificate Authority (it needs to show the correct company name and address, etc.)
<br>Generating a Certificate Signing Request (CSR) on your server. This is an action your hosting company can assist with.
<br>Submitting this to the Certificate Authority to validate your domain and company details
<br>Installing the certificate they provide once the process is complete.
<br>Once obtained, you need to configure the certificate on your web host or on your own servers if you host the website yourself.<br>How quickly you receive your certificate depends on what type of certificate you get and which certificate provider you procure it from. Each level of validation takes a different length of time to complete. A simple Domain Validation SSL certificate can be issued within minutes of being ordered, whereas Extended Validation can take as long as a full week.<br><img alt="Types of SSL certificates" src="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg" referrerpolicy="no-referrer"><br><br>It is possible to use one SSL certificate for multiple domains on the same server. Depending on the vendor, you can also use one SSL certificate on multiple servers. This is because of Multi-Domain SSL certificates, which we discussed above.<br>As the name implies, Multi-Domain SSL Certificates work with multiple domains. The number is left up to the specific issuing Certificate Authority. A Multi-Domain SSL Certificate is different from a Single Domain SSL Certificate, which – again, as the name implies – is designed to secure a single domain.<br>To make matters confusing, you may hear Multi-Domain SSL Certificates, also referred to as SAN certificates. SAN stands for Subject Alternative Name. Every multi-domain certificate has additional fields (i.e., SANs), which you can use to list additional domains that you want to cover under one certificate.<br>Unified Communications Certificates (UCCs) and Wildcard SSL Certificates also allow for multi-domains and, in the latter case, an unlimited number of subdomains.<br><br>SSL certificates do expire; they don't last forever.&nbsp;<a data-tooltip-position="top" aria-label="https://cabforum.org/" rel="noopener nofollow" class="external-link" href="https://cabforum.org/" target="_blank">The Certificate Authority/Browser Forum</a>, which serves as the de facto regulatory body for the SSL industry, states that SSL certificates should have a lifespan of&nbsp;<a data-tooltip-position="top" aria-label="https://cabforum.org/uploads/CA-Browser-Forum-BR-1.6.0.pdf" rel="noopener nofollow" class="external-link" href="https://cabforum.org/uploads/CA-Browser-Forum-BR-1.6.0.pdf" target="_blank">no more than 27 months</a>. This essentially means two years plus you can carry over up to three months if you renew with time remaining on your previous SSL certificate.<br>SSL certificates expire because, as with any form of authentication, information needs to be periodically re-validated to check it is still accurate. Things change on the internet, as companies and also websites are bought and sold. As they change hands, the information relevant to SSL certificates also changes. The purpose of the expiry period is to ensure that the information used to authenticate servers and organizations is as up-to-date and accurate as possible.<br>Previously, SSL certificates could be issued for as long as five years, which was subsequently reduced to three and most recently to two years plus a potential extra three months. In 2020, Google, Apple, and Mozilla announced&nbsp;<a data-tooltip-position="top" aria-label="https://techbeacon.com/security/google-apple-mozilla-enforce-1-year-max-security-certifications" rel="noopener nofollow" class="external-link" href="https://techbeacon.com/security/google-apple-mozilla-enforce-1-year-max-security-certifications" target="_blank">they would enforce one-year SSL certificates</a>, despite this proposal being voted down by the Certificate Authority Browser Forum. This took effect from September 2020. It is possible that in the future, the length of validity will reduce still further.<br>When an SSL certificate expires, it makes the site in question unreachable. When a user's browser arrives at a website, it checks the SSL certificate's validity within milliseconds (as part of the SSL handshake). If the SSL certificate has expired, visitors will receive a message to the effect of — "This site is not secure. Potential risk ahead".<br>While users do have the option to proceed, it is not advisable to do so, given the cybersecurity risks involved, including the possibility of&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/resource-center/threats/malware-protection" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/resource-center/threats/malware-protection" target="_blank">malware</a>. This will significantly impact bounce rates for website owners, as users rapidly click off the homepage and go elsewhere.<br>Keeping on top of when SSL certificates expire presents a challenge for larger businesses. While smaller and&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/small-to-medium-business-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/small-to-medium-business-security" target="_blank">medium-sized businesses (SMEs)</a>&nbsp;may have one or only a few certificates to manage,&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/enterprise-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/enterprise-security" target="_blank">enterprise-level organizations</a>&nbsp;that potentially transact across markets – with numerous websites and networks – will have many more. At this level, allowing an SSL certificate to expire is usually the result of oversight rather than incompetence. The best way for larger businesses to stay on top of when their SSL certificates expire is by using a certificate management platform. There are various products on the market, which you can find using an online search. These allow enterprises to see and manage digital certificates across their entire infrastructure. If you do use one of these platforms, it is important to log in regularly so you can be aware of when renewals are due.<br>If you allow a certificate to expire, the certificate becomes invalid, and you will no longer be able to run secure transactions on your website. The Certification Authority (CA) will prompt you to renew your SSL certificate before the expiration date.<br>Whichever Certificate Authority or SSL service you use to obtain your SSL certificates from will send you expiration notifications at set intervals, usually starting at 90 days out. Try to ensure that these reminders are being sent to an email distribution list — rather than a single individual, who may have left the company or moved to another role by the time the reminder is sent. Think about which stakeholders in your company are on this distribution list to ensure the right people see the reminders at the right time.<br><br>The easiest way to see if a site has an SSL certificate is by looking at the address bar in your browser:<br>
<br>If the URL begins with HTTPS instead of HTTP, that means the site is secured using an SSL certificate.
<br>Secure sites show a closed padlock emblem, which you can click on to see security details – the most trustworthy sites will have green padlocks or address bars.
<br>Browsers also show warning signs when a connection is not secure — such as a red padlock, a padlock which is not closed, a line going through the website's address, or a warning triangle on top of the padlock emblem.
<br><br>Only submit your personal data and online payment details to websites with EV or OV certificates. DV certificates are not suitable for eCommerce websites. You can tell if a site has an EV or OV certificate by looking at the address bar. For an EV SSL, the organization's name will be visible in the address bar itself. For an OV SSL, you can see the organization's name's details by clicking on the padlock icon. For a DV SSL, only the padlock icon is visible.<br>Read the website's privacy policy. This enables you to see how your data will be used. Legitimate companies will be transparent about how they collect your data and what they do with it.<br>Look out for trust signals or indicators on websites.<br>
As well as SSL certificates, these include reputable logos or badges which show the website meets specific security standards. Other signs that can help you determine if a site is real or not include checking for a physical address and telephone number, checking their returns or refunds policy, and making sure prices are believable and not too good to be true.<br>Stay alert to phishing scams.<br>
Sometimes cyber attackers create websites that mimic existing websites to trick people into purchasing something or logging in to their phishing site. It is possible for a&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/resource-center/preemptive-safety/phishing-prevention-tips" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/resource-center/preemptive-safety/phishing-prevention-tips" target="_blank">phishing</a>&nbsp;site to obtain an SSL certificate and therefore encrypt all the traffic that flows between you and it. A growing proportion of phishing scams occur on HTTPS sites — deceiving users who feel reassured by the padlock icon's presence.<br>To avoid these kinds of attacks:<br>
<br>Always examine the domain of the site you are on and ensure it is spelled correctly. The URL of a fake site might differ by only one character – e.g., amaz0n.com instead of amazon.com. If in doubt, type the domain directly into your browser to make sure you are connecting to the website you intend to visit.
<br>Never enter logins, passwords, banking credentials, or any other personal information on the site unless you are sure of its authenticity.
<br>Always consider what a particular site is offering, whether it looks suspicious, and whether you really need to register on it.
<br>Make sure your devices are well protected:&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/internet-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/internet-security" target="_blank">Kaspersky Internet Security</a>&nbsp;checks URLs against an extensive database of phishing sites, and it detects scams regardless of how "safe" the resource looks.
<br>Cybersecurity risks continue to evolve but understanding the types of SSL certificates to look out for and how to distinguish a safe site from a potentially dangerous one will help internet users avoid scams and protect their personal data from cybercriminals.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/ssl-certificate.html</link><guid isPermaLink="false">AWS/For Reading/ssl certificate.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Amazon S3 Storage Classes]]></title><description><![CDATA[ 
 <br><br><br>Amazon S3 offers a range of storage classes that you can choose from based on the performance, data access, resiliency, and cost requirements of your workloads. S3 storage classes are purpose-built to provide the lowest cost storage for different access patterns. S3 storage classes are ideal for virtually any use case, including those with demanding performance needs, data lakes, data residency requirements, unknown or changing access patterns, or archival storage.<br>The S3 storage classes include&nbsp;S3 Intelligent-Tiering&nbsp;for automatic cost savings for data with unknown or changing access patterns,&nbsp;S3 Standard&nbsp;for frequently accessed data,&nbsp;S3 Express One Zone&nbsp;for your most frequently accessed data,&nbsp;S3 Standard-Infrequent Access (S3 Standard-IA)&nbsp;and&nbsp;S3 One Zone-Infrequent Access (S3 One Zone-IA)&nbsp;for less frequently accessed data,&nbsp;S3 Glacier Instant Retrieval&nbsp;for archive data that needs immediate access,&nbsp;S3 Glacier Flexible Retrieval (formerly S3 Glacier)&nbsp;for rarely accessed long-term data that does not require immediate access, and&nbsp;Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive)&nbsp;for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud.<br>Amazon S3 provides the most durable storage in the cloud. Based on its unique architecture, S3 is designed to exceed 99.999999999% (11 nines) data durability. Additionally, S3 stores data redundantly across a minimum of 3 Availability Zones by default, providing built-in resilience against widespread disaster. Customers can store data in a single AZ to minimize storage cost or latency, in multiple AZs for resilience against the permanent loss of an entire data center, or in multiple AWS Regions to meet geographic resilience requirements. If you have data residency requirements that can’t be met by an existing AWS Region, you can use S3 storage classes for AWS Dedicated Local Zones or S3 on Outposts racks to store your data in a specific data perimeter.&nbsp;<br>You can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. Amazon S3 also offers capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application. S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone, and do not support S3 Lifecycle transitions.<br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes-infographic/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes-infographic/" target="_blank">View the Amazon S3 storage classes overview infographic.</a><br><br><br>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.&nbsp;<br>Key features:<br>
<br>General purpose storage for frequently accessed data
<br>Low latency and high throughput performance
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br><br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/" target="_blank"><strong></strong></a>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)&nbsp;is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. S3 Intelligent-Tiering delivers milliseconds latency and high throughput performance for frequently, infrequently, and rarely accessed data in the Frequent, Infrequent, and Archive Instant Access tiers. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.<br>For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier that is optimized for frequent access, a 40% lower-cost tier that is optimized for infrequent access, and a 68% lower-cost tier optimized for rarely accessed data. S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier. For data that does not require immediate retrieval, you can set up S3 Intelligent-Tiering to monitor and automatically move objects that aren’t accessed for 180 days or more to the Deep Archive Access tier to realize up to 95% in storage cost savings.<br>There are no retrieval charges in S3 Intelligent-Tiering. If an object in the Infrequent or Archive Instant Access tier is accessed later, it’s automatically moved back to the Frequent Access tier. If the object you’re retrieving is stored in the optional Deep Archive tiers, before you can retrieve the object, you must first restore a copy using RestoreObject. &nbsp;For information about restoring archived objects, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html" target="_blank">Restoring Archived Objects</a>.&nbsp;No additional tiering charges apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class.<br>Key features:<br>
<br>Automatic cost savings for data with unknown or changing access patterns
<br>Frequent, Infrequent, and Archive Instant Access tiers have the same low-latency and high-throughput performance of S3 Standard
<br>The Infrequent Access tier saves up to 40% on storage costs
<br>The Archive Instant Access tier saves up to 68% on storage costs
<br>Opt-in asynchronous archive capabilities for objects that become rarely accessed
<br>Deep Archive Access tier has the same performance as Glacier Deep Archive and saves up to 95% for rarely accessed objects
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br>Small monthly monitoring and&nbsp;automation charge
<br>No operational overhead, no lifecycle charges, no retrieval charges, and no minimum storage duration
<br>Objects smaller than 128KB can be stored in S3 Intelligent-Tiering but will always be charged at the Frequent Access tier rates, and are not charged the monitoring and automation charge.
<br><br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/express-one-zone/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/express-one-zone/" target="_blank"><strong></strong></a>Amazon S3 Express One Zone&nbsp;is a high-performance, single-Availability Zone storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications.&nbsp;S3 Express One Zone can improve data access speeds by 10x and reduce request costs by 50% compared to S3 Standard. While you have always been able to choose a specific AWS Region to store your S3 data, with S3 Express One Zone you can select a specific AWS Availability Zone within an AWS Region to store your data. You can choose to co-locate your storage and compute resources in the same Availability Zone to further optimize performance, which helps lower compute costs and run workloads faster. With S3 Express One Zone, data is stored in a different bucket type—an Amazon S3 directory bucket—which supports hundreds of thousands of requests per second. Additionally, you can use S3 Express One Zone with services such as&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/sagemaker/train/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/sagemaker/train/" target="_blank">Amazon SageMaker Model Training</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/athena/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/athena/" target="_blank">Amazon Athena</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/emr/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/emr/" target="_blank">Amazon EMR</a>, and&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/glue/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/glue/" target="_blank">AWS Glue</a>&nbsp;Data Catalog to accelerate your ML and analytics workloads. With S3 Express One Zone, storage automatically scales up or down based on your consumption and need, and you no longer need to manage multiple storage systems for low-latency workloads.<br>Key features:<br>
<br>High performance storage for your most frequently accessed data
<br>Consistent single-digit millisecond request latency
<br>Improve access speeds by 10x and reduce request costs by 50% compared to S3 Standard
<br>Select an AWS Availability Zone and have the option to co-locate storage and compute resources for even lower latency, with reduced processing time and more efficient use of compute resources contributing to lower overall total cost of ownership
<br>Accelerate analytics and ML workloads with AWS service integrations
<br>Scale to handle millions of requests per minute
<br>Optimized for large datasets with many small objects
<br>Use existing Amazon S3 APIs with different bucket type – directory buckets
<br>Designed to deliver 99.95% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br><br><br>S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. You can configure S3 storage classes at the object level, and a single bucket can contain objects stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.<br>Key features:<br>
<br>Infrequently accessed data that needs millisecond access
<br>Same low latency and high throughput performance of S3 Standard
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br><br>S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. It’s a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 Cross-Region Replication.<br>S3 One Zone-IA offers the same high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. Using similar engineering designs as S3 Regional storage classes, S3 One Zone-IA also offers 11 nines of durability, but may be susceptible to data loss in the unlikely case of the loss or damage to all or part of an AWS Availability Zone. You can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.<br>Key features:<br>
<br>Re-creatable infrequently accessed data
<br>Same low latency and high throughput performance of S3 Standard
<br>Designed to deliver 99.5% availability with an availability SLA of 99%
<br><br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes&nbsp;are purpose-built for data archiving, and are designed to provide you with the highest performance, the most retrieval flexibility, and the lowest cost archive storage in the cloud. You can choose from three archive storage classes optimized for different access patterns and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5—12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval from 12—48 hours.<br><br>Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. With S3 Glacier Instant Retrieval, you can save up to 68% on storage costs compared to using the S3 Standard-Infrequent Access (S3 Standard-IA) storage class, when your data is accessed once per quarter. S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives. You can upload objects directly to S3 Glacier Instant Retrieval, or use S3 Lifecycle policies to transfer data from the S3 storage classes. For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/" target="_blank"><strong></strong></a>Amazon S3 Glacier Instant Retrieval page »<br>Key features:<br>
<br>Long-lived data that is accessed a few times per year with instant retrievals
<br>Data retrieval in milliseconds with the same performance as S3 Standard
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br>128 KB minimum object size
<br>S3 PUT API for direct uploads to S3 Glacier Instant Retrieval, and S3 Lifecycle management for automatic migration of objects
<br><br>S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1—2 times per year and is retrieved asynchronously. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, S3 Glacier Flexible Retrieval (formerly S3 Glacier) is the ideal storage class. S3 Glacier Flexible Retrieval delivers the most flexible retrieval options that balance cost with access times ranging from minutes to hours and with free bulk retrievals. It is an ideal solution for backup, disaster recovery, offsite data storage needs, and for when some data&nbsp;occasionally need to be retrieved in minutes, and you don’t want to worry about costs. S3 Glacier Flexible Retrieval is designed for 99.999999999% (11 nines) of data durability and 99.99% availability by redundantly storing data across multiple physically separated AWS Availability Zones in a given year. For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes page »<br>Key features:<br>
<br>Backup and archive data that is rarely accessed and low cost
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br>Supports SSL for data in transit and encryption of data at rest
<br>Ideal for backup and disaster recovery use cases when large sets of data&nbsp;occasionally need to be retrieved in minutes, without concern for costs
<br>Configurable retrieval times, from minutes to hours, with free bulk retrievals
<br>S3 PUT API for direct uploads to S3 Glacier Flexible Retrieval, and S3 Lifecycle management for automatic migration of objects
<br><br>S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers—particularly those in highly-regulated industries, such as financial services, healthcare, and public sectors—that retain data sets for 7—10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. All objects stored in S3 Glacier Deep Archive are replicated and stored across at least three geographically-dispersed Availability Zones, protected by 99.999999999% of durability, and can be restored within 12 hours.&nbsp;For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes page »<br>Key features:<br>
<br>Archive data that is very rarely accessed and very low cost
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br>Ideal alternative to magnetic tape libraries
<br>Retrieval time within 12 hours
<br>S3 PUT API for direct uploads to S3 Glacier Deep Archive, and S3 Lifecycle management for automatic migration of objects
<br><br><br>In AWS Dedicated Local Zones, the S3 Express One Zone and S3 One Zone-Infrequent access storage classes are purpose-built to store data in a specific data perimeter to support your data isolation and data residency use cases. Dedicated Local Zones are a type of AWS infrastructure that is fully managed by AWS, built for exclusive use by you or your community, and placed in a location or data center specified by you to help you comply with regulatory requirements. Both storage classes store data in a single Dedicated Local Zone and are supported in directory buckets. S3 supports the AWS Management Console, AWS SDKs, and S3 APIs, so you can run S3 based applications in Dedicated Local Zones. S3 Express One Zone is a high-performance storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications. Amazon S3 One Zone-Infrequent Access is designed for data that is accessed less frequently and is ideal for backups.&nbsp;<br>Key features:<br>
<br>Store S3 objects in a specific data perimeter
<br>Enforce security within a data perimeter using AWS Identity and Access Management (IAM)
<br>Audit bucket and object-level access for governance and compliance use cases with AWS CloudTrail
<br>Designed to durably and redundantly store data in a single Dedicated Local Zone
<br><br>Amazon S3 on Outposts delivers object storage to your on-premises AWS Outposts environment. Using the S3 APIs and features available in AWS Regions today, S3 on Outposts makes it easy to store and retrieve data on your Outpost, as well as secure the data, control access, tag, and report on it. S3 on Outposts provides a single Amazon S3 storage class, named 'OUTPOSTS', which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your Outposts. The S3 Outposts storage class is ideal for workloads with local data residency requirements, and to satisfy demanding performance needs by keeping data close to on-premises applications.<br>Key features:<br>
<br>Store S3 objects in your on-premises AWS Outposts environment&nbsp;
<br>S3 Object compatibility and bucket management through the S3 SDK
<br>Designed to durably and redundantly store data on your AWS Outposts rack
<br>Encryption using SSE-S3 and SSE-C
<br>Authentication and authorization using IAM&nbsp;and S3 Access Points
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/storage-classes-in-s3.html</link><guid isPermaLink="false">AWS/For Reading/Storage Classes in S3.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Subnetting in Computer Networks]]></title><description><![CDATA[ 
 <br><br>source: <a rel="noopener nofollow" class="external-link" href="https://www.javatpoint.com/subnetting-in-computer-networks" target="_blank">https://www.javatpoint.com/subnetting-in-computer-networks</a><br>In this tutorial, we will learn about Subnetting in Computer Networks Subject. First and foremost, the most crucial concept we are going to learn today is that while studying the subject of computer networks is subnetting. The most crucial idea known as Subnetting will help to lessen or disperse the pressure that the networks' heavy load causes. Let's now quickly go through the idea of subnetting for everyone.<br>Now, let us know the definition of Subnetting. But before going into the Subnetting concept let us know the overview of the concept named Subnetting in Computer Networks.<br>Subnetting is a part of Network Layer. The duty of the network layer is to divide the received message into separate components and activities. The Network layer can be called as the heart of Computer Networks.<br><br>Finding a network and delivering data to it was simpler when the IP (Internet Protocol) system was initially implemented since there were fewer individuals online. Sending a data packet to the desired machine in a network is getting more and more challenging these days due to the rise in internet users. Network performance becomes a major issue once a network is large enough to serve an enterprise.<br>In order to divide larger networks logically (firewalls, etc.) or physically (for example), an organization can employ IP subnets (smaller broadcast domains, etc.). To put it another way, routers base their routing choices on subnets. We shall learn more about these ideas in this post.<br><br>Subnetting is a combination of two words i.e. Sub and Netting. Here Sub word means Substitute and netting word means Network. The Substitute Network created for a function to happen is known as Subnetting.<br>Here, Substitute Network does not mean a new network is created. A full piece of network is broken into small pieces and each piece a different is assigned.<br>Subnet is the name given to piece of the broken network or can also be called as the Substitute network is known as Subnet. Subnets are the legal small parts of IP (Internet Protocol) Addressing process<br>Subnetting should be done in such a way that network does not gets affected. This means that we can divide the network into different parts but all when put together should perform the same task when done before splitting in to small parts.<br>Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed<br>Subnetting is a technique for creating logical sub-networks from a single physical network (subnets). A company can grow its network via subnetting without asking for a new network number from its ISP. Subnetting hides network complexity while assisting in the reduction of network traffic. Here, a network which is unique has to provide its services to many Local Area Networks i.e. (LAN). So, for this reason Subnetting is extensively used.<br>Do you know what these little subnets are? As we all know, subnetting divides networks into them. A subnet is a smaller network, also referred to as a sub network. An IP network is logically divided into several smaller network components by subnets. A subnet is used to divide a large network into a number of smaller, linked networks, which helps to minimize traffic. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed.<br>A rapid, effective, and reliable computer network is what subnetting is meant to create. Network traffic must find more effective routes as they become larger and more complicated. If all network traffic used the same path and moved through the system at once, bottlenecks and congestion would form, creating sluggish and inefficient backlogs. You may reduce the number of routers that network traffic must transit through by setting up a subnet. In order to make traffic go the shortest distance feasible inside a bigger network, an engineer will effectively create smaller mini routes.<br><br>
<br>Efficiency of the Network
<br>By removing the need for extra routers, subnetting makes network traffic simpler. This makes sure the data being transmitted can get to its destination as fast as possible, eliminating or avoiding any potential diversions that may slow it down.<br>
<br>Provides Network Security
<br>By isolating or removing vulnerable network regions and making it harder for intruders to move through a company's network, subnetting helps the network managers in reducing network-wide risks.<br>
<br>Internet Protocol (IP) Addressing Relocation
<br>Each class has a finite amount of possible host allocations; for instance, networks with more than 254 devices require a Class B allocation. Assume that you are a network administrator. Now, you have a task of allocating 150 hosts among three physical networks in three distinct cities for a Class B or C network. If so, we must either ask for additional address blocks for each network or split the single big network into small parts named subnets so that we could utilize a single address block across a number of physical networks.<br>We will learn about this concept deeper in the upcoming topics.<br>
<br>Reduction of Network Traffic
<br>Placing all of the computers on the same subnet can assist minimize network traffic if a significant amount of an organization's traffic is intended to be shared routinely among a number of devices. Without a subnet, all computers and servers on the network would be able to see data packets from every other machine.<br>
<br>Network Speed Improvement
<br>The main network is divided into smaller subnets through the process of subnetting, and the goal of these smaller, linked networks is to split the large network into a collection of smaller, less-busy networks. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network.<br>
<br>Division of IP Addresses
<br>An IP address is split into its network address and host address via subnetting.<br>The split address may then be further divided into units using the subnet mask approach, and those units can be assigned to different network devices.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png" referrerpolicy="no-referrer"><br>Here, X refers to the Host ID. This is the only thing which gets changed in the Internet Protocol Address<br>Now, we are going to learn how these subnets provide the different addresses to different devices and also the process of subnetting in computer networks. So, by this example we would easily understand the working of the Subnet.<br>We are going to learn how Subnets are formed for Internet Protocol version 4 (IPv4) Addressing.<br>The IPv4 Addressing has five different classes. They are:<br>
<br>Class A Network
<br>Class B Network
<br>Class C Network
<br>Class D Network
<br>Class E Network
<br>The total number of Internet Protocol Addresses (IP Address) gives the total number of Subnets that can be formed by using a network.<br>
<br>Class A has 24 Host ID Bits
<br>Class B has 16 Host ID Bits
<br>Class C has 8 Host ID Bits
<br>The number of usable IP Addresses that can be created is<br>The total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.<br>
Class A Network can have 224 - 2<br>
Class B Network can have 216 - 2<br>
Class C Network can have 28 - 2<br>
Class D and Class E do not contribute for IP Address creation.<br>
Class D is used for multicasting purpose<br>
Class E is used for Address Range Calculator<br>They are saved for future purposes.<br><br><br>We have arrived at the subject at hand, Subnetting, thanks to the problem of IP address waste. By taking bits from the Host ID section of the address, subnetting enables the creation of smaller networks (sub networks; subnets) within of a larger network. With the help of those borrowed bits, we can build more networks with a reduced overall size.<br>A Subnet is created from the bits taken from the Host ID.<br>To understand about this concept let take an example of a network this belongs to class C.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks2.png" referrerpolicy="no-referrer"><br>Our goal is to create to build a network. The capacity of each network must be Thirty (30) Devices. We have three networks of type Class C Network based on IPv4 Addressing.<br>Each Class C Network can provide Two Hundred and Fifty Four (254) Internet Protocol Addresses.<br>The Capacity of each device which we require is very less than the Capacity which we require.<br>So, now we divide the four networks based on the requirement. Let us see how this division happens.<br>We have four Class C Networks of imaginary Internet Protocol (IP) Addresses like:<br>
<br>Network 1 : 255.147.1.0
<br>Network 2 : 255.147.2.0
<br>Network 3 : 255.147.3.0
<br>Network 4 : 255.147.4.0
<br>We know that each network can produce 254 IP Addresses alone. This means four networks can produce 254 * 4 = 1016 (Thousand and Sixteen ) Internet Protocol Addresses can be formed. But what we require is only thirty Internet Protocol Addresses from each Network. This means we only need hundred and Twenty (120) IP Addresses only.<br>This means 1016 - 120 = 896<br>Eight Hundred and Ninety-Six Addresses created are wasted. So, we need to use the Host ID bits wisely.<br>So, by some calculation we will get to know that if we take 5 bits from each network we will be able to get 30 IP Addresses from each Network.<br>The formula for number of IP Addresses is:<br>The total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.<br>So, now we will consider 5 Host ID Bits.<br>25 - 2 = 30 Internet Protocol Addresses from each Network.<br>So, by considering we can create 30 Usable IP Addresses from each Class C Network.<br>So, now we have 3 more Host ID Bits left over unused. We also have different ways in using these remaining bits.<br><br>
<br>These remaining Host ID Bits can be used to increase the capacity of the IP Addresses to be created in future, if required.
<br>We can also create a new six subnets from each network using these three Host ID Bits.
<br>First method is usually chosen because creation of two different subnets causes wastage of IP Addresses. Let me explain this problem with the help of the above example.<br>Example:<br>The network belongs to Class C Network which has 8 Host ID Bits.<br>
In the above first created Subnet we have only used 30 IP Addresses only.<br>
In the newly created Subnet we have created only 6 IP Addresses only.<br>
This means we have used the full potential of the Class C Network. We might have used the whole 8 bits. But, this is considered as wastage of resources.<br>
This is called wastage because we have now a capacity of 36 IP Addresses to be created.<br>
But, the actual capacity of the Class C is 254 IP Addresses.<br>
This means 254 - 36 = 218 IP Addresses are wasted now because of this Host ID Bits Division.<br>
So, it is better to save the remaining Host ID Bits for future purpose rather than dividing it for these kind of resource wasting purpose.  <br><br>Subnetting, as we all know, separates the network into small subnets. While each subnet permits communication between the devices connected to it, subnets are connected together by routers. The network technology being utilized and the connectivity requirements define the size of a subnet. Each organization is responsible for selecting the number and size of the subnets it produces, within the constraints of the address space available for its use.<br>
<br>For the construction of the subnets, we usually check the MSB (Most Significant Bit) bits of the host ID and if found wrong we make it right. In order to create two network subnets, we fix one of the host's MSB (Most Significant Bit) bits in the table below. We are unable to alter network bits since doing so would alter the entire network.
<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks3.png" referrerpolicy="no-referrer"><br>We need a subnet mask to identify a subnet, which is created by substituting the number "1" for each Network ID bit and the amount of bits we reserve for Host ID to create the subnet. A data packet from the internet is intended to be forwarded to the specified subnet network using the subnet mask.<br>A part of an address should be used as the Subnet ID is also specified by the subnet mask. In order to apply the subnet mask to the whole network address, a binary AND operation is utilized. When performing an AND operation, it is assumed that the result will be "true" if both inputs are. If not, "false" is presented. This is only possible when both bits are 1.<br>The Subnet ID results from this. The Subnet ID is used by routers to choose the best route among the sub - networks.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks4.png" referrerpolicy="no-referrer"><br>
<br>The two components that make up an IP address are the Network Prefix (sometimes called the Network ID) and the Host ID. Depending on whether the address is Class A, B, or C, either the Network Prefix or the Host ID must be separated. A Class B IPv4 address, 172.16.37.5, is seen in the image below. The Network Prefix is 172.16.0.0, and the Host ID is 37.5.
<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks5.png" referrerpolicy="no-referrer"><br>
<br>We use permutations to the amount of bits set aside to form subnets if we wish to produce subnets of varied length. Variable Length Subnet Masking is the name of this subnetting (VLSM).
<br>After setting aside some bits to indicate the subnet, the broadcast address of a subnet is computed by setting all the remaining bits of the host id to 1.The message is sent to all network hosts using the broadcast address.
<br><br>
<br>Subnetting is used to decrease the presence of Internet Protocol (IP) range.
<br>Subnets helps in stopping the devices or gadgets from occupying the whole network, only allowing the hosts to control which kind of user can have access to the important information. Simply, we can tell that network is safe just because of the subnetting concept.
<br>Subnetting concept increases the performance of the total network by deleting the repeated traffic causing errors.
<br>We can convert the whole big network into smaller networks by using the concept of subnetting as discussed earlier.
<br><br>
<br>If the number of subnets increases, then the number of routers must also increase along with the subnet increase number. This happens because each subnet has its own subnet mask, broadcast address and network address.
<br>As told earlier, if we create many subnets many IP Addresses are wasted because of the wastage of Host ID Bits
<br>The cost of the entire network is increased by subnetting, which calls for the acquisition of pricey internal routers, switches, hubs, and bridges, among other things.
<br>The complexity of the network is increased through subnetting. The subnet network must be managed by a skilled network administrator.
<br>This is all about Subnetting Concept in the subject named Computer Networks.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/subnetting.html</link><guid isPermaLink="false">AWS/For Reading/Subnetting.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Types of Autoscaling]]></title><description><![CDATA[ 
 <br>source: <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/amazon-web-services-scaling-amazon-ec2/" target="_blank">https://www.geeksforgeeks.org/amazon-web-services-scaling-amazon-ec2/</a><br><br>Auto Scaling is a feature in cloud computing that allows a cloud-based application to automatically adjust the resources it uses such as servers, compute instances based on demand. The goal of Auto Scaling is to ensure that the application has sufficient resources to meet performance goals and maintain availability, while also optimizing resource utilization and minimizing costs. To know the difference between Auto scaling and load balancer refer to the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/auto-scaling-vs-load-balancer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/auto-scaling-vs-load-balancer/" target="_blank">Auto Scaling vs Load Balancer</a>.<br><br>AWS auto-scaling is used to scale up and scale down the EC2-instance by depending on the incoming traffic. You can scale up and scale down the applications in a few minutes based on the traffic which will decrease the latency of the application to the end-users.<br><br>Dynamical scaling:&nbsp;AWS auto-scaling service doesn’t require any type of manual intervention it will automatically scale the application down and up by depending up on the incoming traffic.<br>
<br>**Pay For You Use:**&nbsp;In auto scaling the resource will be utilised in the optimised way where the demand is low the resource utilisation will be low and the demand will high the resource utilisation will increase so the AWS is going to charge you only for the amount of resources you really used.
<br>**Automatic Performance Maintenance:**&nbsp;AWS auto scaling maintains the optimal application performance with considering the workloads it will ensures that the application is running to desired level which will decrease the latency and also the capacity will be increased by based on your application
<br><br>Following are the components of AWS Scaling Components.<br>
<br>**Groups:**For scaling and managing the EC2 instances are grouped together so that they may be thought of as a single logical entity. You can mention the minimum and maximum no.of EC2 instance are required based up on the demand of the incoming traffic.
<br>**Configuration templates:**&nbsp;Configuration template or an launch template which is used by the EC2 autoscaling group for the EC2 instance. In which you can specify the Amazon Machine Image ID,keypair,security group and so on.
<br>**Scaling options: Aws Autoscaling provides no.of options some of them are mentioned as following.**

<br>Dynamic scaling
<br>Predictive scaling
<br>Scheduled scaling
<br>Manual scaling


<br><img alt="Auto-Scaling EC2" src="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png" referrerpolicy="no-referrer"><br>That’s the point where Amazon EC2 Autoscaling comes into the picture. You may use Amazon EC2 Auto Scaling in order to add or delete Amazon EC2 instances with respect to changes in your application demand. You can maintain a higher feeling of application availability by dynamically scaling your instances in and out as needed.<br><br>You can use three scaling techniques within Amazon EC2 Auto Scaling i.e. Dynamic Scaling, Predictive Scaling, and Scheduled Scaling. They are explained in detail below:<br>
<br>
**Dynamic Scaling: A**dapts to changing environments and responds with the EC2 instances as per the demand. It helps the user to follow the demand curve for the application, which ultimately helps the maintainer/user to scale the instances ahead of time. Target tracking scaling policies, for example, may be used to choose a loaded statistic for your application, such as CPU use. Alternatively, you might use Application Load Balancer’s new “Request Count Per Target” measure, which is a load balancing option for the Elastic Load Balancing service. After that, Amazon EC2 Auto Scaling will modify the number of EC2 instances as needed to keep you on track.&nbsp;

<br>
**Predictive Scaling:**&nbsp;Helps you to schedule the right number of EC2 instances based on the predicted demand. You can use both dynamic and predictive scaling approaches together for faster scaling of the application. Predictive Scaling forecasts future traffic and allocates the appropriate number of EC2 instances ahead of time. Machine learning algorithms in Predictive Scaling identify changes in daily and weekly patterns and automatically update projections. In this way, the need to manually scale the instances on particular days is relieved.&nbsp;

<br>
**Scheduled Scaling:**&nbsp;As the name suggests allows you to scale your application based on the scheduled time you set. For e.g. A coffee shop owner may employ more baristas on weekends because of the increased demand and frees them on weekdays because of reduced demand.

<br>Computing power is a programmed resource in the cloud, so you may take a more flexible approach to scale your applications. When you add Amazon EC2 Auto Scaling to an application, you may create new instances as needed and terminate them when they’re no longer in use. In this way, you only pay for the instances you use, when they’re in use.<br><br>
<br>**Horizontal Scaling:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" target="_blank">Horizontal scaling</a>&nbsp;involves adding more instances to your application to handle increased demand. This can be done manually by launching additional instances, or automatically using Amazon EC2 Auto Scaling, which monitors your application’s workload and adds or removes instances based on predefined rules.
<br>**Vertical Scaling:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" target="_blank">Vertical scaling</a>&nbsp;involves increasing the resources of existing instances, such as CPU, memory, or storage. This can be done manually by resizing instances, or automatically using Amazon EC2 Auto Scaling with launch configurations that specify instance sizes based on the workload.
<br>**Load Balancing:**&nbsp;Load balancing involves distributing incoming traffic across multiple instances to improve performance and availability.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/elastic-load-balancer-in-aws/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/elastic-load-balancer-in-aws/" target="_blank">Amazon Elastic Load Balancing (ELB)</a>&nbsp;is a service that automatically distributes incoming traffic across multiple instances in one or more Availability Zones.
<br>**Multi-Availability Zone Deployment:**&nbsp;Multi-Availability Zone (AZ) deployment involves launching instances in multiple AZs to improve availability and fault tolerance. Amazon EC2 Auto Scaling can be used to automatically launch instances in additional AZs to maintain availability in case of an AZ outage.
<br>**Containerization:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/containerization-using-docker/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/containerization-using-docker/" target="_blank">Containerization</a>&nbsp;involves using containers to package and deploy applications, making them more portable and easier to manage.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-to-amazon-elastic-container-service-ecs/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-to-amazon-elastic-container-service-ecs/" target="_blank">Amazon Elastic Container Service (ECS)</a>&nbsp;is a service that makes it easy to run, stop, and manage Docker containers on a cluster of EC2 instances.
<br>Note
When we created the auto-scaling group, we configured the Desired capacity, Minimum capacity, maximum capacity, and CPU utilization. If CPU utilization increases by 60% in all instances, one more instance is created, and if CPU utilization decreases by 30% in all instances, one instance is terminated.
<br><br><br>
AWS auto scaling is an service provided by the AWS which is used to scale the EC2 by depending up the in coming traffic.
<br><br>
Auto scaling is mainly used to scale up and scale down the application based on the load. There are four main types of AWS autoscaling:

<br>manual scaling,
<br>scheduled scaling,
<br>dynamic scaling, and
<br>predictive scaling

<br><br>
The main components of autoscaling was mentioned below.

<br>Load Balancer.
<br>Snapshot.
<br>EC2 (Elastic Compute Cloud) Instance.
<br>Autoscaling group.

<br><br>
AWS Auto Scaling Group Terraform is a module that allows you to create and manage Auto Scaling groups using Terraform.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/types-of-autoscaling.html</link><guid isPermaLink="false">AWS/For Reading/Types of Autoscaling.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Amazon Web Services In Plain English]]></title><description><![CDATA[ 
 <br>I was searching one liner meaning for aws services and I came across some interesting websites:<br>Meaning of AWS Services in one line

<br><a rel="noopener nofollow" class="external-link" href="https://adayinthelifeof.nl/2020/05/20/aws.html" target="_blank">https://adayinthelifeof.nl/2020/05/20/aws.html</a>
<br><a rel="noopener nofollow" class="external-link" href="https://expeditedsecurity.com/aws-in-plain-english/" target="_blank">https://expeditedsecurity.com/aws-in-plain-english/</a>

<br>Those I need 
EC2 - Virtual Private Servers<br>
Lambda - Programming Functions you can run but can costs a fortune<br>
S3 - File Storage (unmountable)<br>
EFS - Mountable Network Disks<br>
RDS - Managed Relational Database<br>
VPC - Virtual Private Network (consider as VLANs)<br>
DynamoDB - Large &amp; scalable non-relational database<br>
CloudFront - Content Delivery Network<br>
Route 53 - Managed domains names and records<br>
CloudWatch - Monitoring and Logs<br>
Autoscaling - Scale resources dynamically or manually<br>
CloudTrail - Spying on your users (Tracking Users activity)<br>
IAM - Users and their Permissions, policies and roles
]]></description><link>https://notes.sarangwandile.xyz/aws/notes/amazon-web-services-in-plain-english.html</link><guid isPermaLink="false">AWS/Notes/Amazon Web Services In Plain English.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Aws Services Simply Explained]]></title><description><![CDATA[ 
 <br>This is the list of AWS resources required for Entry level devops interview preparation. Sometimes we struggle to explain the definition we already know it's usecase in simpler terms. So I curated this list. The ones written in Italian is my own hand crafted lines while others mostly generated from Gemini Ai.<br>
<br>
EC2 (Elastic Compute Cloud):  Like renting virtual computers in the cloud, you can run whatever software you want on them.
Its like vps or vms in the cloud, they are scalable, flexible and cheaper.

<br>
VPC is like having your own private network within the cloud. It lets you create a secure and isolated space for your resources, like your own little corner of the internet in AWS with all the required tools to create a network such as subnets, route table, internet gateway, VPNs, etc.

<br>
Load Balancer: Distributes traffic across multiple EC2 instances so your application can handle more users and is more resilient.

<br>
CloudWatch:  Monitors your AWS resources and applications, collecting metrics and logs to help you understand performance and troubleshoot issues.
Aws provide cloudwatch service to monitor and manage logs of resources to help user getting better performance and troubleshoot issues.

<br>
CloudFront: A content delivery network (CDN) that makes your websites and applications faster by caching content closer to your users.
Cloudfront is aws managed CDN to caching the content closer to users to make websites and applications faster.

<br>
S3 (Simple Storage Service):  Like a hard drive in the cloud for storing all sorts of files, from websites to backups.
Aws managed storage service like a Google drive to store all sorts of files and store backups

<br>
EFS (Elastic File System):  A network file system that lets you share files between multiple EC2 instances.

<br>
RDS (Relational Database Service): Makes it easy to set up and manage databases in the cloud, like MySQL, PostgreSQL, and Oracle.

<br>
Route 53: A scalable DNS service that translates domain names (like google.com) into IP addresses.

<br>
Lambda: Lets you run code without provisioning or managing servers; you only pay for the compute time you use.<br>
Serverless compute

<br>
SNS (Simple Notification Service): A messaging service that can send notifications to various destinations, like email, SMS, and other applications.

<br>
SQS (Simple Queue Service): Like a post office box in the cloud. You put messages in it (like tasks to be done), and other services can pick them up later. This helps keep different parts of your application loosely connected and make things more reliable.

<br>
EBS (Elastic Block Store): Think of it as a fast hard drive in the cloud. You can attach it to an EC2 instance (your virtual computer) to store data. This is great for things like databases or file storage.
Ebs is like virtual hard drive that can be attached to ec2 instances and its scalable and you can adjust performance and type according to your need.

]]></description><link>https://notes.sarangwandile.xyz/aws/notes/aws-services-simply-explained.html</link><guid isPermaLink="false">AWS/Notes/Aws Services Simply Explained.md</guid><pubDate>Sun, 29 Dec 2024 15:50:09 GMT</pubDate></item><item><title><![CDATA[Simple Definitions To give in Interview]]></title><description><![CDATA[ 
 <br><br><br>Linux is a powerful, flexible, and open-source operating system that's widely used in server environments.<br><br>kernel is the core component of an operating system. It acts as bridge between software and hardware.]]></description><link>https://notes.sarangwandile.xyz/aws/notes/simple-definitions-to-give-in-interview.html</link><guid isPermaLink="false">AWS/Notes/Simple Definitions To give in Interview.md</guid><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[☑️ How to store aws load balancer logs in s3 bucket]]></title><description><![CDATA[ 
 <br><br>1.&nbsp;&nbsp;&nbsp; Create 2 instances and&nbsp;add http port and also install nginx enable it .<br>
2.&nbsp;&nbsp;&nbsp; For load balancer u will need target group<br>
3.&nbsp;&nbsp;&nbsp; Go to EC2 service scroll down u will see target groups<br>
<img alt="{8799CC1A-FB3F-4C46-89AC-E9EA4FA22C1E} 1.png" src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"><br>
4.&nbsp;&nbsp;&nbsp; Create target group &gt; add instances &gt; save and create .<br>
<img alt="{122014CE-B98D-499C-83AC-AD171883DF5B}.png" src="https://notes.sarangwandile.xyz/lib/media/{122014ce-b98d-499c-83ac-ad171883df5b}.png"><br>
<img alt="{76992DE6-07D5-4F62-BF3D-8BAC91D82994}.png" src="https://notes.sarangwandile.xyz/lib/media/{76992de6-07d5-4f62-bf3d-8bac91d82994}.png"><br>
5.&nbsp;&nbsp;&nbsp; Now create load balancer (Application load balancer) &gt; configure it give name , vpc ,select availability zones, select Target Groups u created and create load balancer.<br>
<img alt="{B089AE33-8E08-44B4-BF14-89AC63880F3B}.png" src="https://notes.sarangwandile.xyz/lib/media/{b089ae33-8e08-44b4-bf14-89ac63880f3b}.png"><br>
<img alt="{CC268607-25E9-442F-BAC8-171661E1F704}.png" src="https://notes.sarangwandile.xyz/lib/media/{cc268607-25e9-442f-bac8-171661e1f704}.png"><br>
6.&nbsp;&nbsp;&nbsp; Check it your load balancer is working or not.<br>
7.&nbsp;&nbsp;&nbsp; If working , Create s3 bucket&nbsp; and go to permission &gt; &nbsp;bucket policy and edit it :-<br>
8.&nbsp;&nbsp;&nbsp; Note :- &nbsp;Find policy from this by searching on google enable access log of load balancer to s3<br><img alt="Pasted image 20241201200127.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200127.png"><br>
Policy :-<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws-us-gov:iam::elb-account-id:root"
      },
      "Action": "s3:PutObject",
      "Resource": "s3-bucket-arn"
    }
  ]
}
<br><img alt="Pasted image 20241201200139.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200139.png"><br>
9.&nbsp;&nbsp;&nbsp; Save it.<br>10.&nbsp;&nbsp;Now go to load balancer u created &gt; go to attributes section &gt;&nbsp; scroll down and u will see this monitoring sec enable access logs and add our bucket<br><img alt="Pasted image 20241201200203.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200203.png"><br>
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Save changes.<br>
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; U will see ur logs folder in ur s3 bucket.]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/☑️-how-to-store-aws-load-balancer-logs-in-s3-bucket.html</link><guid isPermaLink="false">AWS/Tasks Done/☑️ How to store aws load balancer logs in s3 bucket.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[☑️ Implement autoscaling on memory utilization]]></title><description><![CDATA[ 
 <br><br>
<br>Implement autoscaling on memory utilization 
<br>SNS will notification each time instance scale up and scale down
<br>In short:<br>
Implementing Autoscaling on Memory Utilization with SNS Notifications<br>
Understanding the Components:<br><br>
<br>AWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.
<br>Amazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.
<br>Amazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).
<br><br>
<br>Navigate to the SNS console.
<br>Create a new topic.
<br>Give it a descriptive name (e.g., "AutoScalingNotifications").
<br><img alt="autoscaling-notifications-created.png" src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"><br><br>
<br>Navigate to the EC2 console and select "Auto Scaling Groups".
<br>Create a new Auto Scaling group with the desired configuration:

<br>Launch Configuration: Specify the AMI, instance type, security groups, etc.
<br>Scaling Policies:

<br>Scaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.
<br>Target Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.




<br><img alt="launch-template-creation-snsautoscalinggroup123.png" src="https://notes.sarangwandile.xyz/lib/media/launch-template-creation-snsautoscalinggroup123.png"><br>Created Launch Template with this User data<br>#!/bin/bash
yum install nginx unzip -y
systemctl enable --now nginx
curl -O [https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)
unzip oxer.zip
rm oxer.zip
mv oxer* /usr/share/nginx/html
systemctl restart nginx
<br><img alt="autoscale-group-size.png" src="https://notes.sarangwandile.xyz/lib/media/autoscale-group-size.png"><br>enable sns notification and select previously created sns topic<br>
<img alt="add-notification-in-auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/add-notification-in-auto-scaling-groups.png"><br><img alt="choose-lauch-template-review.png" src="https://notes.sarangwandile.xyz/lib/media/choose-lauch-template-review.png"><br><br>
<br>Navigate to the CloudWatch console.
<br>Create an alarm for the "MemoryUtilization" metric:

<br>Metric Name: MemoryUtilization
<br>Namespace: AWS/EC2
<br>Statistic: Average
<br>Period: 5 Minutes (or as needed)
<br>Threshold: Set the desired threshold (e.g., 80%)
<br>Comparison Operator: Greater Than or Equal To
<br>Alarm Actions: Select the SNS topic created in step 1.


<br>Associate the Alarm with the Auto Scaling Group:

<br>In the Auto Scaling group settings, under "Notifications," add the CloudWatch alarm created in step 3.


<br>Create Alarm<br>
<img alt="create-alarms-cloudwatch.png" src="https://notes.sarangwandile.xyz/lib/media/create-alarms-cloudwatch.png"><br>select memory utilization metric<br>
<img alt="select-metric-cpu-utilization.png" src="https://notes.sarangwandile.xyz/lib/media/select-metric-cpu-utilization.png"><br><img alt="select-matric.png" src="https://notes.sarangwandile.xyz/lib/media/select-matric.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/☑️-implement-autoscaling-on-memory-utilization.html</link><guid isPermaLink="false">AWS/Tasks Done/☑️ Implement autoscaling on memory utilization.md</guid><pubDate>Sat, 28 Dec 2024 13:29:05 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Create 5 IAM users and 5 S3 buckets and attach them each other]]></title><description><![CDATA[ 
 <br>Task:<br>
requirement: <br>
<br>Create 5 I am users
<br>Create 5 S3 Buckets
<br>assign each user their name specific bucket
<br>no one should able touch other users bucket
<br>
<br>Created User s3-mango with default settings
<br>Created s3 bucket s3-mango-bucket with default settings<br>
after creation added following policy to bucket
<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::970547378605:user/s3-mango"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::s3-mango-bucket",
        "arn:aws:s3:::s3-mango-bucket/*"
      ]
    }
  ]
}
<br>Step 4: Assign IAM Policies to Users<br>
Example for Alice:<br>
Go to the IAM Console and open Alice.<br>
Click Add Permissions, then Attach Policies Directly.<br>
Create a custom policy for Alice:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::s3-mango-bucket",
        "arn:aws:s3:::s3-mango-bucket/*"
      ]
    }
  ]
}
<br>Attach this policy to Alice.<br>granted =AmazonS3ReadOnlyAccess= to each users<br><br>Certainly! Let's go step-by-step with specific names for users and their buckets to make it clearer.<br>Scenario<br>
We want to:<br>Create 5 users: Alice, Bob, Charlie, Dave, and Eve.<br>
Create 5 buckets: alice-bucket, bob-bucket, charlie-bucket, dave-bucket, and eve-bucket.<br>
Ensure that:<br>
Alice can access only alice-bucket.<br>
Bob can access only bob-bucket.<br>
Similarly, each user has access only to their bucket.<br>
Step 1: Create Users<br>
Manually create the following users via the IAM Console:<br>Alice<br>
Bob<br>
Charlie<br>
Dave<br>
Eve<br>
Step 2: Create Buckets<br>
Manually create the following buckets via the S3 Console:<br>alice-bucket<br>
bob-bucket<br>
charlie-bucket<br>
dave-bucket<br>
eve-bucket<br>
Step 3: Set Permissions for Each Bucket<br>
Example for Alice:<br>
Go to the S3 Console and open alice-bucket.<br>Go to the Permissions tab, then Bucket Policy.<br>Add this bucket policy:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ACCOUNT_ID:user/Alice"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::alice-bucket",
        "arn:aws:s3:::alice-bucket/*"
      ]
    }
  ]
}
<br>Replace ACCOUNT_ID with your AWS account ID.<br>Save the policy.<br>Repeat this step for:<br>Bob with bob-bucket<br>
Charlie with charlie-bucket<br>
Dave with dave-bucket<br>
Eve with eve-bucket<br>
Step 4: Assign IAM Policies to Users<br>
Example for Alice:<br>
Go to the IAM Console and open Alice.<br>
Click Add Permissions, then Attach Policies Directly.<br>
Create a custom policy for Alice:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::alice-bucket",
        "arn:aws:s3:::alice-bucket/*"
      ]
    }
  ]
}
<br>Attach this policy to Alice.<br>
Repeat this step for:<br>Bob with bob-bucket<br>
Charlie with charlie-bucket<br>
Dave with dave-bucket<br>
Eve with eve-bucket<br>
Step 5: Test Access<br>
Log in as Alice and try to:<br>
Upload a file to alice-bucket (should work).<br>
Access bob-bucket (should fail).<br>
Repeat the process for Bob, Charlie, Dave, and Eve to ensure they can only access their respective buckets.<br>
How This Works<br>
Each user has a bucket policy on their bucket that allows only them to access it.<br>
Each user also has an IAM policy that allows them to access only their bucket.<br>
This ensures that the permissions are restricted at both the bucket level and the user level.<br>
Let me know if you need help applying this setup!<br>
creating bucket<br>
<br>bucket name: s3-mango bucket
<br>object ownership -<br>
ACLs disabled (recommended)<br>Block Public Access settings for this bucket<br>
block all public access turn on<br>
<br>
Create user =mango=<br>
password- myPassw0rd

<br>
Created policy called =my-s3-mango-bucket-policy=<br>
Permissions defined in this policy

<br> {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::s3-mango-bucket"
        }
    ]
}
<br>attached this policy to user<br><img alt="IAM-user-creating-review-and-create.png" src="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-create-5-iam-users-and-5-s3-buckets-and-attach-them-each-other.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Create 5 IAM users and 5 S3 buckets and attach them each other.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Create notification for s3 bucket activity happen of put and delete]]></title><description><![CDATA[ 
 <br><br>Whenever a new object is added or deleted in s3 bucket you will get notification via email<br><br><img alt="Create-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"><br><br><img alt="create-topic.png" src="https://notes.sarangwandile.xyz/lib/media/create-topic.png"><br>
Create an topic &gt; go to access policy and add a policy<br>&nbsp;<br>
<img alt="sns-create-topic-access-policy.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-topic-access-policy.png"><br>Click on Advanced and Paste this code.<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "SNS:Publish",
      "Resource": "arn:aws:sns:us-west-2:970547378605:sns-s3-topic123",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::snsbucket123testing"
        }
      }
    }
  ]
}
<br>Tip
In above JSON code edit "Resource"  block to replace the ARN id according to your sns topic's Arn id and aws:SourceArn to your S3 bucket's ARN id.<br>
or in last two fields just change your AWS account id and topic name<br>
"Resource": "arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;"<br>
"aws:SourceArn": "arn:aws:s3:::&lt;s3-bucket-name&gt;"
<br><br>   After Topic is created Create Subscription to get notifications <br><img alt="sns-create-subscription.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-subscription.png"><br>
Set:<br>
<br>Topic ARN: Your previously created Topic's ARN ID
<br>Protocol: Email<br>
- Endpoint: Your Gmail address<br>
Click on create subscription button
<br>After your subscription is created, you'll get a mail to confirm it.<br><img alt="email-subscription-confirmation.png" src="https://notes.sarangwandile.xyz/lib/media/email-subscription-confirmation.png"><br>
Goto your mail and click on Confirm Subscription link.<br>
<img alt="sns-subscription-mail.png" src="https://notes.sarangwandile.xyz/lib/media/sns-subscription-mail.png"><br><br>Now go to your buckets<br>
<br>
Select Your Bucket:

<br>
Choose the bucket for which you want to set up notifications.

<br>
Go to the Properties tab.<br>
<img alt="snsbucket123testing-overview.png" src="https://notes.sarangwandile.xyz/lib/media/snsbucket123testing-overview.png">

<br>
Scroll down to Event notifications and click Create event notification.<br>
<img alt="creating-snsbucket123testing-event-notification.png" src="https://notes.sarangwandile.xyz/lib/media/creating-snsbucket123testing-event-notification.png">

<br>
Click on Create event notification 

<br>
Event name: put_and_delete_event

<br>
Event types: 

<br>Select put checkbox
<br>Permanently Deleted checkbox<br>
<img alt="Pasted image 20241201210604.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201210604.png">


<br>
Scroll down u will see Destination Option 

<br>select SNS topic 
<br>choose your previously created SNS Topic 
<br>Click on Save changes<br>
<img alt="selecting-sns-topic-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-sns-topic-s3-bucket.png"><br>
Try adding new files in bucket you should get notification in your mail.<br>
<img alt="uploading-files-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/uploading-files-s3-bucket.png">


<br><img alt="sns-topic-email-notification.png" src="https://notes.sarangwandile.xyz/lib/media/sns-topic-email-notification.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-create-notification-for-s3-bucket-activity-happen-of-put-and-delete.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Create notification for s3 bucket activity happen of put and delete.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-bucket.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Get sns alert when any user launches new instance]]></title><description><![CDATA[ 
 <br><br>Whenever u r launching instance u will get a event and also u have to get details<br><br>
<br>
Create Topic

<br>Type:  Standard
<br>Name: NotifyonInstanceLaunch


<br>
Create subscription 

<br>Select Topic ARN
<br>Protocol: E-mail 
<br>Endpoint: <a data-tooltip-position="top" aria-label="mailto:archsarangx@gmail.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:archsarangx@gmail.com" target="_blank">archsarangx@gmail.com</a>
<br>check your mails and confirm subscription 

4.&nbsp;&nbsp;&nbsp; Go to Amazon Event bridge service .<br>
5.&nbsp;&nbsp;&nbsp; In buses section go to rules and Create rule<br>
<img alt="create-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"><br>
In step 2 Build event pattern scroll down to bottom<br>
In Event Pattern section add configuration like this:

<br>
Event Source: AWS Service

<br>
AWS Service: EC2

<br>
Event Type: EC2 Instance State-change Notification<br>
<img alt="event-pattern-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/event-pattern-eventbridge-rules.png"><br>
Click Next and in Step 3 Select Target page Select

<br>
Target Type: AWS Service

<br>
Select a target: SNS Topic

<br>
Topic: your topic name

<br><img alt="selecting-target-amazon-eventbridge-rules-creation.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-target-amazon-eventbridge-rules-creation.png"><br>
9.&nbsp;&nbsp;&nbsp; Create target and at last create rule<br>&nbsp;You will get a notifications while Launching and terminating instance]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-get-sns-alert-when-any-user-launches-new-instance.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Get sns alert when any user launches new instance.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Host static website on s3 bucket]]></title><description><![CDATA[ 
 <br><br>
<br>Create a bucket
<br>add website template inside bucket
<br>enable static website hosting option
<br>view the website
<br><br><img alt="create-a-bucket-button.png" src="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png"><br>
<br>Goto aws S3 bucket creation page
<br>Click on create bucket button
<br>Select Bucket Type as General Purpose
<br>Give any Bucket Name of your desire (name should be globally unique)
<br><img alt="create-bucket-s3-cdec-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket-s3-cdec-bucket.png"><br>
<br>Here I set s3-cdec-bucket
<br>Leave default setting as it is like ACLs disabled on 
<br>Clear all Block&nbsp;all&nbsp;public access option
<br>I set bucket versioning enabled
<br>Leave other setting as it is and click on create bucket
<br>Here my s3 bucket is created<br>
<img alt="created-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/created-bucket.png"><br><br>You can upload your website files if you have but I'll use free website template<br>
from <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/" target="_blank">https://www.free-css.com/</a><br>Download any template from this site and upload the extracted content from the zip over the bucket<br>I downloaded this template called browny <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/free-css-templates/page296/browny" target="_blank">https://www.free-css.com/free-css-templates/page296/browny</a><br>
<img alt="download-free-css-template-zip.png" src="https://notes.sarangwandile.xyz/lib/media/download-free-css-template-zip.png"><br>Its gave me zip file.<br>
I'll be extracting it and uploading the folder into the bucket<br><img alt="extract-template-zip.png" src="https://notes.sarangwandile.xyz/lib/media/extract-template-zip.png"><br>It contains these files<br>
<img alt="template-files-downloaded.png" src="https://notes.sarangwandile.xyz/lib/media/template-files-downloaded.png"><br>
<br>Lets upload the files into the root of the s3 bucket
<br><img alt="s3-cdec-bucket-uploading-files.png" src="https://notes.sarangwandile.xyz/lib/media/s3-cdec-bucket-uploading-files.png"><br>Click on your bucket name in s3 bucket list and click on upload button<br>
then click on add files<br>you can also drag n drop files and folder <br>Click on Add files to add files and Add folder to upload folder<br>
<img alt="upload-files.png" src="https://notes.sarangwandile.xyz/lib/media/upload-files.png"><br>After the files are shown like this finally click on upload button<br>After the files gets uploaded click on your s3 bucket name and goto properties and<br>
<img alt="s3-cdec-bucket-properties-tab.png" src="https://notes.sarangwandile.xyz/lib/media/s3-cdec-bucket-properties-tab.png"><br>and scroll down till you see Static Website Hosting option<br><img alt="static-website-hosting.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-hosting.png"><br>click on edit button and enable it<br>
after enabling it select Hosting type as host a static website<br>
<img alt="enabling-static-website-hosting.png" src="https://notes.sarangwandile.xyz/lib/media/enabling-static-website-hosting.png"><br>and specify the index document as index.html<br>
<img alt="index-document.png" src="https://notes.sarangwandile.xyz/lib/media/index-document.png"><br>
Now Static website option should be updated with the link to access our newly hosted static website link<br><img alt="static-website-enabled-success.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-enabled-success.png"><br>
Click on the Bucket website endpoint url or open it in another tab<br><br>but before this link to even work we need to specify a permission policy to be able to see our site<br>Under the permission tab click Edit bucket policy<br>
copy and paste this policy and save changes<br>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::your-bucket-name/*"
            ]
        }
    ]
}

<br>The provided JSON defines an AWS IAM policy that grants public read access to all objects in an S3 bucket named "your-bucket-name".<br>replace your-bucket-name with your bucket name<br>save changes and see the link now your site should be visible.<br>
now our website is visible over this url <a rel="noopener nofollow" class="external-link" href="http://s3-cdec-bucket.s3-website-us-west-2.amazonaws.com/" target="_blank">http://s3-cdec-bucket.s3-website-us-west-2.amazonaws.com/</a><br><img alt="static-website-success-final-image.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-success-final-image.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-host-static-website-on-s3-bucket.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Host static website on s3 bucket.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Implement Template with Scheduled Autoscaling]]></title><description><![CDATA[ 
 <br>In this task we will be launching instance with userdata attach for setuping and starting nginx sever. We will create template out of that instance attach it to the autoscaling target groups. So when stress will be added to instance it should create another instances to balance the load.<br><br>
<br>✅ Launch instance 
<br>✅ Pass userdata 
<br>✅ Install nginx 
<br>✅ Create template from instance 
<br>✅ Implement that template into auto scaling 
<br>✅ Create scheduled autoscaling 
<br>✅ Start instance 
<br>✅ Add stress into instance 
<br>✅ Upgrade the template to nginx to httpd
<br><br>
<br>Name: MyEc2Instance
<br>AMI: Amazon Linux (Free Tier)
<br>Instance Type: t2.micro (Free Tier)
<br>Select Key pair
<br>Select Common Security Group (which has 80, 22 port enabled)
<br>Click on Advanced Details and add following Userdata
<br>#!/bin/bash
sudo yum install nginx unzip -y
sudo systemctl enable --now nginx
sudo curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip
sudo unzip oxer.zip
sudo rm oxer.zip
sudo mv oxer* /usr/share/nginx/html
sudo systemctl restart nginx
<br>Click on Launch Instance.<br><br>
<br>✅ Nginx page was successfully visible on root 
<br>✅ And our template site is also visible on <a rel="noopener nofollow" class="external-link" href="http://35.160.157.246/oxer-html/" target="_blank">http://35.160.157.246/oxer-html/</a><br>
<img alt="welcome-to-nginx.png" src="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png">
<br><img alt="html-template-final-site.png" src="https://notes.sarangwandile.xyz/lib/media/html-template-final-site.png"><br><br>To create Launch Template from running instance<br>
Select your Instance Click Actions &gt;&gt; Image and Templates &gt;&gt; Create Templates from Instance<br>
<img alt="creating-launch-template-from-instance.png" src="https://notes.sarangwandile.xyz/lib/media/creating-launch-template-from-instance.png"><br>
Give Your template name and similar setting you gave in Step 1 and in Advanced tab make sure to add previous bash script as Userdata and create launch Template<br>
<img alt="summary-create-laumch-template.png" src="https://notes.sarangwandile.xyz/lib/media/summary-create-laumch-template.png"><br><br>In Ec2 Instance Left Sidebar scroll down to see Auto Scaling groups option<br>
<img alt="auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/auto-scaling-groups.png"><br>
<br>Click on Create AutoScaling group
<br>Give name, Select Launch Template
<br>Choose availability Zones
<br>No load balancer
<br>No VPC Lattice
<br>Set Health check grace period to 100
<br>Desired Capacity: 1
<br>Max desired capacity: 3
<br>For Autoscaling Policy Select Target Tracking Policy
<br>Metric Type: Average CPU Utillization
<br>Target Value: 30
<br>Instance Warmup: 80 Seconds<br>
Click on Create
<br>After Creation Click on Template Name and in Automatic Scaling Tab select Scheduled Actions option<br>
<img alt="scheduled-actions.png" src="https://notes.sarangwandile.xyz/lib/media/scheduled-actions.png"><br>Create Scheduled Action<br><img alt="scheduled-autoscaling.png" src="https://notes.sarangwandile.xyz/lib/media/scheduled-autoscaling.png"><br>Now it at that specific time your scheduled autoscaling will work the desired way and you'll get 2 instance launched minimum and maximum 2 if needed.]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-implement-template-with-scheduled-autoscaling.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Implement Template with Scheduled Autoscaling.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Monitoring Nginx logs with Cloudwatch]]></title><description><![CDATA[ 
 <br>A webserver like nginx can have some important logs that is crucial to the security of server. but as cloud provider its not convenient to always login manually to check the logs file yourself. to automate this task Cloudwatch can be a huge help to monitor the direct logs generated by nginx and other webserver. <br>Here we will see how to set up Logs monitoring with Cloudwatch.<br><br>
<br>install nginx 
<br>activate service
<br>check port 80 on browser to generate some logs
<br>check if these files get created
<br>sudo ls /var/log/nginx/
access.log  error.log
<br><br>
<br>create  IAM role
<br>give that role ec2-describe permission
<br>attach role to instance
<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br><br>
<br>install agent
<br>sudo yum install amazon-cloudwatch-agent -y
<br>
<br>enable agent service
<br>systemctl enable --now amazon-cloudwatch-agent
<br>
<br>Goto this directory
<br>cd /opt/aws/amazon-cloudwatch-agent/bin/
<br>
<br>create configuration file
<br>sudo ./amazon-cloudwatch-agent-config-wizard
<br>after finishing whole setup it will generate config.json file in that folder<br>Log file path:<br>/var/log/nginx/access.log
<br>config.json generated in bin folder<br>Try validating this config with amazon-cloudwatch-agent<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>Check over cloudwatch Log events should be visible like this<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png"><br><br>if you get error of collectd folder or file not found<br>
create an empty collectd file in that said location<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a>
]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅-monitoring-nginx-logs-with-cloudwatch.html</link><guid isPermaLink="false">AWS/Tasks Done/✅ Monitoring Nginx logs with Cloudwatch.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2]]></title><description><![CDATA[ 
 <br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br>
Also add security group rules for port 3306 for database<br>
<img alt="{26BC9CA5-9BB0-4D39-9B13-454B992FE55F}.png" src="https://notes.sarangwandile.xyz/lib/media/{26bc9ca5-9bb0-4d39-9b13-454b992fe55f}.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br><br><a rel="noopener nofollow" class="external-link" href="https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql" target="_blank">https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql</a><br><br>CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';
grant create, alter, drop, insert, update, delete, select on demo.persons to 'sammy'@'localhost';
grant select on demo.* to 'sammy'@'localhost';           
show grants for 'sammy'@'localhost';

<br><br>Install tomcat, git, maven<br><br>Install this specific version from source<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 

<br>hit the instance ip:8080 / curl the ip<br>
You should see tomcat webpage<br>
<img alt="{BF5D483E-E314-42B8-A790-36EC904C637B}.png" src="https://notes.sarangwandile.xyz/lib/media/{bf5d483e-e314-42b8-a790-36ec904c637b}.png"><br><br>sudo yum install git -y
<br><br>sudo yum install maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br>build application<br>cd student-ui
mvn clean package
<br>this must have created .war file in target folder<br>
rename it to suitable short name and move to apache-tomcat's webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war ../apache-tomcat-9.0.97/webapps/
<br>hit the instance ip:8080/studentapp<br>
<img alt="{17627F7C-22F9-448B-B926-BE826D130D98}.png" src="https://notes.sarangwandile.xyz/lib/media/{17627f7c-22f9-448b-b926-be826d130d98}.png"><br>Upon filling the form if you get this<br>
<img alt="{9A94ABEB-EA83-48BB-A58B-36203206528C}.png" src="https://notes.sarangwandile.xyz/lib/media/{9a94abeb-ea83-48bb-a58b-36203206528c}.png"><br>
It means you have not connected RDS with this webapp<br>
lets do this<br><br>mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br><br>CREATE DATABASE studentapp;
use studentapp;
CREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,
	student_name VARCHAR(100) NOT NULL,
    student_addr VARCHAR(100) NOT NULL,
	student_age VARCHAR(3) NOT NULL,
	student_qual VARCHAR(20) NOT NULL,
	student_percent VARCHAR(10) NOT NULL,
	student_year_passed VARCHAR(10) NOT NULL,
	PRIMARY KEY (student_id)
);
<br>show database;
use studentapp;
show tables;
desc students;
<br><img alt="{E35F872A-AB74-43BC-BACD-DE202683E43E}.png" src="https://notes.sarangwandile.xyz/lib/media/{e35f872a-ab74-43bc-bacd-de202683e43e}.png"><br>Ir should look like this<br>
<img alt="{5ABCCE77-EF84-4615-957E-4DD5E23E430E}.png" src="https://notes.sarangwandile.xyz/lib/media/{5abcce77-ef84-4615-957e-4dd5e23e430e}.png"><br><br>goto conf folder in apache-tomcat source folder<br>cd ../conf
#and edit context.xml file
vi context.xml
<br>and paste this code between &lt;context&gt;&lt;/context&gt; block<br>&lt;Resource name="jdbc/TestDB" auth="Container" type="javax.sql.DataSource"
           maxTotal="500" maxIdle="30" maxWaitMillis="1000"
           username="admin" password="Passw0rd123" driverClassName="com.mysql.jdbc.Driver"
           url="jdbc:mysql://database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com:3306/studentapp?useUnicode=yes&amp;amp;characterEncoding=utf8"/&gt;
<br>make sure to edit username and password values with your own and url to your hostaname of database<br><br>copy this url - <a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar</a><br>Place the mysql connector library (mysql-connector.jar) into /lib folder of apache-tomcat directory by using curl or wget<br>cd ../lib
curl -O https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar
<br>now that library file is placed into its correct location lets restart catalina<br>
but first navigate back to bin directory where catalina.sh binary is located<br>cd ../bin
bash catalina.sh stop
bash catalina.sh start
<br>Now navigate to browser with this url<br>
ec2-instance-ip:8080/studentapp<br>now registration data should be able to passed into database<br>After the filling the registration form login to mysql database and check if the data is present in the table now.<br>mysql -h &lt;rds endpoint&gt; -u admin -p
show databases;
use studentapp;
show tables;
select * from students;
]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/✅🐈‍⬛host-tomcat-basesd-web-app-called-student-app-with-rds-and-ec2.html</link><guid isPermaLink="false">AWS/Tasks Done/✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[✅ Tasks To Do]]></title><description><![CDATA[ 
 <br>Practical Tasks to perform<br>✅ = finished<br>
☑️ = done but not satisfied<br><br><br><br><br>]]></description><link>https://notes.sarangwandile.xyz/aws/✅-tasks-to-do.html</link><guid isPermaLink="false">AWS/✅ Tasks To Do.md</guid><pubDate>Sun, 29 Dec 2024 14:47:03 GMT</pubDate></item><item><title><![CDATA[🗒️ Tasks to Read]]></title><description><![CDATA[ 
 <br>📄 Tasks to read<br>
🏡 Read the topics in home.<br><br>Youtube Channel Recommendations <br>
<br>nehra classes
<br>gaurav sharma
<br>abhishek veermalla
]]></description><link>https://notes.sarangwandile.xyz/aws/🗒️-tasks-to-read.html</link><guid isPermaLink="false">AWS/🗒️ Tasks to Read.md</guid><pubDate>Sun, 29 Dec 2024 14:45:40 GMT</pubDate></item><item><title><![CDATA[2024-11-18]]></title><description><![CDATA[ 
 <br>EC2  <br>
<br>instances
<br>security groups
<br>key
<br>ebs, snapshot backup, ami
<br>lifecycle policy
<br>load balancer 
<br>auto scaling groups
<br>VPC<br>
<br>vpc 
<br>subnet 
<br>route table
<br>network
<br>igw
<br>nat gat
<br>publi priv
<br>peering
<br>IAM<br>
<br>Identity Access Management
<br>Idenity ---&gt; user<br>
Permissions --&gt; policy<br>group ---&gt; developer<br>
devops<br>
<br>
Two types of policies<br>
- aws managed policy<br>
- customer managed policy<br>iam components<br>
user<br>
group<br>
policy<br>
role<br>tasks:<br>
<br>direct policy
<br>inline policy
<br>To read

<br>difference between inline policy and customer managed policy
<br>Types of policies in IAM
<br>Types of users in aws in IAM

<br>Types of policy<br>
<br>AWS Managed Policy
<br>Customer Managed Policy
<br>Direct Inline Policy
<br>Types of User<br>
<br>IAM
<br>Root User
<br>IAM Identity Center User
<br>Federeted Identity
<br>AWS Builder ID User
<br>Policy Types<br>
<br>Identity Based
<br>Resource Based
<br>Service control policies
<br>Permission Boundary Policies
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-18.html</link><guid isPermaLink="false">Daily Notes/2024-11-18.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[2024-11-18 EC2 VPC and stuffs]]></title><description><![CDATA[ 
 <br>EC2  <br>
<br>instances
<br>security groups
<br>key
<br>ebs, snapshot backup, ami
<br>lifecycle policy
<br>load balancer 
<br>auto scaling groups
<br>VPC<br>
<br>vpc 
<br>subnet 
<br>route table
<br>network
<br>igw
<br>nat gat
<br>publi priv
<br>peering
<br>IAM<br>
<br>Identity Access Management
<br>Idenity ---&gt; user<br>
Permissions --&gt; policy<br>group ---&gt; developer<br>
devops<br>
<br>
Two types of policies<br>
- aws managed policy<br>
- customer managed policy<br>iam components<br>
user<br>
group<br>
policy<br>
role<br>tasks:<br>
<br>direct policy
<br>inline policy
<br>To read

<br>difference between inline policy and customer managed policy
<br>Types of policies in IAM
<br>Types of users in aws in IAM

<br>Types of policy<br>
<br>AWS Managed Policy
<br>Customer Managed Policy
<br>Direct Inline Policy
<br>Types of User<br>
<br>IAM
<br>Root User
<br>IAM Identity Center User
<br>Federeted Identity
<br>AWS Builder ID User
<br>Policy Types<br>
<br>Identity Based
<br>Resource Based
<br>Service control policies
<br>Permission Boundary Policies
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-18-ec2-vpc-and-stuffs.html</link><guid isPermaLink="false">Daily Notes/2024-11-18 EC2 VPC and stuffs.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3]]></title><description><![CDATA[ 
 <br><br>
S3 is global service<br>
but bucket is region specific
<br>in bucket things stored in the form of object<br>storage classes<br><a rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/" target="_blank">https://aws.amazon.com/s3/storage-classes/</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-19.html</link><guid isPermaLink="false">Daily Notes/2024-11-19.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3 Task]]></title><description><![CDATA[ 
 <br><br>
<br>Create an IAM Policy:

<br>First, create an IAM policy that defines the permissions for the specific S3 bucket(s).
<br>This policy should follow the principle of least privilege, granting only the necessary permissions.


<br>Attach the Policy to the IAM User:

<br>Go to the IAM console and select the user.
<br>Choose "Add permissions" and then "Attach existing policies directly".
<br>Find and select the policy you created for S3 bucket access.


<br>Example Policy (customize as needed):
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}


<br>    
4. Security Considerations:
    - Always use specific bucket ARNs instead of wildcards.
    - Grant only the necessary permissions (e.g., read-only if that's all that's required).
    - Consider using IAM groups for easier management if multiple users need the same access.
	
5. Best Practices:
    - Regularly review and audit permissions.
    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.
    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.
	
6. Testing:
    - After applying the policy, test the access in a safe, non-production environment.
    - Verify that the user can only access the intended buckets and perform only the allowed actions.

Remember, it's crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.


# Static web hosting
Create s3 bucket
properties --&gt; enable static web hosting
permission --&gt; turn off block public access
					--&gt; object ownership --&gt; enable ACL
					--&gt; ACL --&gt; enable everyone list and read
					--&gt; upload files

]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-22.html</link><guid isPermaLink="false">Daily Notes/2024-11-22.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3 Task]]></title><description><![CDATA[ 
 <br><br>
<br>Create an IAM Policy:

<br>First, create an IAM policy that defines the permissions for the specific S3 bucket(s).
<br>This policy should follow the principle of least privilege, granting only the necessary permissions.


<br>Attach the Policy to the IAM User:

<br>Go to the IAM console and select the user.
<br>Choose "Add permissions" and then "Attach existing policies directly".
<br>Find and select the policy you created for S3 bucket access.


<br>Example Policy (customize as needed):
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}


<br>    
4. Security Considerations:
    - Always use specific bucket ARNs instead of wildcards.
    - Grant only the necessary permissions (e.g., read-only if that's all that's required).
    - Consider using IAM groups for easier management if multiple users need the same access.
	
5. Best Practices:
    - Regularly review and audit permissions.
    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.
    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.
	
6. Testing:
    - After applying the policy, test the access in a safe, non-production environment.
    - Verify that the user can only access the intended buckets and perform only the allowed actions.

Remember, it's crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.


# Static web hosting
Create s3 bucket
properties --&gt; enable static web hosting
permission --&gt; turn off block public access
					--&gt; object ownership --&gt; enable ACL
					--&gt; ACL --&gt; enable everyone list and read
					--&gt; upload files

]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-22-s3-task.html</link><guid isPermaLink="false">Daily Notes/2024-11-22 s3 task.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Containerizing-wordpress-app]]></title><description><![CDATA[ 
 <br><br>and migrating from database<br><br>Dockerfile for wordpress<br>
<img alt="hello-ss-wordpress-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"><br>docker-compose file<br>
<img alt="docker-compose-file-for-wordpress.png" src="https://notes.sarangwandile.xyz/lib/media/docker-compose-file-for-wordpress.png"><br>mysql docker file<br>
<img alt="hello-ss-mysql-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-mysql-container.png"><br>myPassw0rd<br>'wpuser123'@'localhost' IDENTIFIED BY 'wpuserPassw0rd';<br>|Site Title|wp-testsite|<br>
|Username|wpwebuser |<br>
|password|)5S@PsG1J$VACnJ@%Q|<br>
|email|<a data-tooltip-position="top" aria-label="mailto:myemail@example.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:myemail@example.com" target="_blank">myemail@example.com</a>|<br>
|||<br>Docker mariadb password<br>
wpuser123Passw0rd<br><a rel="noopener nofollow" class="external-link" href="https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/" target="_blank">https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/</a><br><a rel="noopener nofollow" class="external-link" href="https://devhints.io/mysql" target="_blank">https://devhints.io/mysql</a><br><a rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">https://docs.docker.com/engine/install/ubuntu/</a><br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a><br>#exporting/backuping old mysql data
mysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql

#current mysql configuration
mysql user: wpuser123
mysql passwd: wpuserPassw0rd
db name: new_db

#login to ec2 and run the backup command
mysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql

<br><br>
<br>create new folder in wordpress<br>
cd into it
<br>mkdir wordpress &amp;&amp; cd wordpress
<br>
<br>
pull wordpress and mariadb containers<br>
sudo docker pull wordpress<br>
sudo docker pull mariadb

<br>
create custom docker network<br>
sudo docker network wp-app<br>
sudo docker network ls

<br>
Creation of mariadb database container

<br>
<br>hash the password of db first
<br>echo `openssl passwd` | sudo tee $PWD/.db-pass
<br>  now hashed password file must be in .db-pass file<br>
here i have set myPassw0rd password<br>cat .db-pass
$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0
<br>
<br>Create mariaDB data directory to mount container database with /var/lib/mysql<br>
sudo mkdir -p data
<br>~/wordpress 
	- data
<br>
<br>Create and run Mariadb Docker container
<br>sudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v '/home/ubuntu/wordpress/data:/var/lib/mysql' --name wp-mariadb mariadb
<br>
<br>check health and logs<br>
docker ps
<br>sudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log
<br>example:<br>sudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log
<br>
<br>check mounted volume for generated data<br>
ls -1 data
<br>regarding migration of old database<br>
copy the backup file into container db folder<br>
cp mywpold_db.sql wordpress/data/<br>
<br>login to mariadb container
<br>docker exec -it wp-mariadb bash
<br>restore old db<br>cd /var/lib/mysql
mariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811" target="_blank">https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/backup-and-restore-overview/" target="_blank">https://mariadb.com/kb/en/backup-and-restore-overview/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217" target="_blank">https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-23.html</link><guid isPermaLink="false">Daily Notes/2024-11-23.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Containerizing-wordpress-app]]></title><description><![CDATA[ 
 <br><br>and migrating from database<br><br>Dockerfile for wordpress<br>
<img alt="hello-ss-wordpress-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"><br>docker-compose file<br>
<img alt="docker-compose-file-for-wordpress.png" src="https://notes.sarangwandile.xyz/lib/media/docker-compose-file-for-wordpress.png"><br>mysql docker file<br>
<img alt="hello-ss-mysql-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-mysql-container.png"><br>myPassw0rd<br>'wpuser123'@'localhost' IDENTIFIED BY 'wpuserPassw0rd';<br>|Site Title|wp-testsite|<br>
|Username|wpwebuser |<br>
|password|)5S@PsG1J$VACnJ@%Q|<br>
|email|<a data-tooltip-position="top" aria-label="mailto:myemail@example.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:myemail@example.com" target="_blank">myemail@example.com</a>|<br>
|||<br>Docker mariadb password<br>
wpuser123Passw0rd<br><a rel="noopener nofollow" class="external-link" href="https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/" target="_blank">https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/</a><br><a rel="noopener nofollow" class="external-link" href="https://devhints.io/mysql" target="_blank">https://devhints.io/mysql</a><br><a rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">https://docs.docker.com/engine/install/ubuntu/</a><br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a><br>#exporting/backuping old mysql data
mysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql

#current mysql configuration
mysql user: wpuser123
mysql passwd: wpuserPassw0rd
db name: new_db

#login to ec2 and run the backup command
mysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql

<br><br>
<br>create new folder in wordpress<br>
cd into it
<br>mkdir wordpress &amp;&amp; cd wordpress
<br>
<br>
pull wordpress and mariadb containers<br>
sudo docker pull wordpress<br>
sudo docker pull mariadb

<br>
create custom docker network<br>
sudo docker network wp-app<br>
sudo docker network ls

<br>
Creation of mariadb database container

<br>
<br>hash the password of db first
<br>echo `openssl passwd` | sudo tee $PWD/.db-pass
<br>  now hashed password file must be in .db-pass file<br>
here i have set myPassw0rd password<br>cat .db-pass
$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0
<br>
<br>Create mariaDB data directory to mount container database with /var/lib/mysql<br>
sudo mkdir -p data
<br>~/wordpress 
	- data
<br>
<br>Create and run Mariadb Docker container
<br>sudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v '/home/ubuntu/wordpress/data:/var/lib/mysql' --name wp-mariadb mariadb
<br>
<br>check health and logs<br>
docker ps
<br>sudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log
<br>example:<br>sudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log
<br>
<br>check mounted volume for generated data<br>
ls -1 data
<br>regarding migration of old database<br>
copy the backup file into container db folder<br>
cp mywpold_db.sql wordpress/data/<br>
<br>login to mariadb container
<br>docker exec -it wp-mariadb bash
<br>restore old db<br>cd /var/lib/mysql
mariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811" target="_blank">https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/backup-and-restore-overview/" target="_blank">https://mariadb.com/kb/en/backup-and-restore-overview/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217" target="_blank">https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-23-does-not-belong-here.html</link><guid isPermaLink="false">Daily Notes/2024-11-23 does not belong here.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-25]]></title><description><![CDATA[ 
 <br>aws configure<br>
aws cli<br>to create new bucket<br>
aws s3 mb s3://spiderman-cdec-devops123<br>to list all buckets<br>
aws s3 ls]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-25.html</link><guid isPermaLink="false">Daily Notes/2024-11-25.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[2024-11-26]]></title><description><![CDATA[ 
 <br>cloudwatch<br>after installing nginx logs must be present<br>sudo ls /var/log/nginx/
access.log  error.log
<br>to send these logs to cloudwatch<br>create role<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br>attach role to instance<br>install agent<br>
<a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a><br>enable agent service<br>
systemctl <br>cd /opt/aws<br>create configuration file<br>
sudo /opt/aws/amazon-cloudwatch-agent/b<br>
in/amazon-cloudwatch-agent-config-wizard<br>Log file path:<br>
/var/log/nginx/access.log<br>config.json generated in bin folder<br>autorized vs authenticated read<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>recap<br>
<br>Created ec2-instance
<br>install nginx
<br>ec2-describe permission<br>collectd file create<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png">]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-26.html</link><guid isPermaLink="false">Daily Notes/2024-11-26.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-26 Cloudwatch class]]></title><description><![CDATA[ 
 <br>cloudwatch<br>after installing nginx logs must be present<br>sudo ls /var/log/nginx/
access.log  error.log
<br>to send these logs to cloudwatch<br>create role<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br>attach role to instance<br>install agent<br>
<a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a><br>enable agent service<br>
systemctl <br>cd /opt/aws<br>create configuration file<br>
sudo /opt/aws/amazon-cloudwatch-agent/b<br>
in/amazon-cloudwatch-agent-config-wizard<br>Log file path:<br>
/var/log/nginx/access.log<br>config.json generated in bin folder<br>autorized vs authenticated read<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>recap<br>
<br>Created ec2-instance
<br>install nginx
<br>ec2-describe permission<br>collectd file create<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png">]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-26-cloudwatch-class.html</link><guid isPermaLink="false">Daily Notes/2024-11-26 Cloudwatch class.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-27]]></title><description><![CDATA[ 
 <br>sns - notification service<br>
attach filter before creating alarm<br>
create alarm<br>get graph<br>
get notification]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-27.html</link><guid isPermaLink="false">Daily Notes/2024-11-27.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Creating RDS database]]></title><description><![CDATA[ 
 <br><br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br>Scratchpad
HA - high availability<br>
n-1<br>
3306 maridb sql<br>
sudo yum list<br>
installing mariadb client<br>
yum install mariadb105<br>
diagram<br>
default vpc<br>
3 public subnet<br>
1 subnet --&gt; ec2<br>
2 subnet --&gt; rds<br>
endpoint access<br>
mysql -h  -u  -p&gt; 
<br><img alt="rds-diagram-1.png" src="https://notes.sarangwandile.xyz/lib/media/rds-diagram-1.png"><br>create database demo;

show database;

use demo;

drop database demo;

CREATE TABLE Persons (  
    PersonID int,  
    LastName varchar(255),  
    FirstName varchar(255),  
    Address varchar(255),  
    City varchar(255)  
);

desc Persons;

Insert Into Persons (PersonID, LastName, FirstName) values (1, "DemoName", "Amir");

select * from Persons;

https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql

1. CREATE USER ' ==sammy=='@'localhost' IDENTIFIED BY '==password==';

grant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;

grant select on demo.* to "sammy"@"%";           

show grants for 

<br>install tomcat
build application


create user
CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';

show user
SELECT User, Host FROM mysql.user;



]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-28.html</link><guid isPermaLink="false">Daily Notes/2024-11-28.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Creating RDS database]]></title><description><![CDATA[ 
 <br><br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br>Scratchpad
HA - high availability<br>
n-1<br>
3306 maridb sql<br>
sudo yum list<br>
installing mariadb client<br>
yum install mariadb105<br>
diagram<br>
default vpc<br>
3 public subnet<br>
1 subnet --&gt; ec2<br>
2 subnet --&gt; rds<br>
endpoint access<br>
mysql -h  -u  -p&gt; 
<br><img alt="rds-diagram-1.png" src="https://notes.sarangwandile.xyz/lib/media/rds-diagram-1.png"><br>create database demo;

show database;

use demo;

drop database demo;

CREATE TABLE Persons (  
    PersonID int,  
    LastName varchar(255),  
    FirstName varchar(255),  
    Address varchar(255),  
    City varchar(255)  
);

desc Persons;

Insert Into Persons (PersonID, LastName, FirstName) values (1, "DemoName", "Amir");

select * from Persons;

https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql

1. CREATE USER ' ==sammy=='@'localhost' IDENTIFIED BY '==password==';

grant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;

grant select on demo.* to "sammy"@"%";           

show grants for 

<br>install tomcat
build application


create user
CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';

show user
SELECT User, Host FROM mysql.user;



]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-28-creating-rds.html</link><guid isPermaLink="false">Daily Notes/2024-11-28 Creating RDS.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-30]]></title><description><![CDATA[ 
 <br><br>download zip file<br>
unzip it<br>sudo yum install java-17<br>Info
in ubuntu java packages are called<br>
jdk and jre<br>
java development kit and java runtime
<br>cd into extracted folder<br>cd bin<br>bash catalina.sh start<br>git clone <a rel="noopener nofollow" class="external-link" href="https://github.com/cholekulche/BE-application-student.git" target="_blank">https://github.com/cholekulche/BE-application-student.git</a><br>cd into it<br>
install maven<br>
sudo yum install maven<br>build appliaction<br>
mvn clean package<br>Info
first install java before maven because<br>
maven will install java latest version which might conflict with required versions
<br>cd target<br>code<br>
artifact application<br> mv studentapp-2.2-SNAPSHOT.war studentapp.war<br> mv studentapp.war ../../apache-tomcat-9.0.97/webapps/.<br>cd ../../apache-tomcat-9.0.97/webapps/<br>.war file will get extracted automatically into student app<br>check ip:8080/studentapp<br><img alt="student-app-final-page.png" src="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png"><br>continue to <a data-href="2024-12-02" href="https://notes.sarangwandile.xyz/daily-notes/2024-12-02.html" class="internal-link" target="_self" rel="noopener nofollow">2024-12-02</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-30.html</link><guid isPermaLink="false">Daily Notes/2024-11-30.md</guid><pubDate>Mon, 30 Dec 2024 16:00:30 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-02]]></title><description><![CDATA[ 
 <br>continued from <a data-href="2024-11-30" href="https://notes.sarangwandile.xyz/daily-notes/2024-11-30.html" class="internal-link" target="_self" rel="noopener nofollow">2024-11-30</a><br>create database<br>
copy whole command<br>show database;<br>use studentapp;<br>show tables;<br><br>cd apache-tomcat/conf<br>
vi context.xml<br>edit jdbc connector <br>&lt;Resource Name ... &gt;
		  edit stuffs
		  adding rds hosting etc
<br>url/mysql-connector.jar<br>
put it into tomcat/lib/<br>curl -O url<br>bash catalina stop<br>
bash catalina start<br>restart catalina <br>try filling form<br>if get error check catalina*.log<br><br>pritam sir repo student-ui app<br><br>route53]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-02.html</link><guid isPermaLink="false">Daily Notes/2024-12-02.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-03]]></title><description><![CDATA[ 
 <br><br>ip to name<br>
name to ip <br>dns record max limit 10k <br>route53 &gt;&gt; hosted zone &gt;&gt; create<br>
update nameserver in godaddy<br>
<img alt="{78B1E5B8-5F9F-4C45-A995-997C82837901} 1.png" src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png"><br>
<br>before updating nameserver<br>
<img alt="{E7EF4A45-A02D-40A7-BEA5-6D5466BDCC60}.png" src="https://notes.sarangwandile.xyz/lib/media/{e7ef4a45-a02d-40a7-bea5-6d5466bdcc60}.png">
<br>create s3 bucket<br>
enable static web hosting add files <br>route53 &gt;&gt; hosted zone &gt;&gt; domain name &gt;&gt; create records<br>skip subdomain select A records<br>
alias on<br>
alias to s3 bucket endpoint<br>
region select<br>skip s3 bucket part<br>
create ec2 instance<br>
install nginx<br>
make site available<br>add ip in values in create record option<br>learndevops.live<br> search acm service in aws<br>
certificate manager<br>create<br>
create records in r53<br>routing policy<br>
simple routing]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-03.html</link><guid isPermaLink="false">Daily Notes/2024-12-03.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-04]]></title><description><![CDATA[ 
 <br>
<br>deploy tomcat instance --&gt; no database attach
<br>make ami of it
<br>autoscaling with load balancer
<br>mount efs which will have tomcat
<br>two instances
<br><img alt="{78B1E5B8-5F9F-4C45-A995-997C82837901}.png" src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png"><br>
new task implement failover policy on nginx instance<br>To read --&gt; blue green deployment strategy <br>manually implement it on instance <br>Toread -- &gt;canary deployment]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-04.html</link><guid isPermaLink="false">Daily Notes/2024-12-04.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-07]]></title><description><![CDATA[ 
 <br><br>cache<br>CDN - content delivery network<br>x86 and arm<br><br>name to ip 
ip to name
<br>.in .com -- &gt; registrar<br>doc.aws.amazon.com  --&gt; it reads from right to left<br>Task: read how url works<br>latency - time between request and response<br>edge location (cdn location)]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-07.html</link><guid isPermaLink="false">Daily Notes/2024-12-07.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-10]]></title><description><![CDATA[ 
 <br>cloudfront vs cloudwatch<br>cloudfront<br>
cdn<br>
cache<br>cloudwatch<br>
logs monitoring tool<br>s3<br>
loadbalancer<br>autoscaling<br><br>lambda<br>serverless --&gt; managed by aws<br>lambda function to start and stop ec2 instances<br>boto3 library]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-10.html</link><guid isPermaLink="false">Daily Notes/2024-12-10.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-11]]></title><description><![CDATA[ 
 <br><br><br>
<br>Make new user
<br>attach policy according to task
<br>granular level permission
<br><br>user will create s3 bucket, instance, db, 
instance will have:
	tomcat link to rds
	its logs goes to s3 bucket
	tomcat will be accessible via domain and not via instance ip
routing policy simple
user cant have admin access 
user cant access iam
<br><br><br>  Created custom Policy policy4tom and added permission for ec2 full access<br>{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "VisualEditor0",
			"Effect": "Allow",
			"Action": [
				"rds:*",
				"s3:*",
				"ec2:*"
			],
			"Resource": "*"
		}
	]
}
<br>and set permission boundary for Administrator access<br>
<img alt="Pasted image 20241213154148.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"><br>Created EC2 Instance with security group rule for port 8080 enabled<br>
and added this user data<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 
<br>after running instance check if tomcat page shows in port 8080<br><img alt="{51C73FFF-C61A-4231-BA6B-783F6BD22F4D}.png" src="https://notes.sarangwandile.xyz/lib/media/{51c73fff-c61a-4231-ba6b-783f6bd22f4d}.png"><br>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-11.html</link><guid isPermaLink="false">Daily Notes/2024-12-11.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-11 AWS Task]]></title><description><![CDATA[ 
 <br><br><br>
<br>Make new user
<br>attach policy according to task
<br>granular level permission
<br><br>user will create s3 bucket, instance, db, 
instance will have:
	tomcat link to rds
	its logs goes to s3 bucket
	tomcat will be accessible via domain and not via instance ip
routing policy simple
user cant have admin access 
user cant access iam
<br><br><br>  Created custom Policy policy4tom and added permission for ec2 full access<br>{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "VisualEditor0",
			"Effect": "Allow",
			"Action": [
				"rds:*",
				"s3:*",
				"ec2:*"
			],
			"Resource": "*"
		}
	]
}
<br>and set permission boundary for Administrator access<br>
<img alt="Pasted image 20241213154148.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"><br>Created EC2 Instance with security group rule for port 8080 enabled<br>
and added this user data<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 
<br>after running instance check if tomcat page shows in port 8080<br><img alt="{51C73FFF-C61A-4231-BA6B-783F6BD22F4D}.png" src="https://notes.sarangwandile.xyz/lib/media/{51c73fff-c61a-4231-ba6b-783f6bd22f4d}.png"><br>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-11-aws-task.html</link><guid isPermaLink="false">Daily Notes/2024-12-11 AWS Task.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-14]]></title><description><![CDATA[ 
 <br>aws exam]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-14.html</link><guid isPermaLink="false">Daily Notes/2024-12-14.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-17]]></title><description><![CDATA[ 
 <br>SDLC<br>
Agile<br>
waterfall]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-17.html</link><guid isPermaLink="false">Daily Notes/2024-12-17.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-18]]></title><description><![CDATA[ 
 <br>Git Theory Explained]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-18.html</link><guid isPermaLink="false">Daily Notes/2024-12-18.md</guid><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-19 Git Class]]></title><description><![CDATA[ 
 <br>DVCS vs CVS -- further expanded in <a data-href="Git Theory" href="https://notes.sarangwandile.xyz/devops/for-reading/git-theory.html" class="internal-link" target="_self" rel="noopener nofollow">Git Theory</a><br><br><img alt="DVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"><br><br><img alt="CVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"><br><br>
<br>installing git
<br>signup to github.com
<br><br><br>
<br>git add
<br>git commit
<br>git push
<br>git pull
<br><br><br>Going back to previous commit to and rebasing<br>Seeing git logs<br>
git log<br>Start interactive rebase<br>
git rebase -i &lt;commit hash&gt;<br>In the editor, choose to edit the commit identified in step 3 by changing&nbsp;pick&nbsp;to&nbsp;edit&nbsp;on the first line of the text.<br>edit 8728dbe67 my second commit message
pick 03d69e5d3 my third commit message
pick 8053f7b27 my fourth commit message
<br>make changes and git add . <br>
<br>git commit --amend 
<br>git rebase --continue
<br>git push
<br>removing some files from git index<br>git rm --cached -r folder or files
<br>Learned to create .gitignore file<br>.trash/
.obsidian/
private
<br><br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.github.com/en/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#removing-a-secret-introduced-by-an-earlier-commit-on-your-branch" rel="noopener nofollow" class="external-link" href="https://docs.github.com/en/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#removing-a-secret-introduced-by-an-earlier-commit-on-your-branch" target="_blank">Removing secret introduced by earler commit</a>
<br><a data-tooltip-position="top" aria-label="https://www.freecodecamp.org/news/gitignore-file-how-to-ignore-files-and-folders-in-git/" rel="noopener nofollow" class="external-link" href="https://www.freecodecamp.org/news/gitignore-file-how-to-ignore-files-and-folders-in-git/" target="_blank">How to ignore files in git</a>
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://jvns.ca/blog/2024/01/26/inside-git/" target="_blank">https://jvns.ca/blog/2024/01/26/inside-git/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://githowto.com/git_internals_git_directory" target="_blank">https://githowto.com/git_internals_git_directory</a>
<br><a data-href="Some talks about git in reddit 1" href="https://notes.sarangwandile.xyz/aws/for-reading/some-talks-about-git-in-reddit-1.html" class="internal-link" target="_self" rel="noopener nofollow">Some talks about git in reddit 1</a>
<br><a rel="noopener nofollow" class="external-link" href="https://git-scm.com/book/en/v2" target="_blank">https://git-scm.com/book/en/v2</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-19-git-class.html</link><guid isPermaLink="false">Daily Notes/2024-12-19 Git Class.md</guid><pubDate>Sun, 29 Dec 2024 14:56:24 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-23 docker-day]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br>
Before Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture<br><br><br>
<br>Single unified application
<br><br>
<br>one app divided in chunks
<br><br><img style="max-width:400px; " class="excalidraw-svg excalidraw-embedded-img excalidraw-canvas-immersive" src="https://notes.sarangwandile.xyz/blob://d1b8b7ac-fafb-4035-9519-dc7cfd89b9df" filesource="Drawings/Docker-Architecture-diagram_1.excalidraw.md" w="400" draggable="false" oncanvas="false"><br><br>
<br>its like small virtual machines
<br>containers are free to use all the rams they needs
<br>whereas in vms softwares has limited prealloted ram to use
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/" target="_blank">https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://adventofdocker.com" target="_blank">https://adventofdocker.com</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-23-docker-day.html</link><guid isPermaLink="false">Daily Notes/2024-12-23 docker-day.md</guid><pubDate>Wed, 25 Dec 2024 13:33:38 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-24 docker basics]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20241224092817.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"><br>
docker client ---&gt; docker commands<br>
docker machine ---&gt; docker host<br>ubuntu package name docker.i<br>in amazon-linux<br>
yum install docker -y<br>enable daemon<br>
systemctl enable --now docker<br>docker pull nginx<br>Tip
Images can also be pulled from SHA Id.
<br>docker run nginx<br>-d detached mode<br>
ps list running containers<br>
-a list all containers stopped and running<br>
rm remove stopped containers<br>
kill kill the containers<br>
-P expose container on random host port ]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-24-docker-basics.html</link><guid isPermaLink="false">Daily Notes/2024-12-24 docker basics.md</guid><pubDate>Tue, 24 Dec 2024 04:29:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-30]]></title><description><![CDATA[ 
 <br>Task 1: <a data-href="Task 4 Create daemon service of tomcat" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html" class="internal-link" target="_self" rel="noopener nofollow">Task 4 Create daemon service of tomcat</a><br><a data-href="read about docker.sock file" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html" class="internal-link" target="_self" rel="noopener nofollow">read about docker.sock file</a><br>persistence storage<br>
docker volume<br>
docker volume ls<br>creating docker volume<br>
docker volume create dev<br>Root directory of docker<br>
cd /var/lib/docker/<br>all systemd service files lies in<br>
/etc/systemd/system/<br>configuration files<br>
/etc/docker<br>socket file<br>
/var/run/docker.sock<br>docker run -d -P -v dev:/usr/share/nginx/html nginx<br>Tommorow<br>
dockerfile<br>
network]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-30.html</link><guid isPermaLink="false">Daily Notes/2024-12-30.md</guid><pubDate>Mon, 30 Dec 2024 14:59:04 GMT</pubDate></item><item><title><![CDATA[Agile Development]]></title><description><![CDATA[ 
 <br><br>Agile Manifesto
The agile manifesto outlines 4 values and 12 principles for teams, but—decades later—is it still relevant? Find out
<br>Scrum
In scrum, a product is built in a series of fixed-length iterations called sprints, giving agile teams a framework for shipping software on a regular cadence. Learn how the scrum methodology impacts traditional project management.
<br>Kanban
Kanban is a popular agile framework that requires real-time communication of team's capacity and full transparency of work. Learn how the kanban methodology for agile software development can benefit for your team.
<br><br>The Agile methodology is a project management approach that involves breaking the project into phases and emphasizes continuous collaboration and improvement. Teams follow a cycle of planning, executing, and evaluating.<br>Whereas the traditional "waterfall" approach has one discipline contribute to the project, then "throw it over the wall" to the next contributor, agile calls for collaborative cross-functional teams. Open communication, collaboration, adaptation, and trust amongst team members are at the heart of agile. Although the project lead or product owner typically prioritizes the work to be delivered, the team takes the lead on deciding how the work will get done, self-organizing around granular tasks and assignments.<br>Agile isn't defined by a set of ceremonies or specific development techniques. Rather, agile is a group of methodologies that demonstrate a commitment to tight feedback cycles and continuous improvement.<br><img alt="Agile development example | Atlassian Agile Coach" src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The original&nbsp;<a data-tooltip-position="top" aria-label="http://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="http://agilemanifesto.org/" target="_blank">Agile Manifesto</a>&nbsp;didn't prescribe two-week iterations or an ideal team size. It simply laid out a set of core values that put people first. The way you and your team live those values today – whether you do scrum by the book, or blend elements of kanban and XP – is entirely up to you.<br><br>Teams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year's worth of plans. "Just enough" planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.<br>But it's not just a numbers game—first and foremost, it's about people. As described by the Agile Manifesto, authentic human interactions are more important than rigid processes. Collaborating with customers and teammates is more important than predefined arrangements. And delivering a working solution to the customer's problem is more important than hyper-detailed documentation.<br>An agile team unites under a shared vision, then brings it to life the way they know is best. Each team sets their own standards for quality, usability, and completeness. Their "definition of done" then informs how fast they'll churn the work out. Although it can be scary at first, company leaders find that when they put their trust in an agile team, that team feels a greater sense of ownership and rises to meet (or exceed) management's expectations.<br><br>The publication of the Agile Manifesto in 2001 marks the birth of agile as a methodology. Since then, many agile frameworks have emerged such as scrum,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" target="_blank">lean</a>, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way.&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/scrum" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/scrum" target="_blank">Scrum</a>&nbsp;and XP are favored by software development teams, while&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>&nbsp;is a darling among service-oriented teams like IT or human resources.<br>Today, many agile teams combine practices from a few different frameworks, spiced up with practices unique to the team. Some teams adopt some agile rituals&nbsp;(like regular stand-ups, retros, backlogs, etc.), while others created a&nbsp;new agile practice (<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" target="_blank">agile marketing teams</a>&nbsp;who adhere to the&nbsp;Agile Marketing Manifesto).<br><img alt="Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:8631b273-6575-4586-9a9a-f5eb84d4b46f/illustrations-spot-Code%20Release%202.svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The agile teams of tomorrow will value their own effectiveness over adherence to doctrine. Openness, trust, and autonomy are emerging as the cultural currency for companies who want to attract the best people and get the most out of them. Such companies are already proving that practices can vary across teams, as long as they're guided by the right principles.<br><br>The way each team practices agile should be unique to their needs and culture. Indeed, no two teams inside Atlassian have identical agile practices.<br>Although many of our teams organize their work in sprints, estimate in story points, and prioritize their backlogs, we're not die-hard practitioners of scrum. Or kanban. Or any other trademarked methodology. Instead, we give each team the autonomy to cherry-pick the practices that will make them most effective. And we encourage you to take a similar approach.<br>For example, if you're on a queue-oriented team like IT, kanban provides a solid foundation for your agile practice. But nothing should stop you from sprinkling in a few scrum practices like demo sessions with stakeholders or regular retrospectives.<br>The key to doing agile right is&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" target="_blank">embracing a mindset of continuous improvement</a>. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don't.<br><img alt="Atlassian on agile | Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:358e6b8e-5eec-428f-8d2f-c9aedb962263/illustrations-spot-hero-Status%20Page%20Light-1115x898@2x.png?cdnVersion=2472" referrerpolicy="no-referrer">]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/agile-development.html</link><guid isPermaLink="false">DevOps/For Reading/Agile Development.md</guid><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Agile Vs DevOps]]></title><description><![CDATA[ 
 <br>Agile and DevOps have shaped the way software is developed today. They’ve become so widely adapted and revered as to permeate beyond the world of software development into shaping project management and org charts in businesses of all stripes.<br>DevOps and agile can be tricky to define, and the lines between the two often seem to blur.<br>
<br>At a basic level, DevOps is the combination of two teams (software development and IT operations) to create a more powerful, efficient software development process.
<br>Agile is a series of methodologies around iterative development designed to make tasks smaller and more manageable and increase collaboration.
<br>But what are the deeper differences between DevOps and agile? Is DevOps&nbsp;better&nbsp;than agile? Or can DevOps and agile be implemented together? In this post, we’ll dive into some common agile and DevOps FAQs.<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" target="_blank">What is the difference between DevOps and agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" target="_blank">What is agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" target="_blank">What are the benefits of agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" target="_blank">How can a company be agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" target="_blank">What is DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" target="_blank">Why combine software development and IT operations?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" target="_blank">What are some DevOps concepts and key terms?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" target="_blank">What are the benefits of DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" target="_blank">Is DevOps better than agile? Is DevOps a replacement for agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">What is CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" target="_blank">What are the benefits of CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" target="_blank">What are some common DevOps tools?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" target="_blank">How can I learn more about DevOps?</a>
<br><br>Agile and DevOps are both used in the development of software. While they’re both designed to improve the software development process, they seek to do so by taking different approaches. But they’re not mutually exclusive. (More on that below.)<br>Agile is essentially about giving software developers a shared set of principles to drive decision-making and allow for more responsiveness to change.<br>DevOps is about a culture change intended to drive collaboration between software developers and IT operations.<br>It's often said that DevOps is agile applied beyond the software development team.<br><br><img alt="Post-COVID DevOps" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank"><strong></strong>
</a>Post-COVID DevOps: Accelerating the Future<br>How has COVID affected — or even accelerated — DevOps best practices for engineering teams?&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank">Watch this free, on-demand webinar</a>&nbsp;panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.<br><br><br>Agile is an iterative software development approach that&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" target="_blank">focuses on collaboration and quick, rapid releases</a>. It's a set of values and principles that can be used to help drive decisions in software development.&nbsp;<br>When it comes to agile, it’s tough to define it more concisely than the original micro&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/" target="_blank">manifesto</a>&nbsp;itself, which was written back in 2001. (No TL;DR version needed. It's only 68 words.) It states:<br>We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:<br>
<br>Individuals and interactions&nbsp;over processes and tools
<br>Working software&nbsp;over comprehensive documentation
<br>Customer collaboration&nbsp;over contract negotiation
<br>Responding to change&nbsp;over following a plan
<br>That is, while there is value in the items on the right, we value the items on the left more<br>The manifest is paired with&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/principles.html" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/principles.html" target="_blank">12 agile principles</a>&nbsp;to help make better decisions.&nbsp;<br>Copying agile approaches and investing in agile tools doesn’t make your team agile. It’s not just about doing two-week iterations or building out smaller teams. In agile, the&nbsp;what&nbsp;a team does is less important than the&nbsp;why&nbsp;they do it.<br>Agile (as the name implies) is about the flexibility and adaptability to build software with ever-changing needs and toss set-in-stone plans out the window.<br>This includes better connecting the dev team with end-users. (You could sort of think of that a bit like the&nbsp;<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=m4OvQIGDg4I" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=m4OvQIGDg4I" target="_blank">“What would you say you do here?” scene</a>&nbsp;from&nbsp;Office Space.)<br><br>
<br>Agility&nbsp;-&nbsp; More quickly respond to market changes or customer feedback.
<br>Quality&nbsp;- A focus on testing and sustained customer involvement means the chances of a product’s overall quality being high are greater.
<br>Collaboration&nbsp;- Agile is about people. By placing value on human interactions over processes and “that’s just the way it’s done,” organizations are able to let employees act guided by their experience and a shared set of values rather than being micro-managed or shackled to detailed documentation.
<br>Ownership&nbsp;- The trust required from leadership to have agile teams can create an increased feeling of ownership.
<br>Customer satisfaction&nbsp;- With a focus on finding and fixing problems quickly and a direct line between customers and developers, customers are more likely to be satisfied and come back for more.
<br>Predictability&nbsp;- By doing away with big plans that are set in stone and often detached from reality, organizations can get a better picture of what’s going well and what’s not working right away rather than months down the road when it’s too late to do anything to correct it.
<br>Increase productivity&nbsp;- Regularly used planning tools like Kanban and Scrum help teams keep tabs of work and progress toward goals.
<br><br>Many organizations are used to working in a waterfall model. A waterfall model is a linear approach is a sequence of events that starts with a heavy upfront investment of time and resources in scoping out requirements and budgeting before moving into development, testing, and production.<br>Moving this paradigm to agile and&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/overview/agile-at-scale" rel="noopener nofollow" class="external-link" href="https://acloud.guru/overview/agile-at-scale" target="_blank">running agile at scale</a>&nbsp;can be difficult and complex. This isn’t helped by the fact that “agile” is less of a thing you do and more of a basis for decision-making. For organizations guided by process above all else, this can require a major shift and definitely takes some backing from management.<br><br>With a name that sounds like a covert military team and the kind of goofy capitalization you’ve come to expect from tech terminology, DevOps combines software development and IT operations. Take the "dev" from software development and the "ops" from IT operations and you get this portmanteau, which is the fun-to-say term that describes mashing together of multiple words, like spork, phablet, brunch, jorts, and bromance.<br>DevOps isn't just a process. It’s a shift in workplace culture. It’s a collaboration between teams. Doubling down on automation and installing all of the right software won’t get you there alone. Like agile, people are the key component.<br>Interested in upscaling or beginning your journey with DevOps? A Cloud Guru’s&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/learning-paths/devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/learning-paths/devops" target="_blank">DevOps Learning Paths</a>&nbsp;offers custom courses fit for beginners and advanced gurus!<br>AWS offers the following definition of the DevOps model:<br>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.<br><br><img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/06/cost-optimization-blog-header.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank"><strong></strong></a>Automating AWS Cost Optimization<br>
AWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this&nbsp;<a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">free, on-demand webinar</a>, you'll get an overview of AWS cost-optimization tools and strategies.<br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">Watch Now</a><br><br><br>How did these two groups get combined into one? And why would you want to combine development and IT operations?&nbsp;<br>Way back when (around 2007 or so), IT operations teams were separate from development teams. Then, people in the development and operations communities realized there was some issues in the way these teams were structured and how they interacted (or didn’t interact).<br>
<br>Development and IT operations teams had different objectives
<br>Development and operations teams had different performance indicators
<br>Development and operations teams were siloed physically in different rooms, buildings, or offices
<br>So they started talking. This gave birth to DevOps and the&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" target="_blank">DevOps engineer</a>.&nbsp;<br>Patrick Debois, often called the Godfather of DevOps, brought these groups together at&nbsp;<a data-tooltip-position="top" aria-label="https://devopsdays.org/" rel="noopener nofollow" class="external-link" href="https://devopsdays.org/" target="_blank">devopsdays</a>&nbsp;in 2009 where they discussed ways to bridge the gap between the two fields. Now, thousands of enterprises have adapted or are working toward adapting these practices.<br>These new approaches basically make ops everyone’s job to a degree, which makes&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" target="_blank">the future of ops jobs</a>&nbsp;very interesting.&nbsp;<br><br>
<br>Container&nbsp;- A lightweight, standalone, executable piece of software. It includes everything needed to run that software.
<br>Continuous delivery (CD)&nbsp;- The ongoing and manual or automatic release of software to production. It's aimed around new cycles. Some orgs release a new version with any changes.
<br>Continuous integration (CI)&nbsp;- The ongoing programming, building, and testing of code. This is done before turning it over to the continuous delivery (CD) system for deployment. With CD, it makes up&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">CI/CD</a>.
<br>Infrastructure as Code (IaC)&nbsp;- Defining infrastructure you want to use with programming code that can be understood by cloud services. These cloud services then create the infrastructure for you based on this code. This allows you to define standards and reuse code, which saves time.
<br>Microservices&nbsp;- Application architecture that is broken into multiple small pieces. Containers are often used to deploy microservices.
<br>Open source&nbsp;- Computer software code released under a license for free, like&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/linux-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/linux-this-month" target="_blank">Linux</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/kubernetes-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/kubernetes-this-month" target="_blank">Kubernetes</a>.
<br>Pipeline&nbsp;- A set of connected processes where the output of one process is the input for the next.
<br>Serverless&nbsp;- Running a service or microservice on cloud-based infrastructure without worrying about the servers running your code. You simply provide the code, and the cloud provider runs the code and gives you the results. See more on the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" target="_blank">Function as a Service&nbsp;(FaaS) services of AWS, Azure, and GCP</a>.
<br>Source code repository&nbsp;- A place to upload and track the history of your code, like&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">GitHub</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" target="_blank">AWS CodeCommit</a>.
<br>Unit testing&nbsp;- Breaking your application down into small parts to test that each features works.
<br><br>DevOps is all about producing higher-quality software faster and saving a lot of time and money. Here’s a more detailed breakdown of the benefits.<br>
<br>Speed&nbsp;- Release updates and new features faster, adapt to the changing market, and become more efficient.
<br>Rapid delivery&nbsp;- Increase deployment frequency and the pace of releases. Respond to customers' needs faster and build a competitive advantage.&nbsp;
<br>Reliability&nbsp;- Automatic testing is built-in. Ensures rollouts are of the highest quality and that you have less downtime because you build for stability and test before deploying.
<br>Scale&nbsp;- Implements automation. With the use of cloud and container technology, you can scale usage up and down and save money while you do so.
<br>Collaboration&nbsp;- Allow teams who used to be apart to work together. Workflows can be combined, inefficiency is reduced, and time is saved.
<br>Security&nbsp;- Allows infrastructure to be created by code, just like software. By doing this (instead of doing it manually), you can define policies to stay compliant no matter how many servers will be deployed.
<br>Want an overview of the benefits with a heavy dash of sarcasm? Check out our post&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" target="_blank">5 reasons to NOT move to DevOps</a>.<br><br>DevOps and agile can speed up the delivery of and increase the quality of software.&nbsp;Agile replaced the old-school waterfall model, but DevOps isn’t a replacement for agile.<br>DevOps exists because of agile, and the two can coexist and be used together. You don’t have to choose between DevOps and agile. Ideally, your organization will practice both.&nbsp;<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" target="_blank">top DevOps skills people are learning at ACG right now</a><br><br>Continuous integration and continuous development (or CI/CD) is a DevOps tactic — a way to deliver apps to customers with the introduction of automation into the agile development process.&nbsp;<br>The&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" target="_blank">CI/CD pipeline</a>&nbsp;(as it's called when these practices are combined) has become an integral part of DevOps culture. A variety of tools and techniques are used for implementing such a pipeline. (More on those tools below.)<br>The CI/CD pipeline is supported by teams working in an agile way with either DevOps or a&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank">site reliability engineering (SRE)</a>&nbsp;approach.&nbsp;<br>Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" target="_blank"><em></em></a>why you need SRE skills to thrive in a serverless world&nbsp;and check out our&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank"><em></em></a>Site Reliability Engineering Concepts&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" target="_blank"><em></em></a>Google Cloud DevOps and SREs&nbsp;courses.<br><br>CI/CD pipelines smooth and speed up the flow of code from development through operations and QA into production by automating manual processes and increasing confidence in your releases.&nbsp;<br>This is the exact opposite of the waterfall release approach still practiced by many large organizations, where developers throw code “over the wall” to ops, devs don’t get access to production, and ops doesn’t have much inside knowledge of how the code works.<br>CI/CD allows organizations to:<br>
<br>Build faster
<br>Decrease code review time
<br>Automatic
<br>Faster fault isolation
<br>Additional deployment features
<br>As mentioned before, you can be doing the right things and using the right tools and&nbsp;still&nbsp;not be agile or properly implementing DevOps. A broken and messy team can result in broken and messy CI/CD pipelines. You can almost&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" target="_blank">predict how your CI/CD pipeline looks</a>&nbsp;based on how your dev and ops teams work together.<br><br><img alt="Cloud Dictionary" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/12/Cloud-Dictionary-Resource-Image.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank"><strong></strong></a>Get the Cloud Dictionary of Pain<br>
Speaking cloud doesn’t have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">cloud guide</a>&nbsp;for succinct definitions of some of the most painful cloud terms.<br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">Get the Goods</a><br><br><br>Here are some of the most popular DevOps CI/CD tools you might use if working in an AWS cloud environment.<br>
<br>
Git&nbsp;- Free open-source version control system. It stores the entire history of your code that your developers will continue to push new code to It has a tiny footprint and fast performance. It also supports branching, allowing others to work on features separately without having developers trip over code.

<br>
GitHub&nbsp;-&nbsp;Online service to host Git repositories. GitHub is owned by Microsoft, which offers a similar DevOps tool called Azure DevOps. Whether you are a developer or not, working alone or in a team,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-azure-devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-azure-devops" target="_blank">Azure DevOps training</a>&nbsp;can help you organize the way you plan, create and deliver software. Get the lowdown on&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">Azure DevOps vs GitHub in this comparison of Microsoft DevOps tools</a>.

<br>
AWS CodeCommit&nbsp;-&nbsp;Fully managed server for hosting Git repositories. It’s secure and encrypted, highly available, and easily integrated with other AWS services.

<br>
AWS CodeBuild&nbsp;-&nbsp;Fully managed continuous integration code that complies code, runs tests, and produces software packages. When developers write code and push it into Git, CodeBuild compiles that code and produces a software package. It scales automatically and can process multiple builds concurrently.

<br>
AWS CodeDeploy&nbsp;-&nbsp;Fully managed deployment service that takes the software package (or files you updated and pushed into your Git repository) and deploys it to AWS or on-premises servers. It integrates well with AWS services and your own servers. It completely automates software deployment, eliminating error-prone manual operations.

<br>
AWS CodePipeline&nbsp;-&nbsp;Fully management continuous delivery service that helps you completely automate your release pipeline. It can automate the build, test, and deploy phases of software development and it can integrate with CodeCommit and GitHub, giving you the flexibility to use the source-control system of your choice. Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" target="_blank">automating CI/CD with AWS CodePipeline</a>.

<br>Other common DevOps tools include&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/docker-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/docker-deep-dive" target="_blank">Docker</a>, Jira,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/jenkins-fundamentals" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/jenkins-fundamentals" target="_blank">Jenkins</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/puppet-quick-start" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/puppet-quick-start" target="_blank">Puppet</a>, Chef,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/splunk-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/splunk-deep-dive" target="_blank">Splunk</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-ansible" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-ansible" target="_blank">Ansible</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/kubernetes-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/kubernetes-deep-dive" target="_blank">Kubernetes</a>, Bamboo, and&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/nagios-certified-professional-prep-course" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/nagios-certified-professional-prep-course" target="_blank">Nagios</a>.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/agile-vs-devops.html</link><guid isPermaLink="false">DevOps/For Reading/Agile Vs DevOps.md</guid><pubDate>Sun, 29 Dec 2024 14:52:57 GMT</pubDate><enclosure url="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Git Fetch vs Git pull]]></title><description><![CDATA[ 
 <br><br>Git fetch and git pull are both Git commands used to retrieve update information from a remote repository. So, how do they differ? Git fetch downloads the changes from the remote repository to the local repository but does not make any changes to the current working directory. Since the changes are not merged into the local branch, you can check the changes from the remote repository without interrupting your current work. On the other hand, git pull retrieves the latest changes from the remote repository like git fetch, but it also automatically merges those changes into the current branch. In contrast to git fetch, git pull directly applies the changes from the remote repository to the local working directory.<br><br>The git fetch command retrieves the latest commit history from the remote repository, but it does not affect the local working directory. Even after fetching remote changes, they are not reflected in the local branch. It is primarily used when you want to retrieve the latest status from the remote repository and review the changes before they are reflected in the local repository. To apply the retrieved changes to the local branch, you need to manually run git merge or&nbsp;<a data-tooltip-position="top" aria-label="https://docs.gitlab.com/ee/topics/git/git_rebase.html" rel="noopener nofollow" class="external-link" href="https://docs.gitlab.com/ee/topics/git/git_rebase.html" target="_blank">git rebase</a>.<br><br>The git pull command combines&nbsp;git fetch&nbsp;and&nbsp;git merge&nbsp;(or&nbsp;git rebase) into a single command. This allows you to fetch changes from the remote repository and automatically integrate them into the current local branch.<br>While git fetch retrieves changes from the remote repository without applying them to the local branch, running git pull automatically integrates the changes from the remote repository into the local branch.<br>Git pull is suitable for quickly reflecting remote changes in the local branch, but it can lead to conflicts, so caution is needed, especially when working with multiple people.<br><br>Git fetch is a command used to retrieve the latest information from a remote repository. The retrieved information is not directly reflected in the local branch. Using git pull will reflect all remote branches, including incorrect or problematic ones, in the local branch.<br>When changes are made simultaneously on both remote and local branches, or when there are new users on the team, it is safer to use git fetch to retrieve the remote branch contents first and then perform merge or rebase.<br><br>Git pull is a command that performs more processes compared to git fetch. Git pull can perform both git fetch and additionally execute git merge or git rebase. For this reason, git pull is recommended when you want to quickly reflect changes from the remote repository in the local branch.<br><br><br>Git pull is a command that performs git fetch followed by git merge or git rebase. While git fetch does not affect the local repository, git pull automatically synchronizes changes from the remote repository with the local repository.<br><br>When executing git pull, there may be conflicts between remote and local changes. Merge conflicts are particularly likely to occur, so if conflicts arise, they need to be resolved manually. Additionally, using git pull --rebase allows you to incorporate the latest changes while performing a rebase.<br><br>Git fetch is useful for checking and retrieving the latest status of the remote repository. However, the changes retrieved are not automatically reflected in the local branch; git fetch is used to synchronize the local and remote repositories.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/git-fetch-vs-git-pull.html</link><guid isPermaLink="false">DevOps/For Reading/Git Fetch vs Git pull.md</guid><pubDate>Sun, 29 Dec 2024 14:19:02 GMT</pubDate></item><item><title><![CDATA[Git Theory]]></title><description><![CDATA[ 
 <br><br>A centralized version control system (VCS) uses a single, central repository to store all file versions and their change history. Team members have their own working copies, but all modifications are ultimately committed to this central server. This facilitates collaboration by providing a single source of truth, but it also creates a single point of failure. Examples include <a data-tooltip-position="top" aria-label="https://subversion.apache.org/" rel="noopener nofollow" class="external-link" href="https://subversion.apache.org/" target="_blank">Subversion (SVN)</a> and <a data-tooltip-position="top" aria-label="https://www.nongnu.org/cvs/" rel="noopener nofollow" class="external-link" href="https://www.nongnu.org/cvs/" target="_blank">CVS</a>.<br><img alt="CVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"><br><br>In a Decentralized Version Control System (DVCS), every user has a complete copy of the repository, including its entire history. This eliminates the reliance on a central server, allowing for offline work and greater flexibility. Changes are shared between repositories as needed. Popular examples include <a data-tooltip-position="top" aria-label="https://git-scm.com" rel="noopener nofollow" class="external-link" href="https://git-scm.com" target="_blank">Git</a> and <a data-tooltip-position="top" aria-label="https://www.mercurial-scm.org/" rel="noopener nofollow" class="external-link" href="https://www.mercurial-scm.org/" target="_blank">Mercurial</a>.<br><img alt="DVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"><br><br>Git is like a super-powered tracking system for your files and the changes you make to them over time. Imagine it as a special folder that remembers every version of your work, allowing you to go back to any previous stage if needed. &nbsp;<br>Here's what makes Git special:<br>
<br>Keeps a detailed history: Git meticulously records every change you make to your files, who made it, and when. This history helps you understand how your project evolved. &nbsp;
<br>Branching and merging: Git allows you to create separate branches, like alternate timelines, to experiment with new features or try different ideas without affecting the main project. You can then merge these branches back into the main project when you're ready. &nbsp;
<br>Collaboration made easy: Git is designed for teamwork. Multiple people can work on the same project simultaneously, and Git helps manage and integrate everyone's contributions smoothly. &nbsp;
<br>Offline access: You have the entire project history on your computer, so you can work even without an internet connection. &nbsp;
<br>Popular and widely used: Git is the most popular version control system in the world, used by countless developers and companies. &nbsp;
<br><br>Git was created by Linus Torvalds, the famous creator of the Linux operating system, in 2005. He needed a better tool to manage the Linux kernel development, as the existing version control systems were not efficient enough for such a large and complex project.<br>Here's a glimpse of Git's history:<br>
<br>Early Days (2002-2005): Linux kernel development relied on a proprietary DVCS called <a data-tooltip-position="top" aria-label="https://www.bitkeeper.org/" rel="noopener nofollow" class="external-link" href="https://www.bitkeeper.org/" target="_blank">BitKeeper</a>. When its free-of-charge use was revoked, Torvalds decided to create his own version control system, with the goal of being faster, simpler, and more robust.
<br>Birth of Git (April 2005): Torvalds began work on Git and within a remarkably short period, had a functional system ready to manage the Linux kernel.
<br>Community Takes Over (July 2005): Junio Hamano took over the maintenance of Git, guiding its development and shaping it into the mature system it is today.
<br>Widespread Adoption: Git's speed, flexibility, and powerful features quickly gained popularity among developers. It became the preferred choice for open-source projects and eventually spread to commercial software development.
<br>Today, Git is the most widely used version control system worldwide, powering the development of countless software projects, from small personal projects to massive corporate endeavors.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/git-theory.html</link><guid isPermaLink="false">DevOps/For Reading/Git Theory.md</guid><pubDate>Sun, 29 Dec 2024 13:28:03 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Monolithic vs Microservice Architecture]]></title><description><![CDATA[ 
 <br>Monolithic applications are built as a single, unified unit. Think of it like a large container holding all the components of your application—user interface, business logic, database access—all bundled together. &nbsp;<br>Microservices, on the other hand, break down the application into a collection of small, independent services. Each service focuses on a specific business function and communicates with other services through APIs. Imagine it as a set of interconnected Lego blocks, each serving a distinct purpose. &nbsp;<br>Here's a table highlighting the key differences:<br><br>In a nutshell:<br>
<br>Monolithic apps are simpler to start with but can become complex to manage as they grow. &nbsp;
<br>Microservices offer greater flexibility, scalability, and fault isolation, but they come with increased architectural complexity. &nbsp;
<br>The choice between monolithic and microservices depends on the specific needs and complexity of your application.<br>References:<br>
<br><a rel="noopener nofollow" class="external-link" href="https://rtslabs.com/data-integration-in-a-microservices-architecture" target="_blank">https://rtslabs.com/data-integration-in-a-microservices-architecture</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/monolithic-vs-microservice-architecture.html</link><guid isPermaLink="false">DevOps/For Reading/Monolithic vs Microservice Architecture.md</guid><pubDate>Sun, 29 Dec 2024 14:51:11 GMT</pubDate></item><item><title><![CDATA[read about docker.sock file]]></title><description><![CDATA[ 
 <br><br>Basically this file act as a bridge between docker daemon and docker client.<br>
Its responsible for api communications between docker client and dockerd service that runs in the background.<br>So when you run docker run nginx<br>
docker client send request to this file over at /var/run/docker.sock and then docker daemon which is constantly listening for this socket it receives the request and create container.<br><img alt="docker-sock-file.svg" src="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg"><br><br>The docker.sock file is a Unix socket that allows communication between the Docker client and the Docker daemon (the background process that manages containers). It acts like a bridge, enabling you to manage containers, images, networks, and more.<br>Here's a breakdown:<br>
<br>What is a socket? A socket is a way for programs to communicate with each other, either on the same machine or over a network. Think of it like a phone line specifically for applications.
<br>Why a socket instead of a network connection? Using a Unix socket for communication between the client and daemon on the same host is more efficient and secure than using network-based protocols like HTTP or TCP.
<br>How it works: When you run a Docker command (e.g., docker run), the client sends the command to the docker.sock file. The Docker daemon, listening on the other end of the socket, receives the command and executes it.
<br>Location: The docker.sock file is typically located at /var/run/docker.sock on Linux systems.
<br>Security: The docker.sock file is usually owned by the root user and the docker group, which restricts access and enhances security.
<br>Key points to remember:<br>
<br>It's the primary way to interact with the Docker daemon.
<br>It enables efficient and secure communication.
<br>It's a critical component for managing Docker containers and images.
<br>Misusing or exposing the docker.sock file can lead to security risks.
<br>If you're interested in diving deeper, I recommend checking out these resources:<br>
<br>Understanding /var/run/docker.sock: <a rel="noopener nofollow" class="external-link" href="https://dev.to/piyushbagani15/understanding-varrundockersock-the-key-to-dockers-inner-workings-nm7" target="_blank">https://dev.to/piyushbagani15/understanding-varrundockersock-the-key-to-dockers-inner-workings-nm7</a>
<br>What is Docker socket binding? <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-docker-socket-binding/" target="_blank">https://www.geeksforgeeks.org/what-is-docker-socket-binding/</a>
<br>Docker Tips: about /var/run/docker.sock: <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/docker-tips-about-varrundockersock/" target="_blank">https://www.geeksforgeeks.org/docker-tips-about-varrundockersock/</a>
<br><br>When you install docker in a machine. Two diffrent programs come in:<br>
<br>Docker Client
<br>Docker Server
<br>Docker Server receives commands over a socket (either over a network or through a "file")<br>Docker Client communicates over a network and sends message to the Docker server to say make a container, start a container, stop a container etc.<br>When the client and server are running on the same computer, they can connect through a special file called a socket. And since they can communicate through a file and Docker can efficiently share files between hosts and containers, it means you can run the client inside Docker itself.<br>Here is a sample:<br>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker sh
<br>This command creates a container that docker client installed within. And check the volume part:&nbsp;-v /var/run/docker.sock:/var/run/docker.sock<br>With&nbsp;-v&nbsp;flag it shares host&nbsp;docker.sock&nbsp;file so you can manipulate the containers within the host via a container.<br>docker run --rm -it ubuntu bash # Creates a new container via container 
<br>Run&nbsp;docker ps&nbsp;on host terminal.<br>CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES
0f9e333b59fe   ubuntu    "bash"                   5 seconds ago    Up 4 seconds              zealous_wilson
b4a8af31416b   docker    "docker-entrypoint.s…"   16 minutes ago   Up 16 minutes             epic_elion
<br><br>Docker.sock file<br>The Docker.sock file, also known as the Docker socket, is a Unix domain socket (UDS) file used by Docker to communicate between the Docker daemon (dockerd) and the Docker client. It allows the client to send commands to the daemon and receive responses.<br><br>By default, the Docker.sock file is located at&nbsp;/var/run/docker.sock&nbsp;on Linux systems and&nbsp;C:\ProgramData\docker\socket&nbsp;on Windows systems.<br><br>The Docker.sock file enables the following:<br>
<br>Command execution: The Docker client sends commands (e.g.,&nbsp;docker run,&nbsp;docker ps) to the daemon through the socket, which executes the requested action.
<br>Response retrieval: The daemon sends responses (e.g., container output, error messages) back to the client through the socket.
<br><br>
<br>Access control: The Docker.sock file is owned by the&nbsp;docker&nbsp;group and has restrictive permissions by default, ensuring that only authorized users can access it.
<br>Encryption: The communication between the client and daemon is not encrypted by default. To secure the socket, you can enable TLS encryption using the&nbsp;--tls&nbsp;and&nbsp;--tlscert&nbsp;flags when starting the daemon.
<br><br>You can mount the Docker.sock file inside a container using the&nbsp;-v&nbsp;flag when running the container. For example:<br>docker run -v /var/run/docker.sock:/var/run/docker.sock my-image
<br>This allows the container to access the Docker socket and interact with the Docker daemon on the host system.<br><br>If you need to access the Docker daemon remotely or securely, consider using a TCP socket (e.g.,&nbsp;dockerd -H tcp://0.0.0.0:2375) or a proxy like Docker Compose or Kubernetes.<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://lobster1234.github.io/2019/04/05/docker-socket-file-for-ipc/" target="_blank">https://lobster1234.github.io/2019/04/05/docker-socket-file-for-ipc/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://stackoverflow.com/questions/35110146/what-is-the-purpose-of-the-file-docker-sock" target="_blank">https://stackoverflow.com/questions/35110146/what-is-the-purpose-of-the-file-docker-sock</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.educative.io/answers/var-run-dockersock" target="_blank">https://www.educative.io/answers/var-run-dockersock</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html</link><guid isPermaLink="false">DevOps/For Reading/read about docker.sock file.md</guid><pubDate>Mon, 30 Dec 2024 15:51:49 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Reading Docker Inspect output]]></title><description><![CDATA[ 
 <br>Output of Mysql container from <a data-href="Task 3 Create Mysql Container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 3 Create Mysql Container</a> <br><br>docker inspect &lt;container-id&gt;<br>[
  {
    "Id": "3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3",
    "Created": "2024-12-29T12:46:23.52003502Z",
    "Path": "docker-entrypoint.sh",
    "Args": [
      "mysqld"
    ],
    "State": {
      "Status": "running",
      "Running": true,
      "Paused": false,
      "Restarting": false,
      "OOMKilled": false,
      "Dead": false,
      "Pid": 13864,
      "ExitCode": 0,
      "Error": "",
      "StartedAt": "2024-12-29T12:46:23.928523535Z",
      "FinishedAt": "0001-01-01T00:00:00Z"
    },
    "Image": "sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb",
    "ResolvConfPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/resolv.conf",
    "HostnamePath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hostname",
    "HostsPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hosts",
    "LogPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3-json.log",
    "Name": "/mysql_container",
    "RestartCount": 0,
    "Driver": "overlay2",
    "Platform": "linux",
    "MountLabel": "",
    "ProcessLabel": "",
    "AppArmorProfile": "",
    "ExecIDs": null,
    "HostConfig": {
      "Binds": [
        "mysql_data:/var/lib/mysql"
      ],
      "ContainerIDFile": "",
      "LogConfig": {
        "Type": "json-file",
        "Config": {}
      },
      "NetworkMode": "default",
      "PortBindings": {
        "3306/tcp": [
          {
            "HostIp": "",
            "HostPort": "3306"
          }
        ]
      },
      "RestartPolicy": {
        "Name": "no",
        "MaximumRetryCount": 0
      },
      "AutoRemove": false,
      "VolumeDriver": "",
      "VolumesFrom": null,
      "ConsoleSize": [
        30,
        120
      ],
      "CapAdd": null,
      "CapDrop": null,
      "CgroupnsMode": "private",
      "Dns": [],
      "DnsOptions": [],
      "DnsSearch": [],
      "ExtraHosts": null,
      "GroupAdd": null,
      "IpcMode": "private",
      "Cgroup": "",
      "Links": null,
      "OomScoreAdj": 0,
      "PidMode": "",
      "Privileged": false,
      "PublishAllPorts": false,
      "ReadonlyRootfs": false,
      "SecurityOpt": null,
      "UTSMode": "",
      "UsernsMode": "",
      "ShmSize": 67108864,
      "Runtime": "runc",
      "Isolation": "",
      "CpuShares": 0,
      "Memory": 0,
      "NanoCpus": 0,
      "CgroupParent": "",
      "BlkioWeight": 0,
      "BlkioWeightDevice": [],
      "BlkioDeviceReadBps": [],
      "BlkioDeviceWriteBps": [],
      "BlkioDeviceReadIOps": [],
      "BlkioDeviceWriteIOps": [],
      "CpuPeriod": 0,
      "CpuQuota": 0,
      "CpuRealtimePeriod": 0,
      "CpuRealtimeRuntime": 0,
      "CpusetCpus": "",
      "CpusetMems": "",
      "Devices": [],
      "DeviceCgroupRules": null,
      "DeviceRequests": null,
      "MemoryReservation": 0,
      "MemorySwap": 0,
      "MemorySwappiness": null,
      "OomKillDisable": null,
      "PidsLimit": null,
      "Ulimits": [
        {
          "Name": "nofile",
          "Hard": 65536,
          "Soft": 32768
        }
      ],
      "CpuCount": 0,
      "CpuPercent": 0,
      "IOMaximumIOps": 0,
      "IOMaximumBandwidth": 0,
      "MaskedPaths": [
        "/proc/asound",
        "/proc/acpi",
        "/proc/kcore",
        "/proc/keys",
        "/proc/latency_stats",
        "/proc/timer_list",
        "/proc/timer_stats",
        "/proc/sched_debug",
        "/proc/scsi",
        "/sys/firmware",
        "/sys/devices/virtual/powercap"
      ],
      "ReadonlyPaths": [
        "/proc/bus",
        "/proc/fs",
        "/proc/irq",
        "/proc/sys",
        "/proc/sysrq-trigger"
      ]
    },
    "GraphDriver": {
      "Data": {
        "LowerDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f-init/diff:/var/lib/docker/overlay2/96c958faea5ea8ce734cd4db5008bb2188c285dce8190a8c7d35528a9e2ad055/diff:/var/lib/docker/overlay2/643ab6a924a7f65786e4033a2c82e8815cbca24318f429d9f62600d0e7e955a1/diff:/var/lib/docker/overlay2/a9e3b51cd314aa5cad0258b1330d9ca358ff9e4f063ed45f34948ff834b39c0a/diff:/var/lib/docker/overlay2/37efbaa9117f660d35ed59ca9683c1c22a545a8be5621032781bd59467451a29/diff:/var/lib/docker/overlay2/ea28ca7d3354a82731cf3e4947821e814414353c8b431189aec28a2e758946cd/diff:/var/lib/docker/overlay2/729ec68eb4d4f5273a13a068bb24d70d34dcb160fceb795f3058054552b28174/diff:/var/lib/docker/overlay2/6876ecc19a1ae8a61be41a8f298e9e0a012c0ab952944a4923ac46559c32ed3c/diff:/var/lib/docker/overlay2/8b4ab94e45bbe1ab2068b4488f60434de5788dcd4626f2eda8ccd367e2c513ac/diff:/var/lib/docker/overlay2/06533c7033b5c8d5069139b433e42842ebadf121567da334c956001cd942a139/diff:/var/lib/docker/overlay2/d01a0a333a0ac1376c37feec0c1cb771b28631b213fb399ed12ff685085c4b71/diff",
        "MergedDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/merged",
        "UpperDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/diff",
        "WorkDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/work"
      },
      "Name": "overlay2"
    },
    "Mounts": [
      {
        "Type": "volume",
        "Name": "mysql_data",
        "Source": "/var/lib/docker/volumes/mysql_data/_data",
        "Destination": "/var/lib/mysql",
        "Driver": "local",
        "Mode": "z",
        "RW": true,
        "Propagation": ""
      }
    ],
    "Config": {
      "Hostname": "3ce6ffd3357d",
      "Domainname": "",
      "User": "",
      "AttachStdin": false,
      "AttachStdout": true,
      "AttachStderr": true,
      "ExposedPorts": {
        "3306/tcp": {},
        "33060/tcp": {}
      },
      "Tty": false,
      "OpenStdin": false,
      "StdinOnce": false,
      "Env": [
        "MYSQL_USER=name",
        "MYSQL_PASSWORD=passwd",
        "MYSQL_RANDOM_ROOT_PASSWORD=yes",
        "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
        "GOSU_VERSION=1.17",
        "MYSQL_MAJOR=innovation",
        "MYSQL_VERSION=9.1.0-1.el9",
        "MYSQL_SHELL_VERSION=9.1.0-1.el9"
      ],
      "Cmd": [
        "mysqld"
      ],
      "Image": "mysql:latest",
      "Volumes": {
        "/var/lib/mysql": {}
      },
      "WorkingDir": "/",
      "Entrypoint": [
        "docker-entrypoint.sh"
      ],
      "OnBuild": null,
      "Labels": {}
    },
    "NetworkSettings": {
      "Bridge": "",
      "SandboxID": "86cf8ccab46e8354b1949fbc9c3c96452864b775418ee65163a356bb7de1eaac",
      "SandboxKey": "/var/run/docker/netns/86cf8ccab46e",
      "Ports": {
        "3306/tcp": [
          {
            "HostIp": "0.0.0.0",
            "HostPort": "3306"
          },
          {
            "HostIp": "::",
            "HostPort": "3306"
          }
        ],
        "33060/tcp": null
      },
      "HairpinMode": false,
      "LinkLocalIPv6Address": "",
      "LinkLocalIPv6PrefixLen": 0,
      "SecondaryIPAddresses": null,
      "SecondaryIPv6Addresses": null,
      "EndpointID": "b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2",
      "Gateway": "172.17.0.1",
      "GlobalIPv6Address": "",
      "GlobalIPv6PrefixLen": 0,
      "IPAddress": "172.17.0.2",
      "IPPrefixLen": 16,
      "IPv6Gateway": "",
      "MacAddress": "02:42:ac:11:00:02",
      "Networks": {
        "bridge": {
          "IPAMConfig": null,
          "Links": null,
          "Aliases": null,
          "MacAddress": "02:42:ac:11:00:02",
          "NetworkID": "1306a6d500d7b8f9a7da615852fab3d6ee12750f0d8550e6292d8ed5b5850930",
          "EndpointID": "b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2",
          "Gateway": "172.17.0.1",
          "IPAddress": "172.17.0.2",
          "IPPrefixLen": 16,
          "IPv6Gateway": "",
          "GlobalIPv6Address": "",
          "GlobalIPv6PrefixLen": 0,
          "DriverOpts": null,
          "DNSNames": null
        }
      }
    }
  }
]
<br><br>The docker inspect command provides a detailed view of a Docker object's configuration. This could be a container, image, network, volume, etc. You've provided the output for a container. Let's break down the key components:<br>1. Id: 3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3<br>
<br>This is the unique identifier for your container.
<br>2. Created: 2024-12-29T12:46:23.52003502Z<br>
<br>Timestamp indicating when the container was created (in UTC).
<br>3. Path: docker-entrypoint.sh<br>
<br>This is the default command that will be executed within the container when it starts. Entrypoint scripts are useful for setting up the container environment, running services, or executing other tasks.
<br>4. Args: ["mysqld"]<br>
<br>These are the arguments passed to the Path (entrypoint). In this case, "mysqld" is likely the command to start the MySQL server.
<br>5. State:<br>
<br>Status: running - Current state of the container.
<br>Running: true - Indicates if the container is currently running.
<br>Paused: false - Indicates if the container is paused.
<br>Restarting: false - Indicates if the container is in the process of restarting.
<br>OOMKilled: false - Indicates if the container was killed due to running out of memory (Out Of Memory).
<br>Dead: false - Indicates if the container has exited.
<br>Pid: 13864 - Process ID of the main process running inside the container.
<br>ExitCode: 0 - Exit code of the container (0 usually means success).
<br>Error: "" - Any error message associated with the container's state.
<br>StartedAt: 2024-12-29T12:46:23.928523535Z - Timestamp when the container was started.
<br>FinishedAt: 0001-01-01T00:00:00Z - Timestamp when the container finished (if applicable).
<br>6. Image: sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb<br>
<br>The ID (SHA256 hash) of the image that was used to create this container.
<br>7. ResolvConfPath, HostnamePath, HostsPath, LogPath:<br>
<br>Paths to files within the container's filesystem that handle DNS resolution, hostname, host mappings, and container logs.
<br>8. Name: /mysql_container<br>
<br>The name you assigned to the container when you created it.
<br>9. RestartCount: 0<br>
<br>Number of times the container has been restarted.
<br>10. Driver: overlay2<br>
<br>The storage driver used for the container's filesystem layers.
<br>11. Platform: linux<br>
<br>The platform (OS) for which the container was built.
<br>12. MountLabel, ProcessLabel, AppArmorProfile: - Security-related labels and profiles.<br>13. ExecIDs: null - List of IDs of exec instances that have been run in this container (if any).<br>14. HostConfig: This section contains container settings that are specific to the host machine where the container is running:<br>- **Binds:** `["mysql_data:/var/lib/mysql"]` -  Shows volumes mounted inside the container. Here, a volume named "mysql_data" is mounted to `/var/lib/mysql` inside the container. This is often used for persistent storage.
- **LogConfig:** Configuration for container logging.
- **NetworkMode:** `default` - Networking mode for the container.
- **PortBindings:** `{"3306/tcp": [{"HostIp": "", "HostPort": "3306"}]}` -  Port mappings between the host and the container. This exposes the container's port 3306 on the host's port 3306.
- **RestartPolicy:** `{"Name": "no", "MaximumRetryCount": 0}` - Container restart policy (in this case, it won't restart automatically).
- **AutoRemove:** `false` - Whether the container should be automatically removed when it exits.
- **[Many other settings]:**  Resource limits (memory, CPU), security options, and other low-level configurations.
<br>15. GraphDriver: - Data: Information about the graph driver (storage driver) used by the container.<br>16. Mounts: - Detailed information about the mounts configured for the container, including the source, destination, and options.<br>17. Config: This section describes the container's configuration, which is defined by the image and any modifications you made:<br>- **Hostname:** `3ce6ffd3357d` - Hostname of the container.
- **Domainname:**  Domain name of the container.
- **User:** User that the container runs as.
- **AttachStdin, AttachStdout, AttachStderr:**  Whether to attach to standard input/output/error streams.
- **ExposedPorts:**  Ports that the container exposes (may or may not be published to the host).
- **Tty:**  Whether the container has a pseudo-TTY allocated.
- **OpenStdin:** Whether to keep STDIN open even if not attached.
- **Env:** `["MYSQL_USER=name", "MYSQL_PASSWORD=passwd", ...]` - Environment variables set within the container.
- **Cmd:** `["mysqld"]` -  The command to be executed when the container starts.
- **Image:** `mysql:latest` - The image the container was created from.
- **Volumes:** Volumes defined in the container configuration.
- **WorkingDir:** `/` - The working directory inside the container.
- **Entrypoint:** `["docker-entrypoint.sh"]` - The entrypoint script for the container.
- **OnBuild:**  Triggers that execute in the context of the image build.
- **Labels:**  Metadata labels associated with the container.
<br>18. NetworkSettings: This section provides details about the container's network configuration:<br>- **Bridge:**  Name of the bridge network the container is attached to.
- **SandboxID, SandboxKey:** Information related to the container's network namespace.
- **Ports:**  Details about the ports that are published to the host.
- **[Other network settings]:** IP address, MAC address, gateway, and other network-related information.
<br>If you have a specific question about any of these elements, feel free to ask! I'm here to help you understand your Docker container's configuration.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/reading-docker-inspect-output.html</link><guid isPermaLink="false">DevOps/For Reading/Reading Docker Inspect output.md</guid><pubDate>Mon, 30 Dec 2024 15:27:24 GMT</pubDate></item><item><title><![CDATA[Docker Basics]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br>
Before Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture<br><br>At initial stages of the software development developers usually build software in monolithic way meaning everything that software has to offer is bundle in the single unit but as the development goes on the need to federate the services of the software arises and developers adapt Microservices method to divide software into chunks so all chunks communicate with each other creating illusion of one single software.<br><br>
<br>Single unified application
<br><br>
<br>one app divided in chunks
<br><br>*Docker is a containerization technology that packages your application in special portable sandboxed format along with its dependencies and system libraries for better compatibility across devices.<br><img alt="Docker-Architecture-diagram_1.excalidraw.svg" src="https://notes.sarangwandile.xyz/lib/media/docker-architecture-diagram_1.excalidraw.svg"><br><br>
<br>its like small virtual machines
<br>containers are free to use all the system resources they needs
<br>whereas in vms softwares has limited preallocated resources to use
<br><br>#Creating a nginx container
docker run nginx

# run as detached mode
docker run -d nginx

# list running containers
docker ps 

# list all containers
docker ps -a

# kill running container
docker kill &lt;container-id&gt;

# expose container on random host port
docker run -P nginx

# expose container on custom host port
docker run -d -p 1313:8080 nginx:latest

# Stopping the container
docker stop &lt;container id&gt;

# removing stopped containers
docker rm &lt;container id&gt;

<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/" target="_blank">https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://adventofdocker.com" target="_blank">https://adventofdocker.com</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/docker-basics.html</link><guid isPermaLink="false">DevOps/Notes/Docker Basics.md</guid><pubDate>Wed, 25 Dec 2024 17:29:52 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Docker little bit Advance]]></title><description><![CDATA[ 
 <br><br><br>Installing docker on amazonlinux and rhel based distros are much easier than installling on ubuntu<br>sudo yum install docker # rhel/amazonlinux/fedora etc
sudo pacman -S docker   # archlinux based distros
<br><br>curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; sudo sh get-docker.sh`
<br>Running this script should install following package inside your ubuntu based distro docker-engine, docker.io, containerd, runc.<br>However I still recommend to check <a data-tooltip-position="top" aria-label="https://docs.docker.com/engine/install/ubuntu/" rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">this official installation guide</a><br><br>systemctl start docker
systemctl enable docker
<br>To run docker command its absolutely require to have docker daemon running in background otherwise it wont work so make sure you have docker daemon running.<br><br>docker pull nginx
<br>This will pull latest version of nginx image from docker hub.<br>
By default its always going to pull latest tag<br>
In order to pull specific version you need to mention its tag name like this<br>docker pull nginx:1.23.1
<br><br>docker run nginx
<br>This command creates and starts a Docker container running the Nginx web serve. Although By default, Nginx runs on port 80 within the container. This command doesn't map any ports, so you won't be able to access the Nginx server from your host machine to specify port you can use-p flag.<br><br>docker run -p 8080:80 nginx
<br>Here 8080 is host machine port and 80 is container port<br>docker run -P nginx
<br>This will map random port for the host machine.<br><br>docker run -d nginx`
<br>Normally running container will cause container to run in foreground and occupy the terminal screen. So its better choice to use -d to make it run in background.<br><br>docker run --name my-container nginx
<br>By default docker give funny names to containers. but you can specify your custom name and can use that name instead of 'container-id' in docker commands.<br><br>docker stop &lt;conainer-id&gt;
<br>Stopped containers and be restarted with start command<br><br>docker rm &lt;container-id&gt;
<br>Sometimes stopped or exited containers wont have any use and needs to be removed to save space.<br><br>docker ps
<br>This will list all the running containers only and not exited or stopped ones.<br>To list all containers including exited and stopped ones use ps -a<br>docker ps -a
<br><br>docker logs &lt;container-id&gt;
<br>This is important to monitor activities inside of containers.<br><br>docker inspect &lt;container-id&gt;
<br>This will list all the information of container<br><br>docker commit &lt;container-id&gt; 
<br>This will come in handy when you done changes inside running container and want to incorporate these changes into your image.<br>$ sudo docker images
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
&lt;none&gt;       &lt;none&gt;    5c9d3a455e0f   10 seconds ago   214MB
<br><br>docker tag &lt;image-id&gt; new-name
<br>Adding tag is like giving Image a name this comes in handy for identifying images.<br>$ sudo docker images
REPOSITORY     TAG       IMAGE ID       CREATED          SIZE
custom-nginx   latest    5c9d3a455e0f   47 seconds ago   214MB
<br><br>docker push &lt;image-id&gt;
<br>If you have verified docker hub login through docker login command it will automatically push the image to your docker hub repo assuming your docker hub repo has same name as your image tag<br>and same goes to ECR except you need to authenticate through slightly different method:<br>aws ecr get-login-password --region &lt;region-name&gt; | docker login --username AWS --password-stdin &lt;amazon-account-id&gt;.dkr.ecr.&lt;region-name&gt;.amazonaws.com
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/docker-little-bit-advance.html</link><guid isPermaLink="false">DevOps/Notes/Docker little bit Advance.md</guid><pubDate>Mon, 30 Dec 2024 03:48:40 GMT</pubDate></item><item><title><![CDATA[Git Notes]]></title><description><![CDATA[ 
 <br><br>Git is a Distributed Version Control System that mainly used for Source code management by developers.<br>Git basically divides your code into three part.<br>
<br>Working Area
<br>Staging Area
<br>Head or commit
<br>Working Area is your state of code before running the git add command.<br>
The files in here also known as "untracked files".<br>To add the files into git you do git add and they moved to staging area.<br>Here your code got the stage. <br>Before finalizing your code into production i.e. 'commit' you make required changes and after making sure<br>
your code is ready to go into production you run git commit command.<br>
which is basically telling git this is a final changes with the special stamp of "commit hash".<br>Then If you want you can push your code into remote repository sitting onto the cloud waiting to be forked and contributed by peoples across the glob.<br><br>
<br>git add     To add code into git
<br>git commit  To commit changes 
<br>git push    To upload your code into remote repo
<br>git pull    To download latest changes from remote to your local
<br>git fetch   Fetching Metadata and logs from remote 
<br><br>Git Provide "Branches" which are nothing but a way to seperate your code into different version so you can later integrate your features and bugfixes into production once they finished peacefully in their own isolated places.<br>There are well known three branches people generally creates:<br>
<br>Main  (which is your production branch)
<br>Dev   (which is your ongoing development which never cease to stop.)
<br>Test  (Here your code is being tested against performance and security standards.)
<br>Some people also like to create their own custom branches like alpha, beta, stable etc i.e. according to release state.<br><br>
<br>git branch -a   List all available branches
<br>git checkout -b Create and switch to the new branch
<br>git checkout    To switch to different branch
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/git-notes.html</link><guid isPermaLink="false">DevOps/Notes/Git Notes.md</guid><pubDate>Mon, 30 Dec 2024 15:58:08 GMT</pubDate></item><item><title><![CDATA[Git Practical Command]]></title><description><![CDATA[ 
 <br><br><br>
<br>Signup to <a data-tooltip-position="top" aria-label="https://github.com" rel="noopener nofollow" class="external-link" href="https://github.com" target="_blank">github.com</a>
<br>Install git with <a data-tooltip-position="top" aria-label="http://git-scm.com" rel="noopener nofollow" class="external-link" href="http://git-scm.com" target="_blank">gitbash</a> in windows 
<br>or via package manager in linux

<br>for ubuntu apt install git
<br>for rhel based distros yum install git
<br>for arch based distros pacman -S git


<br><br>
<br>Create empty folder in local machine
<br>navigate to it and run git init
<br>add some files and run git add .
<br>commit changes `git commit -m "git initialized and new files added"
<br>Create repository in github and use its link to below step
<br>Add remote repo git remote add origin git@github.com:&lt;username&gt;/&lt;repo-name&gt;.git
<br>update the remote repo with push git push origin main
<br><br><br><br>git add . 
<br>This command will staged all files and modifications done in current and subdirectories but wont wont stage deletions.<br>git add -A 
<br>This command will stage all types of changes across your entire working directory including newfiles, modifications and deletions<br><br>git commit -m "new commit"
<br>Creates a commit with the specified message. This is the most common way to commit changes.<br>git commit -A "your commit message"
<br>Automatically stages all tracked files that have been modified and deleted, then creates a commit. It combines git add -A and git commit -m into a single command.<br>Does not stage any new files. If you have new files you want to include in the commit, you still need to stage them separately using git add.<br><br>git push origin main
<br>Uploads your local changes to the remote repository.<br>
After pushing, other developers can access your changes by pulling from the remote repository.<br><br>git pull origin main
<br>Downloads changes from the remote repository to your local repository. Used to stay up-to-date with the latest changes made by other develpers.*]]></description><link>https://notes.sarangwandile.xyz/devops/notes/git-practical-command.html</link><guid isPermaLink="false">DevOps/Notes/Git Practical Command.md</guid><pubDate>Sun, 29 Dec 2024 13:20:50 GMT</pubDate></item><item><title><![CDATA[Task 1 - Creating Tomcat student-ui container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Get the docker image of Amazon linux 
<br>Add tomcat package  
<br>Add student ui 
<br>Then commit the image 
<br>Store on ecr as well as on docker hub
<br><br>Step 1: Getting the the Amazon Linux Image<br>
Upon searching the for amazonlinux image in <a data-tooltip-position="top" aria-label="https://hub.docker.com" rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">dockerhub</a> I found its official image<br>
I pulled it into the system with docker pull amazonlinux<br>sudo docker run -it -d -p 32768:8080 --name tomcat-student-ui amazonlinux`
<br>and I run the image interactively and in detached mode so i can execute shell command later on<br><img alt="{1CC503E6-0A58-42E5-93CF-64946C7AA1EF}.png" src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"><br>Lets dive into the container <br>sudo docker exec -it &lt;containerid&gt; &lt;shell-command&gt;`
<br><img alt="{0119C4A3-2DE3-4CB0-9692-E63934BF4F5C}.png" src="https://notes.sarangwandile.xyz/lib/media/{0119c4a3-2de3-4cb0-9692-e63934bf4f5c}.png"><br><br>Our App student-ui required specific version of tomcat thats why we gonna install it from source<br>Make sure to install dependancies and unzip package before hand<br><br>Install this specific version from source<br>cd /opt # we can use this directory for temporary space
curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip
unzip apache-tomcat-9.0.98.zip # unzip the archive
yum install java-17 -y # tomcat 9 requires java-17 to work properly
cd apache-tomcat-9.0.98/bin/
bash ./catalina.sh start 
<br><img alt="{9BD47135-6F08-466A-A3C7-A11FC17A759D}.png" src="https://notes.sarangwandile.xyz/lib/media/{9bd47135-6f08-466a-a3c7-a11fc17a759d}.png"><br><img alt="{40C024BF-0490-4CFC-9507-9363489D25BD}.png" src="https://notes.sarangwandile.xyz/lib/media/{40c024bf-0490-4cfc-9507-9363489d25bd}.png"><br>It looks like our tomcat server is up and running on desired port<br><br>but first we need these package building tools git and maven for building our student-ui app<br><br>sudo yum install git maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br><br>cd student-ui
mvn clean package
<br><img alt="{8E723B0C-8E54-4F56-94BC-F7D231F3287A}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e723b0c-8e54-4f56-94bc-f7d231f3287a}.png"><br>
this creates .war file in ./target folder<br>
rename it to suitable short name and move to /opt/apache-tomcat-9.0.98/webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war /opt/apache-tomcat-9.0.97/webapps/
<br>and the app should be accessible on tomcat server on http://instance-ip:32768/studentapp<br><img alt="{0DFF6CA3-CC34-4310-933F-F66D3256551D}.png" src="https://notes.sarangwandile.xyz/lib/media/{0dff6ca3-cc34-4310-933f-f66d3256551d}.png"><br><br>Before creating the image its better to remove the unnecessary packages that we no longer need to make the size of the image minimal as possible.<br><br>yum remove maven git unzip -y
<br>Lets exit from container shell and build the image <br>sudo docker commit &lt;container-id&gt;
<br><img alt="{A2D16CE6-81BA-4DAF-A151-58EA34744A1B}.png" src="https://notes.sarangwandile.xyz/lib/media/{a2d16ce6-81ba-4daf-a151-58ea34744a1b}.png"><br>You see the created image doesnt have any name so lets give it a tag<br>sudo docker tag &lt;image-id&gt; &lt;newtag&gt;
<br><img alt="{A6DFB555-CE8C-404D-AE0E-0E5BE4AECD43}.png" src="https://notes.sarangwandile.xyz/lib/media/{a6dfb555-ce8c-404d-ae0e-0e5be4aecd43}.png"><br>
now push it to docker hub and ECR<br><br><br>
<br>Login to docker hub
<br>Click on repositories
<br>Create new repository
<br>Give it proper name and click create
<br>Here is my repo looks llike<br><img alt="{8E536B96-794C-49C6-B49F-257C6FEE2E9E}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e536b96-794c-49c6-b49f-257c6fee2e9e}.png"><br>Lets push our image into this repo<br>
First rename add new tag to image appropriate according to docker hub repo name<br>sudo docker tag &lt;old-tag-name&gt; &lt;newtag-name&gt;
<br><img alt="{62B7DAF9-7C95-4E53-BEB1-28F215445939}.png" src="https://notes.sarangwandile.xyz/lib/media/{62b7daf9-7c95-4e53-beb1-28f215445939}.png"><br>and now push it to docker hub<br>sudo docker push archsarangx/tomcat-student-ui:latest
<br><img alt="{3386541C-1516-4A09-9493-93A52B3FF2E0}.png" src="https://notes.sarangwandile.xyz/lib/media/{3386541c-1516-4a09-9493-93a52b3ff2e0}.png"><br>and its successfully uploaded on docker hub at<br>
<img alt="{034DA07E-A936-4284-9683-BF0300BC5EB4}.png" src="https://notes.sarangwandile.xyz/lib/media/{034da07e-a936-4284-9683-bf0300bc5eb4}.png"><br>
and anyone can pull it with<br>docker pull archsarangx/tomcat-student-ui:latest
<br><br><br>
<br>goto amazon ECR service and create repositoy
<br><img alt="Pasted image 20241228180114.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228180114.png"><br>then click on blue repo name and click on view push commands<br><br>aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 970547378605.dkr.ecr.us-west-2.amazonaws.com
<br><br>``<br>docker tag archsarangx/tomcat-student-ui:latest 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br>docker push 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br><img alt="{8230C2A4-4835-4E3A-9500-E4C8B9187FB3}.png" src="https://notes.sarangwandile.xyz/lib/media/{8230c2a4-4835-4e3a-9500-e4c8b9187fb3}.png"><br><img alt="{E9698F18-9E9B-4973-9E1A-3E629727DA11}.png" src="https://notes.sarangwandile.xyz/lib/media/{e9698f18-9e9b-4973-9e1a-3e629727da11}.png"><br>
🎉 And our Image is successfully uploaded on both ECR and docker hub.<br><br>Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-1-creating-tomcat-student-ui-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 1 Creating Tomcat student-ui container.md</guid><pubDate>Sun, 29 Dec 2024 13:47:56 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task 2: Create the Free-css template container]]></title><description><![CDATA[ 
 <br><br><br>
<br>get the docker image of Amazon linux 
<br>install nginx 
<br>add free-css template
<br><br><br>
<br>
Goto docker hub --&gt; <a rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">https://hub.docker.com</a>

<br>
Click on Search and search for ---&gt; amazonlinux

<br>
Here I found this official image<br>
<img alt="{BACB7514-FD34-4BC8-9CB6-0314BF45113F}.png" src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png">

<br>
I simply pulled the image into my ec2 instance<br>
docker pull amazonlinux
here is successful pull looks like
<img alt="{BD7598CF-AF45-41FB-82E8-A7E26163D176}.png" src="https://notes.sarangwandile.xyz/lib/media/{bd7598cf-af45-41fb-82e8-a7e26163d176}.png">

<br>
We can further confirm if the image is really present in the system with docker images
<img alt="Pasted image 20241227205640.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227205640.png"><br>
Looks like amazonlinux is successfully pulled in our system<br>
lets start the container using this image

<br>
Running the container from image<br>
sudo docker run -d -p 32768:80 --name free-css-template amazonlinux
We used -d to run the container in detached mode meaning its output wont occupy the terminal screen and quietly run in the background after running this command it will simply give us container-id and gives us prompt.<br>
--name flag will assign the name for our container instead of their random funny names and with -p we are forwarding the network traffic of port 80 of the container to 32768 port of our host machine in this case ec2-instance.<br>
<img alt="{5005E6A5-9A5C-44EE-B8B4-37364CD7D1E2}.png" src="https://notes.sarangwandile.xyz/lib/media/{5005e6a5-9a5c-44ee-b8b4-37364cd7d1e2}.png">
We can check for the further detail of the container with<br>
docker ps<br>
Upon inspection we can see there is no container running<br>
lets check again with docker ps -a and for our surprise container is exited just after it run. Thats because its an Operating system container which doesnt really do anything itself unless we assign it a task or a process that will run in background constantly so we can enter into the container to perform our tasks.
We can achieve this by running the container interactively
but first clean up the exited container with docker rm &lt;container-id&gt;

<br><br>
<br>Run a container interactively.
<br>docker run -it -d -p 32768:80 --name free-css-template amazonlinux
<br>   I run the container with -it and -d so it opened the interactive stdin shell session for me to interact with it and the container itself is in detached mode allowing us to enter into it anytime with exec command<br>docker exec -it &lt;container-id&gt; &lt;shell-command&gt;
<br><img alt="{8DD627B8-39AA-4F0D-8320-CF6DA2D7BB33}.png" src="https://notes.sarangwandile.xyz/lib/media/{8dd627b8-39aa-4f0d-8320-cf6da2d7bb33}.png"><br>we got bash shell session inside container<br>
Now we need to update the packages inside the container<br><br>yum update
<br><img alt="{FEF2B4B8-3A70-49CA-B07C-475B398724EC}.png" src="https://notes.sarangwandile.xyz/lib/media/{fef2b4b8-3a70-49ca-b07c-475b398724ec}.png"><br>
<br>Install nginx package start it
<br>yum install nginx -y
<br> since this amazonlinux container image doesn't come with systemd preinstalled we cannot start nginx daemon so we need to run this command manually in background<br>nginx &amp;
<br>and our nginx server is successfully started at port 32768 on our host ec2-instance<br><img alt="{E0754AEE-5490-497B-AA74-428CD4B239A9}.png" src="https://notes.sarangwandile.xyz/lib/media/{e0754aee-5490-497b-aa74-428cd4b239a9}.png"><br><br>Lets add free css template in it<br>
Head on to <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/" target="_blank">https://www.free-css.com/</a> and download any free css template you want by right clicking on the download button and copying link address<br>
<img alt="{B54DF9F8-AC7B-420B-9EAA-54AE5D0B261F}.png" src="https://notes.sarangwandile.xyz/lib/media/{b54df9f8-ac7b-420b-9eaa-54ae5d0b261f}.png"><br>
and go to your containers terminal session and download this file in nginx server directory with curl or wget<br>cd /usr/share/nginx/html
curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/carvilla.zip

# Extract the zip file
unzip carvilla.zip

# If unzip is not found install unzip package
sudo yum install unzip
<br>extracting the zip has extracted all contents into folder named carvilla-v1.0<br>
I'll rename it for my convenient. <br>mv carvilla-v1.0 mywebsite
<br>and the website is running at port 32768/mywebsite<br>now we can get out of this container with exit command.<br><img alt="{ADBAE0E2-C52F-4FE1-9DBE-85A5BABFF759}.png" src="https://notes.sarangwandile.xyz/lib/media/{adbae0e2-c52f-4fe1-9dbe-85a5babff759}.png"><br>We have our container successfully hosting our free-css template on port http://instance-ip:32768/mywebsite<br>
Lets create the image out of this container so we can spin many containers as we want and save the hustle to do all these steps all over again.<br><br>
<br>Creating the image<br>
<img alt="{FE045571-00F2-44B6-B1CB-E7F26EE9AE7E}.png" src="https://notes.sarangwandile.xyz/lib/media/{fe045571-00f2-44b6-b1cb-e7f26ee9ae7e}.png"><br>
Now that we have our container running and our site is visible I want to make image out of it and push it to docker hub 
<br>for that we use docker commit &lt;container-id&gt;<br>it returns with the sha-id<br>
<img alt="{83AC20A6-5137-45BC-8761-6AD28799FF56}.png" src="https://notes.sarangwandile.xyz/lib/media/{83ac20a6-5137-45bc-8761-6ad28799ff56}.png"><br>with docker images we can confirm the image creation<br>
<img alt="{9EE50687-0A6E-4930-966B-D6D50B9E78B7}.png" src="https://notes.sarangwandile.xyz/lib/media/{9ee50687-0a6e-4930-966b-d6d50b9e78b7}.png"><br>our image shown in first row doesnt have name so lets give it a tag and then push it to repo<br>
docker tag &lt;image-id&gt; &lt;tagname&gt;<br>
docker tag local-image:tagname new-repo:tagname<br><img alt="Pasted image 20241227225859.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227225859.png"><br>Here I have already created repository in docker hub so I gave its name here<br>and run docker hub login command before running push command<br>
docker login<br>and push it to our docker hub<br>
docker push new-repo:tagname<br><img alt="Pasted image 20241227231042.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227231042.png"><br>And our project is successful pushed over docker hub and can be access from here<br><a rel="noopener nofollow" class="external-link" href="https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags" target="_blank">https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags</a><br><br><img alt="{393988AE-E9F0-47A9-A960-5EBE39901FED}.png" src="https://notes.sarangwandile.xyz/lib/media/{393988ae-e9f0-47a9-a960-5ebe39901fed}.png"><br>To store the image into Amazon's Elastic Container Registry which is similar to docker hub but their own we need to create a repository here too.<br>I gave the repo same name<br>
<img alt="{F7CDF0B9-3C17-4D22-9547-7D65863E4BDE}.png" src="https://notes.sarangwandile.xyz/lib/media/{f7cdf0b9-3c17-4d22-9547-7d65863e4bde}.png"><br>and Upon clicking create button my repo is created<br>
<img alt="{7B336AAA-A8CB-4F61-823E-072A4025B6A6}.png" src="https://notes.sarangwandile.xyz/lib/media/{7b336aaa-a8cb-4f61-823e-072a4025b6a6}.png"><br>
Click on the Repo name in blue and click on view push commands to get instruction for pushing our image into this repo<br><img alt="{7CB87CED-E1FD-4698-BCA5-DFC2D6C53B23}.png" src="https://notes.sarangwandile.xyz/lib/media/{7cb87ced-e1fd-4698-bca5-dfc2d6c53b23}.png"><br><br>To push our image we need to first Authenticate with registry for that carefully use first command also we need to have aws-cli installed and configured for that luckily for me I am using amazonlinux ec2 instance so its already installed i can just run aws configure to configure aws-cli<br>
For those havent can check <a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" target="_blank">this documentation</a> to configure their aws-cli within instance.<br><img alt="Pasted image 20241228154309.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228154309.png"><br>and then run this command provided in the instructions but remember to add sudo as we are using standard user<br>
<img alt="{AC6817B4-D44D-4A43-AEFF-16292F4D44D0}.png" src="https://notes.sarangwandile.xyz/lib/media/{ac6817b4-d44d-4a43-aeff-16292f4d44d0}.png"><br>Now that We have successfully login to ECR lets tag the image accordingly and push it to ECR<br>sudo docker tag &lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt; &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;newname&gt;:&lt;newtag&gt;<br><img alt="{19760D25-4AB9-4A59-9757-62575607EF5A}.png" src="https://notes.sarangwandile.xyz/lib/media/{19760d25-4ab9-4a59-9757-62575607ef5a}.png"><br>
and push to ECR<br>sudo docker push &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt;<br><img alt="{D18E449C-3E7D-4B49-8AC1-75893D2A92F7}.png" src="https://notes.sarangwandile.xyz/lib/media/{d18e449c-3e7d-4b49-8ac1-75893d2a92f7}.png"><br>and refresh the amazon EKS web site to see the latest image listed in there<br>
<img alt="{48017E2C-FBB3-4AB6-B125-BFEFA58746BE}.png" src="https://notes.sarangwandile.xyz/lib/media/{48017e2c-fbb3-4ab6-b125-bfefa58746be}.png"><br>Thank you for Reading<br>
Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-2-create-the-free-css-template-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 2 Create the Free-css template container.md</guid><pubDate>Sun, 29 Dec 2024 13:47:16 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Create the MYSQL Container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Host mysql container 
<br>Use -e ## environment variables
<br>use MYSQL_USER, MYSQL_PASSWORD
<br><br><br><br>sudo docker pull mysql
<br><br>sudo docker run -e MYSQL_USER=sarang -e MYSQL_PASSWORD=mysupersecretpassword -e MYSQL_RANDOM_ROOT_PASSWORD=yes -p 3306:3306 --name mysql_container -v mysql_data:/var/lib/mysql mysql:latest
<br>Here we used -e flag to specify each environment variables for mysql<br>MYSQL_USER=sarang  This sets the username for our mysql database<br>
MYSQL_PASSWORD=mysupersecretpassword This sets the password for our database<br>MYSQL_RANDOM_ROOT_PASSWORD=yes    # This tells the mysql to generate a random root password (it can be seen in docker logs)<br>
-v mysql_data:/var/lib/mysql mysql:latest This flag sets the persistence volume for our container and it mounts /var/lib/mysql directory of the container to the mysql_data directory of our host.]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 3 Create Mysql Container.md</guid><pubDate>Mon, 30 Dec 2024 15:50:54 GMT</pubDate></item><item><title><![CDATA[Task 4 Create daemon service of tomcat]]></title><description><![CDATA[ 
 <br>The simplest possible script I could make for tomcat is this one<br># make catalina.sh executable
chmod +x ~/apache-tomcat-9.0.98/bin/catalina.sh 

# navigate to systemd services directory
cd /etc/systemd/system

# Create new tomcat service file
vi tomcat.service
<br>Add this script <br>[Unit]
Description=Tomcat Service Daemon

[Service]
Type=forking
ExecStart=home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start
ExecReload=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start
ExecStop=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh stop

[Install]
WantedBy=multi-user.target
<br>Run these commands afterwards<br># Reloading the systemd daemon
systemctl daemon-reload

# Enabling and starting tomcat.service
systemctl enable --now tomcat.service

# Checking Status of our service
systemctl status tomcat.service
<br>However this isnt sufficient for tomcats full usecase<br>
for that you need to add Environment Variables<br># Set environment variables (optional but recommended) 

# Update with your Java installation path
Environment=JAVA_HOME=/usr/lib/jvm/jre 

# Update with your Tomcat PID file path 
Environment=CATALINA_PID=/home/ec2-user/apache-tomcat-9.0.98/temp/tomcat.pid 

# Update with your Tomcat installation path 
Environment=CATALINA_HOME=/home/ec2-user/apache-tomcat-9.0.98 

# Update with your Tomcat base directory 
Environment=CATALINA_BASE=/home/ec2-user/apache-tomcat-9.0.98 

# Customize JVM options 
Environment='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'
Environment='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'

<br>also for security purposes its recommended to use tomcat service as a dedicated tomcat user<br>after making changes in the file its important to reload systemd daemon with systemctl daemon-reload and restarting the tomat service and check the status.<br>If everything is working and your tomcat page is displaying on port 8080 it means your tomcat service is successfully working.]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 4 Create daemon service of tomcat.md</guid><pubDate>Mon, 30 Dec 2024 15:50:44 GMT</pubDate></item><item><title><![CDATA[Top 40 Linux Commands]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>]]></description><link>https://notes.sarangwandile.xyz/linux/top-40-linux-commands.html</link><guid isPermaLink="false">Linux/Top 40 Linux Commands.md</guid><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[Understanding Vim]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br>Vim Modes<br>]]></description><link>https://notes.sarangwandile.xyz/linux/understanding-vim.html</link><guid isPermaLink="false">Linux/Understanding Vim.md</guid><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[My DevOps Notes]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br><br>Author: Sarang Wandile<br>
Github: <a data-tooltip-position="top" aria-label="https://github.com/srngx" rel="noopener nofollow" class="external-link" href="https://github.com/srngx" target="_blank">srngx</a><br>Hello, My name is Sarang, in this place I store my DevOps related Notes, assignments and Practical. Feel free to explore.<br>These notes are made using <a data-tooltip-position="top" aria-label="https://obsidian.md" rel="noopener nofollow" class="external-link" href="https://obsidian.md" target="_blank">Obsidian App</a> - A Markdown Notetaking App and Hosted with <a data-tooltip-position="top" aria-label="https://pages.cloudflare.com/" rel="noopener nofollow" class="external-link" href="https://pages.cloudflare.com/" target="_blank">Cloudflare page</a>.<br>Warning
"These notes are not complete and subject to grow, modify and update in time to time, so its very likely that your shared page specific links might not work as that page itself either moved to different folder or replaced. "
<br><br><br><br>Tasks to perform
<a data-href="☑️ Implement autoscaling on memory utilization" href="https://notes.sarangwandile.xyz/aws/tasks-done/☑️-implement-autoscaling-on-memory-utilization.html" class="internal-link" target="_self" rel="noopener nofollow">☑️ Implement autoscaling on memory utilization</a><br>
<a data-href="✅ Host static website on s3 bucket" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-host-static-website-on-s3-bucket.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Host static website on s3 bucket</a><br>
<a data-href="✅ Create 5 IAM users and 5 S3 buckets and attach them each other" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-create-5-iam-users-and-5-s3-buckets-and-attach-them-each-other.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Create 5 IAM users and 5 S3 buckets and attach them each other</a><br>
<a data-href="✅ Implement Template with Scheduled Autoscaling" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-implement-template-with-scheduled-autoscaling.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Implement Template with Scheduled Autoscaling</a><br>
<a data-href="✅ Monitoring Nginx logs with Cloudwatch" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-monitoring-nginx-logs-with-cloudwatch.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Monitoring Nginx logs with Cloudwatch</a><br>
<a data-href="☑️ How to store aws load balancer logs in s3 bucket" href="https://notes.sarangwandile.xyz/aws/tasks-done/☑️-how-to-store-aws-load-balancer-logs-in-s3-bucket.html" class="internal-link" target="_self" rel="noopener nofollow">☑️ How to store aws load balancer logs in s3 bucket</a><br>
<a data-href="✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅🐈‍⬛host-tomcat-basesd-web-app-called-student-app-with-rds-and-ec2.html" class="internal-link" target="_self" rel="noopener nofollow">✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2</a><br>
<a data-href="✅ Get sns alert when any user launches new instance" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-get-sns-alert-when-any-user-launches-new-instance.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Get sns alert when any user launches new instance</a><br>
<a data-href="✅ Create notification for s3 bucket activity happen of put and delete" href="https://notes.sarangwandile.xyz/aws/tasks-done/✅-create-notification-for-s3-bucket-activity-happen-of-put-and-delete.html" class="internal-link" target="_self" rel="noopener nofollow">✅ Create notification for s3 bucket activity happen of put and delete</a>
<br>Task assigned for Reading

<br><a data-href="Agile Development" href="https://notes.sarangwandile.xyz/devops/for-reading/agile-development.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Development</a>
<br><a data-href="Agile Vs DevOps" href="https://notes.sarangwandile.xyz/devops/for-reading/agile-vs-devops.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Vs DevOps</a>
<br><a data-href="Amazon Web Services In Plain English" href="https://notes.sarangwandile.xyz/aws/notes/amazon-web-services-in-plain-english.html" class="internal-link" target="_self" rel="noopener nofollow">Amazon Web Services In Plain English</a>
<br><a data-href="Authorized vs Authonticated" href="https://notes.sarangwandile.xyz/aws/for-reading/authorized-vs-authonticated.html" class="internal-link" target="_self" rel="noopener nofollow">Authorized vs Authonticated</a>
<br><a data-href="AWS Instance Types" href="https://notes.sarangwandile.xyz/aws/for-reading/aws-instance-types.html" class="internal-link" target="_self" rel="noopener nofollow">AWS Instance Types</a>
<br><a data-href="Difference between Load balancers" href="https://notes.sarangwandile.xyz/aws/for-reading/difference-between-load-balancers.html" class="internal-link" target="_self" rel="noopener nofollow">Difference between Load balancers</a>
<br><a data-href="Git Fetch vs Git pull" href="https://notes.sarangwandile.xyz/devops/for-reading/git-fetch-vs-git-pull.html" class="internal-link" target="_self" rel="noopener nofollow">Git Fetch vs Git pull</a>
<br><a data-href="How DNS Works" href="https://notes.sarangwandile.xyz/aws/for-reading/how-dns-works.html" class="internal-link" target="_self" rel="noopener nofollow">How DNS Works</a>
<br><a data-href="IAM Policies" href="https://notes.sarangwandile.xyz/aws/for-reading/iam-policies.html" class="internal-link" target="_self" rel="noopener nofollow">IAM Policies</a>
<br><a data-href="Load Balancer" href="https://notes.sarangwandile.xyz/aws/for-reading/load-balancer.html" class="internal-link" target="_self" rel="noopener nofollow">Load Balancer</a>
<br><a data-href="Managed policies and inline policies" href="https://notes.sarangwandile.xyz/aws/for-reading/managed-policies-and-inline-policies.html" class="internal-link" target="_self" rel="noopener nofollow">Managed policies and inline policies</a>
<br><a data-href="Network Protocols" href="https://notes.sarangwandile.xyz/aws/for-reading/network-protocols.html" class="internal-link" target="_self" rel="noopener nofollow">Network Protocols</a>
<br><a data-href="OSI Model" href="https://notes.sarangwandile.xyz/aws/for-reading/osi-model.html" class="internal-link" target="_self" rel="noopener nofollow">OSI Model</a>
<br><a data-href="SDLC - Software Developement LifeCycle" href="https://notes.sarangwandile.xyz/aws/for-reading/sdlc-software-developement-lifecycle.html" class="internal-link" target="_self" rel="noopener nofollow">SDLC - Software Developement LifeCycle</a>
<br><a data-href="Simple Definitions To give in Interview" href="https://notes.sarangwandile.xyz/aws/notes/simple-definitions-to-give-in-interview.html" class="internal-link" target="_self" rel="noopener nofollow">Simple Definitions To give in Interview</a>
<br><a data-href="Some talks about git in reddit" href="https://notes.sarangwandile.xyz/Some talks about git in reddit" class="internal-link" target="_self" rel="noopener nofollow">Some talks about git in reddit</a>
<br><a data-href="ssl certificate" href="https://notes.sarangwandile.xyz/aws/for-reading/ssl-certificate.html" class="internal-link" target="_self" rel="noopener nofollow">ssl certificate</a>
<br><a data-href="Storage Classes in S3" href="https://notes.sarangwandile.xyz/aws/for-reading/storage-classes-in-s3.html" class="internal-link" target="_self" rel="noopener nofollow">Storage Classes in S3</a>
<br><a data-href="Subnetting" href="https://notes.sarangwandile.xyz/aws/for-reading/subnetting.html" class="internal-link" target="_self" rel="noopener nofollow">Subnetting</a>
<br><a data-href="Types of Autoscaling" href="https://notes.sarangwandile.xyz/aws/for-reading/types-of-autoscaling.html" class="internal-link" target="_self" rel="noopener nofollow">Types of Autoscaling</a>

<br><br><br><br>Tasks to perform

<br><a data-href="Task 1 Creating Tomcat student-ui container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-1-creating-tomcat-student-ui-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 1 Creating Tomcat student-ui container</a>
<br><a data-href="Task 2 Create the Free-css template container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-2-create-the-free-css-template-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 2 Create the Free-css template container</a>
<br><a data-href="Task 3 Create Mysql Container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 3 Create Mysql Container</a>
<br><a data-href="Task 4 Create daemon service of tomcat" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html" class="internal-link" target="_self" rel="noopener nofollow">Task 4 Create daemon service of tomcat</a>

<br>Task assigned for Reading

<br><a data-href="Agile Development" href="https://notes.sarangwandile.xyz/devops/for-reading/agile-development.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Development</a>
<br><a data-href="Agile Vs DevOps" href="https://notes.sarangwandile.xyz/devops/for-reading/agile-vs-devops.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Vs DevOps</a>
<br><a data-href="Git Fetch vs Git pull" href="https://notes.sarangwandile.xyz/devops/for-reading/git-fetch-vs-git-pull.html" class="internal-link" target="_self" rel="noopener nofollow">Git Fetch vs Git pull</a>
<br><a data-href="Monolithic vs Microservice Architecture" href="https://notes.sarangwandile.xyz/devops/for-reading/monolithic-vs-microservice-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Monolithic vs Microservice Architecture</a>
<br><a data-href="Git Theory" href="https://notes.sarangwandile.xyz/devops/for-reading/git-theory.html" class="internal-link" target="_self" rel="noopener nofollow">Git Theory</a> 
<br><a data-href="Reading Docker Inspect output" href="https://notes.sarangwandile.xyz/devops/for-reading/reading-docker-inspect-output.html" class="internal-link" target="_self" rel="noopener nofollow">Reading Docker Inspect output</a>
<br><a data-href="read about docker.sock file" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html" class="internal-link" target="_self" rel="noopener nofollow">read about docker.sock file</a>

]]></description><link>https://notes.sarangwandile.xyz/index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Mon, 30 Dec 2024 15:24:24 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>