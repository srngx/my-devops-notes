<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Sarang's DevOps Notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://notes.sarangwandile.xyz/</link><image><url>https://notes.sarangwandile.xyz/lib/media/favicon.png</url><title>Sarang's DevOps Notes</title><link>https://notes.sarangwandile.xyz/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 17 Feb 2025 01:38:00 GMT</lastBuildDate><atom:link href="https://notes.sarangwandile.xyz/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 17 Feb 2025 01:37:36 GMT</pubDate><copyright><![CDATA[Sarang Wandile]]></copyright><ttl>60</ttl><dc:creator>Sarang Wandile</dc:creator><item><title><![CDATA[aws-cli cheatsheet]]></title><description><![CDATA[ 
 <br>Keypairs<br>]]></description><link>https://notes.sarangwandile.xyz/aws/notes/aws-cli-cheatsheet.html</link><guid isPermaLink="false">AWS/Notes/aws-cli cheatsheet.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2024-12-19 Git Class]]></title><description><![CDATA[ 
 <br>DVCS vs CVS -- further expanded in <a data-href="Git Theory and Practical" href="https://notes.sarangwandile.xyz/devops/notes/git-theory-and-practical.html" class="internal-link" target="_self" rel="noopener nofollow">Git Theory and Practical</a><br><br><img alt="DVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"><br><br><img alt="CVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"><br><br>
<br>installing git
<br>signup to github.com
<br><br><br>
<br>git add
<br>git commit
<br>git push
<br>git pull
<br><br><br>Going back to previous commit to and rebasing<br>Seeing git logs<br>
git log<br>Start interactive rebase<br>
git rebase -i &lt;commit hash&gt;<br>In the editor, choose to edit the commit identified in step 3 by changing&nbsp;pick&nbsp;to&nbsp;edit&nbsp;on the first line of the text.<br>edit 8728dbe67 my second commit message
pick 03d69e5d3 my third commit message
pick 8053f7b27 my fourth commit message
<br>make changes and git add . <br>
<br>git commit --amend 
<br>git rebase --continue
<br>git push
<br>removing some files from git index<br>git rm --cached -r folder or files
<br>Learned to create .gitignore file<br>.trash/
.obsidian/
private
<br><br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.github.com/en/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#removing-a-secret-introduced-by-an-earlier-commit-on-your-branch" rel="noopener nofollow" class="external-link" href="https://docs.github.com/en/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#removing-a-secret-introduced-by-an-earlier-commit-on-your-branch" target="_blank">Removing secret introduced by earler commit</a>
<br><a data-tooltip-position="top" aria-label="https://www.freecodecamp.org/news/gitignore-file-how-to-ignore-files-and-folders-in-git/" rel="noopener nofollow" class="external-link" href="https://www.freecodecamp.org/news/gitignore-file-how-to-ignore-files-and-folders-in-git/" target="_blank">How to ignore files in git</a>
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://jvns.ca/blog/2024/01/26/inside-git/" target="_blank">https://jvns.ca/blog/2024/01/26/inside-git/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://githowto.com/git_internals_git_directory" target="_blank">https://githowto.com/git_internals_git_directory</a>
<br><a data-href="Some talks about git in reddit 1" href="https://notes.sarangwandile.xyz/Some talks about git in reddit 1" class="internal-link" target="_self" rel="noopener nofollow">Some talks about git in reddit 1</a>
<br><a rel="noopener nofollow" class="external-link" href="https://git-scm.com/book/en/v2" target="_blank">https://git-scm.com/book/en/v2</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-19-git-class.html</link><guid isPermaLink="false">Daily Notes/2024-12-19 Git Class.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2025-01-23 Tomcat on Single Node Task]]></title><description><![CDATA[ 
 <br>  po<br>
dep<br>
rs<br>
clus<br>
ip nodpo<br>
LB<br>
secre<br>
configmap<br>
volume<br>
namespace<br>
hpa<br>
pvc<br>
rolling upd<br>
applic lifecycle<br>
stateful set<br>
daemon set<br>
tenant<br>
pod affinity<br>
node affinity<br>
node selector<br>
anti afinity<br>
taint and tolerance<br>what is probe?<br>Task <br>
<br>1 Microservice Tomcat 
<br>RS =1
<br>Node selector
<br>pod affinity
<br>node affinity
<br>Implement probe to check if service is running
<br>Database tomcat in Cluster with persistent volume
<br>ingress 
<br>tomcat app resource alloaction:

<br>Request
<br>CPU: 500m
<br>Mem: 1GB
<br>Limit
<br>cpu: 1
<br>memory: 2gb


<br>hard coded values can be sent no secret will be used
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-23-tomcat-on-single-node-task.html</link><guid isPermaLink="false">Daily Notes/2025-01-23 Tomcat on Single Node Task.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-01-29]]></title><description><![CDATA[ 
 <br>sudo yum install -y java-17-amazon-corretto
<br>git code --&gt; build --&gt; push --&gt; deploy<br>devsecops --&gt; git checkout --&gt; git leaks scan --&gt; sonar scan --&gt; build --&gt; image scan / trivy --&gt; push --&gt; deploy<br>ecr policy<br>Re-run Jenkins with Docker Socket Mounted: When you run Jenkins in Docker, you'll need to mount the host‚Äôs Docker socket into the Jenkins container. This allows the Jenkins container to communicate with the Docker daemon running on your local system.<br>Run the following command to start the Jenkins container with Docker socket mounted:<br>docker run -d \
  -p 8080:8080 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v jenkins_home:/var/jenkins_home \
  --name jenkins \
  jenkins/jenkins:latest
<br>Here‚Äôs what this does:<br>
-v /var/run/docker.sock:/var/run/docker.sock: This mounts the Docker socket from your host (/var/run/docker.sock) into the Jenkins container, allowing Jenkins inside the container to access Docker on the host.<br>-v jenkins_home:/var/jenkins_home: This persists your Jenkins configuration and jobs, even if the container is removed.<br>-p 8080:8080: Exposes the Jenkins UI on port 8080 of your host.]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-29.html</link><guid isPermaLink="false">Daily Notes/2025-01-29.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-01-30]]></title><description><![CDATA[ 
 <br>Jenkinsfile to push image into aws ecr<br>agent any

    stages {
        stage('git checkout') {
            steps {
                checkout scmGit(branches: [[name: '*/${BRANCH}']], extensions: [], userRemoteConfigs: [[credentialsId: 'git-user', url: 'https://github.com/cholekulche/BE-application-student.git']])
            }
        }
        stage('build') {
            steps {
                sh 'sudo docker build -t student-ui .'
            }
        }
        stage('ecr push') {
            steps {
                sh '''
                aws ecr get-login-password --region us-east-2 | sudo docker login --username AWS --password-stdin 970653867674.dkr.ecr.us-east-2.amazonaws.com
                sudo docker tag student-ui:latest 970653867674.dkr.ecr.us-east-2.amazonaws.com/student-ui:${BRANCH}-${BUILD_TIMESTAMP}
                sudo docker push 970653867674.dkr.ecr.us-east-2.amazonaws.com/student-ui:${BRANCH}-${BUILD_TIMESTAMP}
                '''
            }
        }
        stage('docker cleanup') {
            steps {
                sh 'sudo docker system prune -a -f'
            }
        }
    }
}
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-30.html</link><guid isPermaLink="false">Daily Notes/2025-01-30.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-02-01]]></title><description><![CDATA[ 
 <br>Docker in docker<br>Sonarcube<br>jenkins upgrade<br>sonar cube = code coverage<br>
<br>not testing tool
<br>code analysis and improvement
<br>sudo alternative --config java<br>Installing sonarcube<br>
<br>
install java-17

<br>
install postgres

<br>
   maven plugin<br>
- surefire

<br>pom.xml file for sonarqube plugin]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-02-01.html</link><guid isPermaLink="false">Daily Notes/2025-02-01.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-02-02]]></title><description><![CDATA[ 
 <br>üî• Adding Docker-Based Jenkins Agents<br>To add Jenkins agents running as Docker containers, you need to:<br>
1Ô∏è‚É£ Install Docker in the Jenkins controller container.<br>
2Ô∏è‚É£ Install and configure the "Docker Plugin" in Jenkins.<br>
3Ô∏è‚É£ Configure Jenkins to launch Docker-based agents dynamically.<br>
üöÄ Step 1: Run Jenkins with Docker Access<br>First, ensure your Jenkins container can communicate with the Docker daemon. Run Jenkins with the Docker socket mounted:<br>docker run -d \
  -p 8080:8080 \
  -p 50000:50000 \
  --name jenkins \
  --user root \
  -v jenkins_home:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  jenkins/jenkins:lts
<br>Now, install Docker inside the Jenkins container:<br>docker exec -it jenkins bash
apt-get update &amp;&amp; apt-get install -y docker.io
exit
<br>üîß Step 2: Install the Docker Plugin in Jenkins<br> Open Jenkins at http://your-server-ip:8080/<br>
<br>Go to Manage Jenkins ‚Üí Manage Plugins
<br>In the "Available" tab, search for "Docker Plugin" and install it.
<br>Restart Jenkins after installation.
<br>‚öôÔ∏è Step 3: Configure Docker Cloud in Jenkins<br>Go to Manage Jenkins ‚Üí Manage Nodes and Clouds ‚Üí Configure Clouds
Click Add a new cloud ‚Üí Select Docker
In "Docker Host URI", enter:

unix:///var/run/docker.sock

Click Test Connection ‚Üí It should be successful.
<br>üèó Step 4: Create a Docker Agent Template<br>Under "Docker Agent templates," click Add Docker Template
Set:
    Label: docker-agent
    Docker Image: Use jenkins/inbound-agent (official Jenkins agent)
    Remote File System Root: /home/jenkins
    Usage: "Use this node as much as possible"
    Launch Method: Select "Attach Docker container"
Click Save.
<br>‚úÖ Step 5: Run a Test Pipeline Using Docker Agents<br>Now create a simple Jenkins Pipeline:<br>Go to Jenkins Dashboard ‚Üí New Item ‚Üí Pipeline<br>
Use the following script:<br>pipeline {
    agent { label 'docker-agent' }
    stages {
        stage('Run on Docker Agent') {
            steps {
                sh 'echo "Running on a Docker Agent!"'
                sh 'docker --version'
            }
        }
    }
}
<br>Click Build Now ‚Üí Jenkins will automatically start a Docker agent and run the job inside it.<br>üéØ Final Notes<br>
<br>Dynamic Agents: Jenkins will only spin up agents when needed and remove them after execution.
<br>If agents don‚Äôt start: Check Manage Nodes and Clouds ‚Üí Check agent logs.
<br>Using Custom Images: You can specify a different agent image, e.g., python:3.10, maven:latest, etc.
<br><br>Let me know if you need more details! üöÄ<br>It looks like Jenkins inside your container does not have the Docker CLI installed. Mounting the Docker socket (/var/run/docker.sock) allows the container to communicate with the Docker daemon running on your host, but the Jenkins container itself still needs the docker CLI to execute commands.<br>
Solution: Use a Docker-in-Docker (DinD) Approach<br>Instead of using the basic jenkins/jenkins image, use jenkins/jenkins:lts with Docker installed or use a dedicated image like jenkins/jenkins:lts-jdk17 that comes with Docker.<br>Steps to Fix<br>1Ô∏è‚É£ Run Jenkins with Docker Installed<br>Instead of jenkins/jenkins:latest, use jenkins/jenkins:lts and manually install Docker inside it.<br>docker run -d \
  -p 8080:8080 \
  -p 50000:50000 \
  --name jenkins \
  --user root \
  -v jenkins_home:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  jenkins/jenkins:lts
<br>2Ô∏è‚É£  Install Docker in the Jenkins Container<br>Once the container is running, enter it:<br>docker exec -it jenkins bash
<br>Then, install Docker inside the Jenkins container:<br>apt-get update &amp;&amp; apt-get install -y docker.io
<br>3Ô∏è‚É£ Verify Docker Inside Jenkins<br>Run inside the container:<br>docker --version
docker ps
<br>If these commands work, your Jenkins pipelines should now be able to execute docker commands successfully.<br>
Alternative: Use the jenkinsci/blueocean Image<br>The jenkinsci/blueocean image comes pre-installed with Docker CLI:<br>docker run -d \
  -p 8080:8080 \
  -p 50000:50000 \
  --name jenkins \
  --user root \
  -v jenkins_home:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  jenkinsci/blueocean

<br>This might be an easier way if you don‚Äôt want to manually install Docker inside Jenkins.<br>Let me know if you need more help! üöÄ<br>
You said:<br>
why   -p 50000:50000 \<br>
ChatGPT said:<br>
ChatGPT<br>The -p 50000:50000 option in your Docker run command is used to expose the Jenkins agent (slave) communication port.<br>
üîπ Why is Port 50000 Used?<br>Jenkins uses port 50000 for JNLP-based (Java Network Launch Protocol) agent communication. This is needed when Jenkins connects to remote build agents (worker nodes) that run jobs.<br>
<br>If you plan to run Jenkins agents on different machines and connect them to your Jenkins controller, you should expose port 50000.
<br>If you are running everything on a single container without agents, you don't need -p 50000:50000.
<br>‚úÖ When to Use -p 50000:50000<br>‚úîÔ∏è If you want to add remote Jenkins agents.<br>
‚úîÔ∏è If you use JNLP (Java Web Start) agents to connect workers to Jenkins.<br>
‚ùå When You Can Skip It<br>‚ùå If you are running all jobs inside the Jenkins container itself.<br>
‚ùå If you are using Docker-based agents instead of traditional JNLP agents.<br>
üõ† Example Without Agents (If You Don‚Äôt Need Port 50000)<br>docker run -d \
  -p 8080:8080 \
  --name jenkins \
  --user root \
  -v jenkins_home:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  jenkins/jenkins:lts
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-02-02.html</link><guid isPermaLink="false">Daily Notes/2025-02-02.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-02-03]]></title><description><![CDATA[ 
 <br>Run using docker<br>$ docker run -d --name sonarqube -p 9000:9000 sonarqube
<br>mvn clean verify sonar:sonar \
  -Dsonar.projectKey=student \
  -Dsonar.projectName='student' \
  -Dsonar.host.url=http://3.137.177.165:32768 \
  -Dsonar.token=sqp_3584391716cf8fbc01eeea4e17d34b35b07d5a2f
<br> docker run -d --privileged -P --user root  -v /mnt:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkins/jenkins<br>  nexus<br>
jfrog<br>  -&gt; s3 ]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-02-03.html</link><guid isPermaLink="false">Daily Notes/2025-02-03.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[2025-02-04]]></title><description><![CDATA[ 
 <br>Run sonarqube using docker<br>artifactory to store artifact<br>
<br>nexus
<br>jfrog
<br>s3
<br>aws-cli container<br>storing artifact in s3<br>github pages - How to create free portfolio website using github pages<br>cluster creates infra in its own way<br>
terraform gives complete control<br>IAC = Infrastructure as a Code<br>main.tf<br>provider.tf<br>
cloud provider information<br>variable.tf<br>terraform init<br>
provider download<br>
terraform.lock.hcl created<br>terraform plan<br>
will show infra plan<br>terraform apply<br>
terraform.tfstate created<br>tf destroy<br>
deletes all resources mentioned in tfstate file<br>Task: create 3 ec2 instances with terraform]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-02-04.html</link><guid isPermaLink="false">Daily Notes/2025-02-04.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Read about each and every component of kubernetes]]></title><description><![CDATA[ 
 <br><br>Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It follows a master-worker architecture consisting of multiple components that work together to maintain the desired state of a cluster. Below is an explanation of each component in Kubernetes, along with an architecture diagram and manifest files.<br><br><br>The master node is the brain of the Kubernetes cluster. It manages and controls the cluster, making global decisions (e.g., scheduling), detecting and responding to cluster events (e.g., starting up pods), and managing the overall state of the cluster.<br>The master node consists of the following components:<br>
<br>API Server (kube-apiserver):<br>
The API server is the central management point for Kubernetes. It exposes the Kubernetes API, which is used for communicating with other components of the system. All requests (whether for interacting with pods, nodes, deployments, etc.) are handled by the API server.<br>

<br>Scheduler (kube-scheduler):<br>
The scheduler is responsible for selecting which node a newly created pod will run on based on resource availability and constraints defined by the user. It makes the decision to place pods on specific worker nodes.<br>

<br>Controller Manager (kube-controller-manager):<br>
The controller manager ensures that the desired state of the cluster is maintained. It manages controllers like the ReplicaSet, Deployment, and Node controllers. The controllers constantly check the state of the cluster and take corrective actions to meet the desired state.<br>

<br>etcd:<br>
etcd is a distributed key-value store that holds the entire configuration and state of the Kubernetes cluster. It is the source of truth for all cluster data, such as the definitions of nodes, pods, deployments, services, and more. It‚Äôs highly available and fault-tolerant.<br>

<br><br>Worker nodes (also called minions) are the machines responsible for running the application workloads in the form of containers. Each node in a Kubernetes cluster contains the following components:<br>
<br>Kubelet:<br>
The kubelet is an agent that runs on each worker node. It ensures that containers are running in a pod by communicating with the API server and monitoring the health of the containers.<br>

<br>Kube Proxy:<br>
Kube Proxy manages the networking and load balancing between pods. It ensures that each pod has network connectivity and routes traffic to the correct backend service.<br>

<br>Container Runtime:<br>
This is the software responsible for running containers. Examples of container runtimes are Docker, containerd, and CRI-O.<br>

<br><br>A pod is the smallest and simplest Kubernetes object. It is a single instance of a running process in the cluster, and it can contain one or more containers. All containers in a pod share the same network namespace and storage.<br><br>Namespaces provide a way to divide cluster resources between multiple users. Namespaces are used for organizing resources within a cluster into logical groups. They are primarily useful in large clusters with many users.<br><br>A service is an abstraction that defines a set of pods and a policy by which to access them. Services ensure that applications can communicate with each other reliably, regardless of pod lifecycles or IP changes.<br><br>A deployment provides declarative updates for pods and ReplicaSets. It allows you to manage and scale applications running in pods, ensuring that the desired number of replicas is always running.<br><br>ReplicaSets ensure that a specified number of identical pods are running at any given time. A ReplicaSet can be used by a Deployment to maintain the number of replicas.<br><br>StatefulSets are similar to ReplicaSets but designed for applications that require persistent storage and stable network identities. These are often used for stateful applications like databases.<br><br>
<br>ConfigMap: A way to inject configuration data into pods, which can be used as environment variables, command-line arguments, or configuration files.
<br>Secret: Stores sensitive data like passwords, OAuth tokens, or ssh keys in an encrypted form.
<br><br>Ingress is a collection of rules that allow inbound connections to reach the cluster services. It manages HTTP and HTTPS routes to services.<br><br><br>Here's a high-level diagram of the Kubernetes architecture:<br>lua<br>Copy code<br>                +--------------------------------------+
                |               Kubernetes             |
                |             Master Node              |
                |                                      |
                |   +----------------------------+     |
                |   |    kube-apiserver           |     |
                |   +----------------------------+     |
                |   |    kube-scheduler           |     |
                |   +----------------------------+     |
                |   |    kube-controller-manager  |     |
                |   +----------------------------+     |
                |   |    etcd                     |     |
                +--------------------------------------+
                           /              \
                          /                \
                         /                  \
                        /                    \
       +------------------+           +------------------+  
       |  Worker Node 1   |           |  Worker Node 2   |
       |                  |           |                  |
       |  +-----------+   |           |  +-----------+   |
       |  | Kubelet   |   |           |  | Kubelet   |   |
       |  +-----------+   |           |  +-----------+   |
       |  | Kube Proxy|   |           |  | Kube Proxy|   |
       |  +-----------+   |           |  +-----------+   |
       |  | Container  |   |           |  | Container  |   |
       |  | Runtime    |   |           |  | Runtime    |   |
       |  +-----------+   |           |  +-----------+   |
       +------------------+           +------------------+


<br><br>Kubernetes uses manifest files (usually written in YAML or JSON) to declare the desired state of various Kubernetes resources. Below are some key examples of manifest files.<br><br>A simple pod manifest that defines a pod with a single container running Nginx:<br>apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
<br><br>A deployment manifest that creates a Deployment for Nginx with 3 replicas:<br>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

<br><br>A service manifest to expose the Nginx deployment:<br>apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

<br><br>A ConfigMap for providing configuration data to a pod:<br>apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    server {
      listen       80;
      server_name  localhost;
      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
    }

<br><br>A Secret to store sensitive information like a password:<br>apiVersion: v1
kind: Secret
metadata:
  name: db-password
type: Opaque
data:
  password: cGFzc3dvcmQ=  # This is 'password' encoded in base64

<br><br>An Ingress resource to manage HTTP routing:<br>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  rules:
  - host: nginx.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
<br><br><br>Kubernetes is a complex system made up of many components that work together to provide a platform for managing containerized applications. The Master Node is responsible for controlling the cluster, while the Worker Nodes run the actual workloads in containers. Resources such as Pods, Services, Deployments, ConfigMaps, and Secrets are defined through YAML manifests that allow for declarative management of the desired state.<br>By using Kubernetes' powerful abstractions, you can efficiently deploy, scale, and manage applications in a fault-tolerant and automated manner.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/read-about-each-and-every-component-of-kubernetes.html</link><guid isPermaLink="false">DevOps/For Reading/Read about each and every component of kubernetes.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Tomcat on single node]]></title><description><![CDATA[ 
 <br><br>
<br>1 Microservice = Tomcat 
<br>ReplicaSet 1
<br>Add Node selector
<br>add pod affinity
<br>add node affinity
<br>Implement probe to check to see if service is running
<br>Add Database tomcat in Cluster with persistent volume
<br>Use ingress LB
<br>tomcat app resource alloaction:

<br>Request
<br>CPU: 500m
<br>Mem: 1GB
<br>Limit
<br>cpu: 1
<br>memory: 2gb


<br>hard coded values can be sent so no secret will be used<br>
or<br>
Use online bookshop application where secrets and configMap can be used
]]></description><link>https://notes.sarangwandile.xyz/devops/k8s-tasks/tomcat-on-single-node.html</link><guid isPermaLink="false">DevOps/K8S Tasks/Tomcat on single node.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Docker Theory and Practical]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br>
Before Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture<br><br>At initial stages of the software development developers usually build software in monolithic way meaning everything that software has to offer is bundle in the single unit but as the development goes on the need to federate the services of the software arises and developers adapt Microservices method to divide software into chunks so all chunks communicate with each other creating illusion of one single software.<br><br>
<br>Single unified application
<br><br>
<br>one app divided in chunks
<br><br>*Docker is a containerization technology that packages your application in special portable sandboxed format along with its dependencies and system libraries for better compatibility across devices.<br><img alt="Docker-Architecture-diagram_1.excalidraw.svg" src="https://notes.sarangwandile.xyz/lib/media/docker-architecture-diagram_1.excalidraw.svg"><br><br>
<br>its like small virtual machines
<br>containers are free to use all the system resources they needs
<br>whereas in vms softwares has limited preallocated resources to use
<br><br>#Creating a nginx container
docker run nginx

# run as detached mode
docker run -d nginx

# list running containers
docker ps 

# list all containers
docker ps -a

# kill running container
docker kill &lt;container-id&gt;

# expose container on random host port
docker run -P nginx

# expose container on custom host port
docker run -d -p 1313:8080 nginx:latest

# Stopping the container
docker stop &lt;container id&gt;

# removing stopped containers
docker rm &lt;container id&gt;

<br><br><br>Installing docker on amazonlinux and rhel based distros are much easier than installling on ubuntu<br>sudo yum install docker # rhel/amazonlinux/fedora etc
sudo pacman -S docker   # archlinux based distros
<br><br>curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; sudo sh get-docker.sh`
<br>Running this script should install following package inside your ubuntu based distro docker-engine, docker.io, containerd, runc.<br>Tip
However I still recommend to check <a data-tooltip-position="top" aria-label="https://docs.docker.com/engine/install/ubuntu/" rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">this official installation guide</a>
<br><br>systemctl start docker
systemctl enable docker
<br>To run docker command its absolutely require to have docker daemon running in background otherwise it wont work so make sure you have docker daemon running.<br><br>docker pull nginx
<br>This will pull latest version of nginx image from docker hub.<br>
By default its always going to pull latest tag<br>
In order to pull specific version you need to mention its tag name like this<br>docker pull nginx:1.23.1
<br><br>docker run nginx
<br>This command creates and starts a Docker container running the Nginx web serve. Although By default, Nginx runs on port 80 within the container. This command doesn't map any ports, so you won't be able to access the Nginx server from your host machine to specify port you can use-p flag.<br><br>docker run -p 8080:80 nginx
<br>Here 8080 is host machine port and 80 is container port<br>docker run -P nginx
<br>This will map random port for the host machine.<br><br>docker run -d nginx`
<br>Normally running container will cause container to run in foreground and occupy the terminal screen. So its better choice to use -d to make it run in background.<br><br>docker run --name my-container nginx
<br>By default docker give funny names to containers. but you can specify your custom name and can use that name instead of 'container-id' in docker commands.<br><br>docker stop &lt;conainer-id&gt;
<br>Stopped containers and be restarted with start command<br><br>docker rm &lt;container-id&gt;
<br>Sometimes stopped or exited containers wont have any use and needs to be removed to save space.<br><br>docker ps
<br>This will list all the running containers only and not exited or stopped ones.<br>To list all containers including exited and stopped ones use ps -a<br>docker ps -a
<br><br>docker logs &lt;container-id&gt;
<br>This is important to monitor activities inside of containers.<br><br>docker inspect &lt;container-id&gt;
<br>This will list all the information of container<br><br>docker commit &lt;container-id&gt; 
<br>This will come in handy when you done changes inside running container and want to incorporate these changes into your image.<br>$ sudo docker images
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
&lt;none&gt;       &lt;none&gt;    5c9d3a455e0f   10 seconds ago   214MB
<br><br>docker tag &lt;image-id&gt; new-name
<br>Adding tag is like giving Image a name this comes in handy for identifying images.<br>$ sudo docker images
REPOSITORY     TAG       IMAGE ID       CREATED          SIZE
custom-nginx   latest    5c9d3a455e0f   47 seconds ago   214MB
<br><br>docker push &lt;image-id&gt;
<br>If you have verified docker hub login through docker login command it will automatically push the image to your docker hub repo assuming your docker hub repo has same name as your image tag<br>and same goes to ECR except you need to authenticate through slightly different method:<br>aws ecr get-login-password --region &lt;region-name&gt; | docker login --username AWS --password-stdin &lt;amazon-account-id&gt;.dkr.ecr.&lt;region-name&gt;.amazonaws.com
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/" target="_blank">https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://adventofdocker.com" target="_blank">https://adventofdocker.com</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/docker-theory-and-practical.html</link><guid isPermaLink="false">DevOps/Notes/Docker Theory and Practical.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Git Notes]]></title><description><![CDATA[ 
 <br><br>Git is a Distributed Version Control System that mainly used for Source code management by developers.<br>Git basically divides your code into three part.<br>
<br>Working Area
<br>Staging Area
<br>Head or commit
<br>Working Area is your state of code before running the git add command.<br>
The files in here also known as "untracked files".<br>To add the files into git you do git add and they moved to staging area.<br>Here your code got the stage. <br>Before finalizing your code into production i.e. 'commit' you make required changes and after making sure<br>
your code is ready to go into production you run git commit command.<br>
which is basically telling git this is a final changes with the special stamp of "commit hash".<br>Then If you want you can push your code into remote repository sitting onto the cloud waiting to be forked and contributed by peoples across the glob.<br><br>
<br>git add     To add code into git
<br>git commit  To commit changes 
<br>git push    To upload your code into remote repo
<br>git pull    To download latest changes from remote to your local
<br>git fetch   Fetching Metadata and logs from remote 
<br><br>Git Provide "Branches" which are nothing but a way to seperate your code into different version so you can later integrate your features and bugfixes into production once they finished peacefully in their own isolated places.<br>There are well known three branches people generally creates:<br>
<br>Main  (which is your production branch)
<br>Dev   (which is your ongoing development which never cease to stop.)
<br>Test  (Here your code is being tested against performance and security standards.)
<br>Some people also like to create their own custom branches like alpha, beta, stable etc i.e. according to release state.<br><br>
<br>git branch -a   List all available branches
<br>git checkout -b Create and switch to the new branch
<br>git checkout    To switch to different branch
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/git-notes-in-my-own-words.html</link><guid isPermaLink="false">DevOps/Notes/Git Notes in My Own words.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Git Theory and Practical]]></title><description><![CDATA[ 
 <br><br><br>A centralized version control system (VCS) uses a single, central repository to store all file versions and their change history. Team members have their own working copies, but all modifications are ultimately committed to this central server. This facilitates collaboration by providing a single source of truth, but it also creates a single point of failure. Examples include <a data-tooltip-position="top" aria-label="https://subversion.apache.org/" rel="noopener nofollow" class="external-link" href="https://subversion.apache.org/" target="_blank">Subversion (SVN)</a> and <a data-tooltip-position="top" aria-label="https://www.nongnu.org/cvs/" rel="noopener nofollow" class="external-link" href="https://www.nongnu.org/cvs/" target="_blank">CVS</a>.<br><img alt="CVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"><br><br>In a Decentralized Version Control System (DVCS), every user has a complete copy of the repository, including its entire history. This eliminates the reliance on a central server, allowing for offline work and greater flexibility. Changes are shared between repositories as needed. Popular examples include <a data-tooltip-position="top" aria-label="https://git-scm.com" rel="noopener nofollow" class="external-link" href="https://git-scm.com" target="_blank">Git</a> and <a data-tooltip-position="top" aria-label="https://www.mercurial-scm.org/" rel="noopener nofollow" class="external-link" href="https://www.mercurial-scm.org/" target="_blank">Mercurial</a>.<br><img alt="DVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"><br><br>Git is like a super-powered tracking system for your files and the changes you make to them over time. Imagine it as a special folder that remembers every version of your work, allowing you to go back to any previous stage if needed. &nbsp;<br>Here's what makes Git special:<br>
<br>Keeps a detailed history: Git meticulously records every change you make to your files, who made it, and when. This history helps you understand how your project evolved. &nbsp;
<br>Branching and merging: Git allows you to create separate branches, like alternate timelines, to experiment with new features or try different ideas without affecting the main project. You can then merge these branches back into the main project when you're ready. &nbsp;
<br>Collaboration made easy: Git is designed for teamwork. Multiple people can work on the same project simultaneously, and Git helps manage and integrate everyone's contributions smoothly. &nbsp;
<br>Offline access: You have the entire project history on your computer, so you can work even without an internet connection. &nbsp;
<br>Popular and widely used: Git is the most popular version control system in the world, used by countless developers and companies. &nbsp;
<br><br>Git was created by Linus Torvalds, the famous creator of the Linux operating system, in 2005. He needed a better tool to manage the Linux kernel development, as the existing version control systems were not efficient enough for such a large and complex project.<br>Here's a glimpse of Git's history:<br>
<br>Early Days (2002-2005): Linux kernel development relied on a proprietary DVCS called <a data-tooltip-position="top" aria-label="https://www.bitkeeper.org/" rel="noopener nofollow" class="external-link" href="https://www.bitkeeper.org/" target="_blank">BitKeeper</a>. When its free-of-charge use was revoked, Torvalds decided to create his own version control system, with the goal of being faster, simpler, and more robust.
<br>Birth of Git (April 2005): Torvalds began work on Git and within a remarkably short period, had a functional system ready to manage the Linux kernel.
<br>Community Takes Over (July 2005): Junio Hamano took over the maintenance of Git, guiding its development and shaping it into the mature system it is today.
<br>Widespread Adoption: Git's speed, flexibility, and powerful features quickly gained popularity among developers. It became the preferred choice for open-source projects and eventually spread to commercial software development.
<br>Today, Git is the most widely used version control system worldwide, powering the development of countless software projects, from small personal projects to massive corporate endeavors.<br><br>
<br>Signup to <a data-tooltip-position="top" aria-label="https://github.com" rel="noopener nofollow" class="external-link" href="https://github.com" target="_blank">github.com</a>
<br>Install git with <a data-tooltip-position="top" aria-label="http://git-scm.com" rel="noopener nofollow" class="external-link" href="http://git-scm.com" target="_blank">gitbash</a> in windows 
<br>or via package manager in linux

<br>for ubuntu apt install git
<br>for rhel based distros yum install git
<br>for arch based distros pacman -S git


<br><br>
<br>Create empty folder in local machine
<br>navigate to it and run git init
<br>add some files and run git add .
<br>commit changes `git commit -m "git initialized and new files added"
<br>Create repository in github and use its link to below step
<br>Add remote repo git remote add origin git@github.com:&lt;username&gt;/&lt;repo-name&gt;.git
<br>update the remote repo with push git push origin main
<br><br><br><br>git add . 
<br>This command will staged all files and modifications done in current and subdirectories but wont wont stage deletions.<br>git add -A 
<br>This command will stage all types of changes across your entire working directory including newfiles, modifications and deletions<br><br>git commit -m "new commit"
<br>Creates a commit with the specified message. This is the most common way to commit changes.<br>git commit -A "your commit message"
<br>Automatically stages all tracked files that have been modified and deleted, then creates a commit. It combines git add -A and git commit -m into a single command.<br>Does not stage any new files. If you have new files you want to include in the commit, you still need to stage them separately using git add.<br><br>git push origin main
<br>Uploads your local changes to the remote repository.<br>
After pushing, other developers can access your changes by pulling from the remote repository.<br><br>git pull origin main
<br>Downloads changes from the remote repository to your local repository. Used to stay up-to-date with the latest changes made by other develpers.*]]></description><link>https://notes.sarangwandile.xyz/devops/notes/git-theory-and-practical.html</link><guid isPermaLink="false">DevOps/Notes/Git Theory and Practical.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[My limited diff usecase In case I forgot]]></title><description><![CDATA[ 
 <br>What is diff?<br>Shows difference/comparison between files<br>Why is it useful?<br>
<br>Tracking changes: See how a file has evolved over time.
<br>Comparing versions: Find out what's different between two versions of a document or code.
<br>Merging changes: Helps you combine different versions of a file.
<br>Troubleshooting: Identify what went wrong in a configuration file or program.
<br>How to use it?<br>The basic command is super simple:<br>diff file1 file2
<br>Just replace file1 and file2 with the actual names of your files.<br>Making the output more helpful<br>diff has some options to make the output easier to understand:<br>
<br>-u (or --unified): Shows the changes in a "unified" format, which is more compact and easier to read.
<br>-y (or --side-by-side): Displays the files side-by-side, so you can see the differences directly.
<br>-w (or --ignore-all-space): Ignores differences in whitespace (spaces and tabs), which is helpful if you only care about the content.
<br>-r (or --recursive): Compares entire directories, not just individual files.
<br>-q (or --brief): Only tells you if the files are different, not what the differences are.
<br>Example:<br>Let's say you have two files, original.txt and revised.txt. To see the differences in a unified format:<br>diff -u original.txt revised.txt
<br>Understanding the output<br>diff uses some symbols to show you the changes:<br>
<br>+: Lines added in the second file.
<br>-: Lines removed from the first file.
<br>@@: Indicates a block of changed lines.
<br><br>See difference between two folders<br>
Basic Usage<br>diff -qr folder1 folder2
<br>
<br>-q: Only reports when files differ.
<br>-r: Recursively compares subdirectories.
<br>More detailed output<br>diff -ruN dir1 dir2
<br>
<br>-r: Recursively compares subdirectories.
<br>-u: Produces a unified diff format, which is easier to read.
<br>-N: Treats absent files as empty.
<br>For files<br>diff -wy file1.txt file2.txt
<br>
<br>-w : for ignoring whitespaces
<br>-y : side by side comparison
<br>Some Very Important Flags<br>
<br>--color=always
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/my-limited-diff-usecase-in-case-i-forgot.html</link><guid isPermaLink="false">DevOps/Notes/My limited diff usecase In case I forgot.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Understanding Manifest File]]></title><description><![CDATA[ 
 <br>Each configuration file has 3 part<br>
<br>Metadata
<br>Specification
<br>Status (often autogenerated by kubernetes)
<br>Attributes of "spec" are specific to the "kind"<br>Metadata part contains Labels and<br>
Specification part contains Selectors]]></description><link>https://notes.sarangwandile.xyz/devops/notes/understanding-manifest-file.html</link><guid isPermaLink="false">DevOps/Notes/Understanding Manifest File.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Jenkins First Look]]></title><description><![CDATA[ 
 <br><br><img alt="Jenkins Logo" src="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Jenkins-Logo.png" referrerpolicy="no-referrer"><br>Jenkins is an open-source server that is written entirely in Java. It lets you execute a series of actions to achieve the continuous integration process, that too in an automated fashion.<br>This CI server runs in servlet containers such as Apache Tomcat. Jenkins facilitates <a data-tooltip-position="top" aria-label="https://www.lambdatest.com/blog/what-is-continuous-integration-and-continuous-delivery/" rel="noopener nofollow" class="external-link" href="https://www.lambdatest.com/blog/what-is-continuous-integration-and-continuous-delivery/" target="_blank">continuous integration and continuous delivery</a> in software projects by automating parts related to build, test, and deployment. This makes it easy for developers to continuously work on the betterment of the product by integrating changes to the project.<br>Jenkins automates the software builds in a continuous manner and lets the developers know about the errors at an early stage. A strong Jenkins community is one of the prime reasons for its popularity. Jenkins is not only extensible but also has a thriving plugin ecosystem.<br>Some of the possible steps that can be performed using Jenkins are:<br>
<br>Software build using build systems such as Gradle, Maven, and more.
<br>Automation testing using test frameworks such as Nose2, PyTest, Robot, Selenium, and more.
<br>Execute test scripts (using Windows terminal, Linux shell, etc.
<br>Achieve test results and perform post actions such as printing test reports, and more.
<br>Execute test scenarios against different input combinations for obtaining improved test coverage.
<br>Continuous Integration (CI) where the artifacts are automatically created and tested. This aids in identification of issues in the product at an early stage of development.
<br>At the time of what is Jenkins blog, it had close to 1500+ plugins contributed by the community. Plugins help in customizing the experience with Jenkins, along with providing support for accelerating activities related to building, deploying, and automating a project.<br><br>Jenkins has an early mover advantage since it has been in development since 2011. Kohsuke Kawaguchi created Jenkins (then called ‚ÄòHudson‚Äô) while working at Sun Microsystems. Hudson was created in the summer of 2004 and the first release was in February 2005.<br>After the acquisition of Sun Microsystems by Oracle, a proposal was approved by the Hudson community for creating the Jenkins project. In February 2011, Oracle intended that the development of Hudson should continue hence, Hudson was forked instead of renaming it to Jenkins.<br>Though Hudson and Jenkins were being developed independently, Jenkins acquired significantly more projects &amp; contributors than Hudson. Consequently, Hudson is no longer maintained by the community.<br><br>Like other open-source projects, Jenkins also produces two release lines ‚Äì LTS (Long-Term Support) and Weekly (regular) releases. Jenkins is very good with releases, as stable releases happen every four weeks.<br><br>Jenkins is a self-contained Java program that is agnostic of the platform on which it is installed. It is available for almost all the popular operating systems such as Windows, different flavors of Unix, and Mac OS.<br>It is available as a normal installer, as well as a .war file. Once installed, it is easy to configure using its web interface.<br><br>As it is open-source, it is free for use. There is a strong involvement of the community which makes it a powerful CI/CD tool. You can take support from the Jenkins community, whether it is for extensibility, support, documentation, or any other feature related to Jenkins.<br><br>The backbone of Jenkins is the community and the community members have been instrumental in the development (and testing) of close to 1500+ plugins available in the Update Center.<br><br>Jenkins is designed in such a manner that makes it relatively easy to distribute work across multiple machines and platforms for the accelerated build, testing, and deployment.<br><br>In this section of the What is Jenkins blog, we look at the internal functioning of Jenkins i.e. what happens once the developer commits changes to the repository and how CI/CD is realized in Jenkins. We also look at the Master-Agent architecture in Jenkins.<br><br>Before we dive into how does Jenkins works, we must understand the architecture of Jenkins. These are the series of steps that outlines the interaction between different elements in Jenkins:<br>
<br>Developers do the necessary modifications in the source code and commit the changes to the repository. A new version of that file will be created in the version control system that is used for maintaining the repository of source code.
<br>The repository is continuously checked by the Jenkins CI server for any changes (either in the form of code or libraries) and changes are pulled by the server.
<br>In the next step, we ensure that the build with the ‚Äòpulled changes‚Äô is going through or not. The Build server performs a build with the code and an executable is generated if the build process is successful. In case of a build failure, an automated email with a link to build logs and other build artifacts is sent to the developer.
<br>In case of a successful build, the built application (or executable) is deployed to the test server. This step helps in realizing continuous testing where the newly built executable goes through a series of automated tests. Developers are alerted in case the changes have caused any breakage in functionality.
<br>If there are no build, integration, and testing issues with the checked-in code, the changes and tested application are automatically deployed to the Prod/Production server.
<br>Here is the diagrammatic representation of the Jenkins architecture:<br><img alt="Jenkins architecture" src="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Jenkins-architecture-1.png" referrerpolicy="no-referrer"><br>A single Jenkins server might not be sufficient to realize the following requirements:<br>
<br>Testing needs to be performed on different environments (i.e. code written using different languages e.g. Java, Python, C, etc. are committed to the version control system), where a single server might not suffice the requirement.
<br>A single Jenkins server might not be sufficient to handle the load that comes with large-scale software projects.
<br>In such scenarios, the distributed (or Master-Agent) architecture of Jenkins is used for continuous integration and testing. Diving deeper into how does Jenkins works, we take a look at the architecture of Jenkins.<br><br>The master-agent (or distributed) architecture in Jenkins is used for managing distributed builds. The Master and Agent(s) communicate through the TCP/IP protocol.<br>These are the roles and responsibilities of the Jenkins Master and Agent(s):<br><br>The main server in Jenkins is the Master. Here are the jobs handled by Jenkins Master:<br>
<br>Schedule build jobs
<br>Choosing the appropriate agent in the master-agent ecosystem for dispatching the builds.
<br>Monitor agents and take them online/offline as and when required.
<br>Presenting the build results (and reports) to the developer.
<br>The Jenkins master can also execute the jobs directly but it is always recommended to select the appropriate agent(s) for build and execution-related tasks.<br><br>A agent is a remote machine that is connected to the Master. Depending on the project and build requirements, you could opt for ‚ÄòN‚Äô number of agents. agents can run on different operating systems and depending on the ‚Äòtype of build request‚Äô, the appropriate Agent is chosen by the Master for build execution and testing.<br>Here are the jobs handled by the Jenkins Agent(s):<br>
<br>Listen to commands from the Jenkins Master.
<br>Execute build jobs that are dispatched by the Master.
<br>Developers have the flexibility to run the build and execute tests on a particular agent or a particular type of Agent. The default option is Jenkins Master selecting the best-suited Agent for the job.
<br>Here is a simple diagrammatic representation of how does Jenkins work, with multiple Jenkins Agents connected to the Jenkins Master:<br><img src="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Agent_Master.jpg" referrerpolicy="no-referrer"><br><br>In the previous section of the What is Jenkins blog, we touched upon the brief responsibilities of Master and Agent(s) in Jenkins. Let‚Äôs look at exactly how does Jenkins works in the Master-Agent (or distributed) architecture:<br>In the Jenkins Master-Agent architecture shown below, there are three Agents, each running on a different operating system (i.e. Windows 10, Linux, and Mac OS).<br><img src="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Jenkins-Architecture.jpg" referrerpolicy="no-referrer"><br>
<br>Developers check-in their respective code changes in ‚ÄòThe Remote Source Code Repository‚Äô that is depicted on the left-hand side.
<br>Only the Jenkins master is connected to the repository and it checks for code-changes (in the repository) at periodic intervals. All the Jenkins Agents are connected to the Jenkins Master.
<br>Jenkins master dispatches the request (for build and test) to the appropriate Jenkins Agent depending on the environment required for performing the build. This lets you perform builds and execute tests in different environments across the entire architecture.
<br>The Agent performs the testing, generates test reports, and sends the same to the Jenkins Master for monitoring.
<br>As developers keep pushing code, Jenkins Agents can run different builds versions of the code for different platforms. Jenkins Master (or Master Node) controls how the respective builds should operate.<br>In subsequent sections of the What is Jenkins blog, we would look at the steps for setting up Jenkins Master and Agents.]]></description><link>https://notes.sarangwandile.xyz/jenkins/jenkins-first-look.html</link><guid isPermaLink="false">Jenkins/Jenkins First Look.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Jenkins-Logo.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.lambdatest.com/blog/wp-content/uploads/2020/09/Jenkins-Logo.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Introduction]]></title><description><![CDATA[ 
 <br><img alt="sonar" src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*KOadiTidZoHtrfanE3Ck5A.png" referrerpolicy="no-referrer"><br>Sonar provides a comprehensive code quality and security analysis solution to scan your IaC files in your managed cloud environments to review a wide range of possible issues or security vulnerabilities.<br><br>SonarQube is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect <a data-tooltip-position="top" aria-label="https://www.techopedia.com/definition/3758/bug" rel="noopener nofollow" class="external-link" href="https://www.techopedia.com/definition/3758/bug" target="_blank">bugs</a>, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Code_smell" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Code_smell" target="_blank">code smells</a>, and <a data-tooltip-position="top" aria-label="https://owasp.org/www-community/vulnerabilities/" rel="noopener nofollow" class="external-link" href="https://owasp.org/www-community/vulnerabilities/" target="_blank">security vulnerabilities</a> on 20+ programming languages.<br>
It can report duplicated code, coding standards, unit tests, code coverage, code complexity and comments.
<br>
The only prerequisite for running SonarQube is to have Java (<a data-tooltip-position="top" aria-label="https://www.oracle.com/java/technologies/javase-jre8-downloads.html" rel="noopener nofollow" class="external-link" href="https://www.oracle.com/java/technologies/javase-jre8-downloads.html" target="_blank">Oracle JRE 11</a> or <a data-tooltip-position="top" aria-label="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html" rel="noopener nofollow" class="external-link" href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html" target="_blank">OpenJDK 11</a>) installed on your machine.
<br><br>Using docker run sonarqube container<br>docker run -d --name sonarqube -p 9000:9000 sonarqube
<br>In your web browser, connect to <a rel="noopener nofollow" class="external-link" href="http://localhost:9000" target="_blank">http://localhost:9000</a> to access the SonarQube web interface.<br>Use the following default credentials to log in:<br>Username: admin  
Password: admin
<br>You'll probably prompted to change the default password<br>
<br>
To run sonar scan against our locally hosted code select manually option

<br>
Enter the&nbsp;token name and click the&nbsp;"Generate" button. You will get the sonar token.

<br>
Save that token and Click "Continue".

<br>
Select Maven and copy the code

<br>and open terminal and navigate to your locally hosted code's directory<br>
and run that command<br>mvn clean verify sonar:sonar \
  -Dsonar.projectKey=student \
  -Dsonar.projectName='student' \
  -Dsonar.host.url=http://3.137.177.165:32768 \
  -Dsonar.token=sqp_3584391716cf8fbc01eeea4e17d34b35b07d5a2f
<br>and then check sonarqube dashboard for code analysis report.]]></description><link>https://notes.sarangwandile.xyz/jenkins/sonarqube-with-maven.html</link><guid isPermaLink="false">Jenkins/SonarQube with maven.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:720/format:webp/1*KOadiTidZoHtrfanE3Ck5A.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*KOadiTidZoHtrfanE3Ck5A.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[EC2 Creation]]></title><description><![CDATA[ 
 <br>Terraform is IAC i.e. Infrastructure as Code which leverages code to create Infrastructure resource in any of the cloud provider that support terraform.<br>We create two files main.tf and provider.tf<br>
in main.tf we specify the resources with their configuration to create and in provider.tf we declare information about the cloud provider at we are creating resources.<br>Here is the simple Terraform code to create aws_instance resource in aws platform.<br>File Name: main.tf<br>resource "aws_instance" "a" {
  ami           = "ami-0c614dee691cbbf37"
  instance_type = "t2.micro"
  key_name = "my-key"

  tags = {
    Name = "HelloWorld"
  }
}
<br>File Name: provider.tf<br>terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.80.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}
<br>To apply the execution plan we run<br>
terraform plan This shows us detailed view of what changes going to take place.<br>
and with terraform apply we make these changes.]]></description><link>https://notes.sarangwandile.xyz/terraform/ec2-creation.html</link><guid isPermaLink="false">Terraform/EC2 Creation.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 16:56:58 GMT</pubDate></item><item><title><![CDATA[HCL Basics]]></title><description><![CDATA[ 
 <br>HCL Syntax:<br>
<br>consists of a block and argument
<br>&lt;block&gt; &lt;parameter&gt; {
	key1 = value1
	key2 = value2
}
<br>A block is defined within curly braces and it contains a set of arguments in key value pair format representing the configuration detail.<br>A block in terraforms contains information about infrastructure platform and set of resources withing that platform we want to create.<br>For example to create a local file we will use local_file resource and we'll create local.tf file cotaining following configuration:<br>resource "local_file" "pet" {
	filename = "/root/pets.txt"
	content = "We love pets!"
}
<br><img alt="tf-resource-block.png" src="https://notes.sarangwandile.xyz/lib/media/tf-resource-block.png"><br>
Here is resource is a block name (keyword) which shows the type of block we are using.<br>
after the resource declaration we have "resource type" called local_file its fixed value.<br>
Here "local" is a provider and "file" follwing the underscore _ shows the type of resource which is file<br>
Next is a Resource name pet which can be anything.<br>
Next we have Arguments specific to the resource type in a key value pair format. In this case for local_file we have filename and content which are self explanatory.]]></description><link>https://notes.sarangwandile.xyz/terraform/hcl-basics.html</link><guid isPermaLink="false">Terraform/HCL Basics.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/tf-resource-block.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/tf-resource-block.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[S3 bucket Creation]]></title><description><![CDATA[ 
 <br>There are two methods to do this:<br>
<br>Manually giving buckets names for each buckets
<br>Using Increment in Variable to add numbers after specific Unique name
<br><br>Filename: Main.tf<br>resource "aws_s3_bucket" "my-known-buckets" {
  count     = length(var.bucket_names) 
  bucket    = var.bucket_names[count.index]
}
<br>Filename: vars.tf<br>variable "bucket_names" {
  description = "List of S3 bucket names"
  type        = list(string)
  default     = ["chandratsdfsdasfdfaara", "nayantara123qq3112", "rocketshopzxys", "123asdxzsdgheoi", "asdf21alkj44dsfj"]
}
<br>Filename: provider.tf<br>terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.80.0"
    }
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}
<br>This will create 5 buckets with the given names ["chandratsdfsdasfdfaara", "nayantara123qq3112", "rocketshopzxys", "123asdxzsdgheoi", "asdf21alkj44dsfj"]<br>Apply it using<br>
terraform plan<br>
terraform apply<br>You can list s3 buckets with aws cli commands<br>aws s3 ls
<br><br>Use this method when you have too many buckets to create and naming doesnt really matters<br>
then this will save your time as you only need to change bucket count in vars.tf file<br>Filename: main.tf<br>resource "aws_s3_bucket" "c" {
  count     = var.bucket_count
  bucket    = "wingardiumleviosamybucketsintheair-${count.index + 1}"
}
<br>Filename: vars.tf<br>variable "bucket_count" {
	description = "number of s3 buckets to create"
	type = number
	default = 5
}
]]></description><link>https://notes.sarangwandile.xyz/terraform/s3-bucket-creation.html</link><guid isPermaLink="false">Terraform/S3 bucket Creation.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[Terraform Commands]]></title><description><![CDATA[ 
 <br><br><br><br>]]></description><link>https://notes.sarangwandile.xyz/terraform/terraform-commands.html</link><guid isPermaLink="false">Terraform/Terraform Commands.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 17 Feb 2025 01:37:00 GMT</pubDate></item><item><title><![CDATA[VPC Creation]]></title><description><![CDATA[ 
 <br>element()<br>
length()<br>main.tf<br># vpc

resource "aws_vpc" "main" {
  cidr_block       = var.vpc_cidr

  tags = {
    Name = "${var.tag}-vpc"
  }
}

# public subnet

resource "aws_subnet" "main" {
  count = length(var.public_subnet_cidr)
  vpc_id     = aws_vpc.main.id
  cidr_block = element( var.public_subnet_cidr, count.index )
  tags = {
    Name = "${var.tag}-public-subnet-${element( var.public_subnet_cidr, count.index )}"
  }
}

## private subnet
resource "aws_subnet" "private" {
    count = length(var.private_subnet_cidr)
  vpc_id     = aws_vpc.main.id
  cidr_block = element(var.private_subnet_cidr, count.index)

  tags = {
    Name = "${var.tag}-private-subnet-${element(var.private_subnet_cidr, count.index)}"
  }
}

# public rt

resource "aws_route_table" "main" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }


  tags = {
    Name = "${var.tag}-public-route"
  }
}


# private rt

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.nat.id
  }


  tags = {
    Name = "${var.tag}-private-route"
  }
}
# eip
resource "aws_eip" "nat" {
  domain   = "vpc"
} 

#nat 
resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat.id
  subnet_id     = aws_subnet.main[0].id

  tags = {
    Name = "${var.tag}-nat"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.gw]
}

# igw 
resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "${var.tag}-igw"
  }
}

# subnet association

resource "aws_route_table_association" "a" {
  count = length(var.public_subnet_cidr)
  subnet_id = aws_subnet.main[count.index].id
  route_table_id = aws_route_table.main.id
}

# private subnet association

resource "aws_route_table_association" "b" {
  count = length(var.private_subnet_cidr)
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}
<br>Provider.tf<br>terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.85.0"
    }
  }
}

provider "aws" {
  # Configuration options
  region = "us-east-2"

}
<br>var.tf<br>variable "vpc_cidr" {
  type = string
  default = "10.0.0.0/16"
  description = "vpc cidr "
}

variable "tag" {
  type = string
  default = "cdec"
}

variable "public_subnet_cidr" {
  default = ["10.0.1.0/24", "10.0.2.0/24"]
}

variable "private_subnet_cidr" {
  default = ["10.0.3.0/24", "10.0.4.0/24", "10.0.5.0/24","10.0.6.0/24"]
}
]]></description><link>https://notes.sarangwandile.xyz/terraform/vpc-creation.html</link><guid isPermaLink="false">Terraform/VPC Creation.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate></item><item><title><![CDATA[My DevOps Notes]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br><br>Author: Sarang Wandile<br>
Github: <a data-tooltip-position="top" aria-label="https://github.com/srngx" rel="noopener nofollow" class="external-link" href="https://github.com/srngx" target="_blank">srngx</a><br>Hello, My name is Sarang, in this place I store my DevOps Notes, assignments and Practical. Its not much but I like to keep track of everything I learned. <br>This Place also acts like a Second brain kinda thing where I can always go back in time and recollect my old thoughts and Ideas. Its not perfect and thats the beautify of it. I like to keep it this way original and imperfect. <br>In this Digital Garden you are always welcome to explore and share your notes too if you want or <a data-tooltip-position="top" aria-label="https://github.com/srngx/my-devops-notes" rel="noopener nofollow" class="external-link" href="https://github.com/srngx/my-devops-notes" target="_blank">contribute/give suggestion</a> anytime.<br>These notes are made using <a data-tooltip-position="top" aria-label="https://obsidian.md" rel="noopener nofollow" class="external-link" href="https://obsidian.md" target="_blank">Obsidian App</a> - A Markdown Notetaking App and Hosted with <a data-tooltip-position="top" aria-label="https://pages.cloudflare.com/" rel="noopener nofollow" class="external-link" href="https://pages.cloudflare.com/" target="_blank">Cloudflare page</a>.<br>Recent Notes

<br><a data-href="aws-cli cheatsheet" href="https://notes.sarangwandile.xyz/aws/notes/aws-cli-cheatsheet.html" class="internal-link" target="_self" rel="noopener nofollow">aws-cli cheatsheet</a>
<br><a data-href="SonarQube with maven" href="https://notes.sarangwandile.xyz/jenkins/sonarqube-with-maven.html" class="internal-link" target="_self" rel="noopener nofollow">SonarQube with maven</a>
<br><a data-href="HCL Basics" href="https://notes.sarangwandile.xyz/terraform/hcl-basics.html" class="internal-link" target="_self" rel="noopener nofollow">HCL Basics</a>
<br><a data-href="Jenkins First Look" href="https://notes.sarangwandile.xyz/jenkins/jenkins-first-look.html" class="internal-link" target="_self" rel="noopener nofollow">Jenkins First Look</a>

<br><br><br><br>Tasks to perform
<a data-href="‚òëÔ∏è Implement autoscaling on memory utilization" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-implement-autoscaling-on-memory-utilization.html" class="internal-link" target="_self" rel="noopener nofollow">‚òëÔ∏è Implement autoscaling on memory utilization</a><br>
<a data-href="‚úÖ Host static website on s3 bucket" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-host-static-website-on-s3-bucket.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Host static website on s3 bucket</a><br>
<a data-href="‚úÖ Create 5 IAM users and 5 S3 buckets and attach them each other" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-create-5-iam-users-and-5-s3-buckets-and-attach-them-each-other.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Create 5 IAM users and 5 S3 buckets and attach them each other</a><br>
<a data-href="‚úÖ Implement Template with Scheduled Autoscaling" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-implement-template-with-scheduled-autoscaling.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Implement Template with Scheduled Autoscaling</a><br>
<a data-href="‚úÖ Monitoring Nginx logs with Cloudwatch" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-monitoring-nginx-logs-with-cloudwatch.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Monitoring Nginx logs with Cloudwatch</a><br>
<a data-href="‚òëÔ∏è How to store aws load balancer logs in s3 bucket" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-how-to-store-aws-load-balancer-logs-in-s3-bucket.html" class="internal-link" target="_self" rel="noopener nofollow">‚òëÔ∏è How to store aws load balancer logs in s3 bucket</a><br>
<a data-href="‚úÖüêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖüêà‚Äç‚¨õhost-tomcat-basesd-web-app-called-student-app-with-rds-and-ec2.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖüêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2</a><br>
<a data-href="‚úÖ Get sns alert when any user launches new instance" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-get-sns-alert-when-any-user-launches-new-instance.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Get sns alert when any user launches new instance</a><br>
<a data-href="‚úÖ Create notification for s3 bucket activity happen of put and delete" href="https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-create-notification-for-s3-bucket-activity-happen-of-put-and-delete.html" class="internal-link" target="_self" rel="noopener nofollow">‚úÖ Create notification for s3 bucket activity happen of put and delete</a>
<br>Task assigned for Reading

<br><a data-href="Agile Development" href="https://notes.sarangwandile.xyz/aws/for-reading/agile-development.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Development</a>
<br><a data-href="Agile Vs DevOps" href="https://notes.sarangwandile.xyz/aws/for-reading/agile-vs-devops.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Vs DevOps</a>
<br><a data-href="Amazon Web Services In Plain English" href="https://notes.sarangwandile.xyz/aws/notes/amazon-web-services-in-plain-english.html" class="internal-link" target="_self" rel="noopener nofollow">Amazon Web Services In Plain English</a>
<br><a data-href="Authorized vs Authonticated" href="https://notes.sarangwandile.xyz/aws/for-reading/authorized-vs-authonticated.html" class="internal-link" target="_self" rel="noopener nofollow">Authorized vs Authonticated</a>
<br><a data-href="AWS Instance Types" href="https://notes.sarangwandile.xyz/aws/for-reading/aws-instance-types.html" class="internal-link" target="_self" rel="noopener nofollow">AWS Instance Types</a>
<br><a data-href="Difference between Load balancers" href="https://notes.sarangwandile.xyz/aws/for-reading/difference-between-load-balancers.html" class="internal-link" target="_self" rel="noopener nofollow">Difference between Load balancers</a>
<br><a data-href="Git Fetch vs Git pull" href="https://notes.sarangwandile.xyz/aws/for-reading/git-fetch-vs-git-pull.html" class="internal-link" target="_self" rel="noopener nofollow">Git Fetch vs Git pull</a>
<br><a data-href="How DNS Works" href="https://notes.sarangwandile.xyz/aws/for-reading/how-dns-works.html" class="internal-link" target="_self" rel="noopener nofollow">How DNS Works</a>
<br><a data-href="IAM Policies" href="https://notes.sarangwandile.xyz/aws/for-reading/iam-policies.html" class="internal-link" target="_self" rel="noopener nofollow">IAM Policies</a>
<br><a data-href="Load Balancer" href="https://notes.sarangwandile.xyz/aws/for-reading/load-balancer.html" class="internal-link" target="_self" rel="noopener nofollow">Load Balancer</a>
<br><a data-href="Managed policies and inline policies" href="https://notes.sarangwandile.xyz/aws/for-reading/managed-policies-and-inline-policies.html" class="internal-link" target="_self" rel="noopener nofollow">Managed policies and inline policies</a>
<br><a data-href="Network Protocols" href="https://notes.sarangwandile.xyz/aws/for-reading/network-protocols.html" class="internal-link" target="_self" rel="noopener nofollow">Network Protocols</a>
<br><a data-href="OSI Model" href="https://notes.sarangwandile.xyz/aws/for-reading/osi-model.html" class="internal-link" target="_self" rel="noopener nofollow">OSI Model</a>
<br><a data-href="SDLC - Software Developement LifeCycle" href="https://notes.sarangwandile.xyz/aws/for-reading/sdlc-software-developement-lifecycle.html" class="internal-link" target="_self" rel="noopener nofollow">SDLC - Software Developement LifeCycle</a>
<br><a data-href="Simple Definitions To give in Interview" href="https://notes.sarangwandile.xyz/aws/notes/simple-definitions-to-give-in-interview.html" class="internal-link" target="_self" rel="noopener nofollow">Simple Definitions To give in Interview</a>
<br><a data-href="Some talks about git in reddit" href="https://notes.sarangwandile.xyz/aws/for-reading/some-talks-about-git-in-reddit.html" class="internal-link" target="_self" rel="noopener nofollow">Some talks about git in reddit</a>
<br><a data-href="ssl certificate" href="https://notes.sarangwandile.xyz/aws/for-reading/ssl-certificate.html" class="internal-link" target="_self" rel="noopener nofollow">ssl certificate</a>
<br><a data-href="Storage Classes in S3" href="https://notes.sarangwandile.xyz/aws/for-reading/storage-classes-in-s3.html" class="internal-link" target="_self" rel="noopener nofollow">Storage Classes in S3</a>
<br><a data-href="Subnetting" href="https://notes.sarangwandile.xyz/aws/for-reading/subnetting.html" class="internal-link" target="_self" rel="noopener nofollow">Subnetting</a>
<br><a data-href="Types of Autoscaling" href="https://notes.sarangwandile.xyz/aws/for-reading/types-of-autoscaling.html" class="internal-link" target="_self" rel="noopener nofollow">Types of Autoscaling</a>

<br><br><br><br>Tasks to perform

<br><a data-href="Task 1 Creating Tomcat student-ui container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-1-creating-tomcat-student-ui-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 1 Creating Tomcat student-ui container</a>
<br><a data-href="Task 2 Create the Free-css template container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-2-create-the-free-css-template-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 2 Create the Free-css template container</a>
<br><a data-href="Task 3 Create Mysql Container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 3 Create Mysql Container</a>
<br><a data-href="Task 4 Create daemon service of tomcat" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html" class="internal-link" target="_self" rel="noopener nofollow">Task 4 Create daemon service of tomcat</a>
<br><a data-href="Task 5 Create dockerfiles" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-5-create-dockerfiles.html" class="internal-link" target="_self" rel="noopener nofollow">Task 5 Create dockerfiles</a>
<br><a data-href="Task 6 How to set permanent alias" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-6-how-to-set-permanent-alias.html" class="internal-link" target="_self" rel="noopener nofollow">Task 6 How to set permanent alias</a>
<br>Kubernetes <a data-href="2025-01-22 3-Microservice Tasks" href="https://notes.sarangwandile.xyz/daily-notes/2025-01-22-3-microservice-tasks.html" class="internal-link" target="_self" rel="noopener nofollow">2025-01-22 3-Microservice Tasks</a>
<br><a data-href="K8s Task 1" href="https://notes.sarangwandile.xyz/devops/k8s-tasks/k8s-task-1.html" class="internal-link" target="_self" rel="noopener nofollow">K8s Task 1</a>
<br><a data-href="3 Microservices Task" href="https://notes.sarangwandile.xyz/devops/k8s-tasks/3-microservices-task.html" class="internal-link" target="_self" rel="noopener nofollow">3 Microservices Task</a>
<br><a data-href="Tomcat on single node" href="https://notes.sarangwandile.xyz/devops/k8s-tasks/tomcat-on-single-node.html" class="internal-link" target="_self" rel="noopener nofollow">Tomcat on single node</a>

<br>Task assigned for Reading

<br><a data-href="Agile Development" href="https://notes.sarangwandile.xyz/aws/for-reading/agile-development.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Development</a>
<br><a data-href="Agile Vs DevOps" href="https://notes.sarangwandile.xyz/aws/for-reading/agile-vs-devops.html" class="internal-link" target="_self" rel="noopener nofollow">Agile Vs DevOps</a>
<br><a data-href="Git Fetch vs Git pull" href="https://notes.sarangwandile.xyz/aws/for-reading/git-fetch-vs-git-pull.html" class="internal-link" target="_self" rel="noopener nofollow">Git Fetch vs Git pull</a>
<br><a data-href="Monolithic vs Microservice Architecture" href="https://notes.sarangwandile.xyz/devops/for-reading/monolithic-vs-microservice-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Monolithic vs Microservice Architecture</a>
<br><a data-href="Git Theory and Practical" href="https://notes.sarangwandile.xyz/devops/notes/git-theory-and-practical.html" class="internal-link" target="_self" rel="noopener nofollow">Git Theory and Practical</a> 
<br><a data-href="Reading Docker Inspect output" href="https://notes.sarangwandile.xyz/devops/for-reading/reading-docker-inspect-output.html" class="internal-link" target="_self" rel="noopener nofollow">Reading Docker Inspect output</a>
<br><a data-href="read about docker.sock file" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html" class="internal-link" target="_self" rel="noopener nofollow">read about docker.sock file</a>
<br><a data-href="Kubernetes Architecture" href="https://notes.sarangwandile.xyz/devops/notes/kubernetes-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Kubernetes Architecture</a>
<br><a data-href="Read about each and every component of kubernetes" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-each-and-every-component-of-kubernetes.html" class="internal-link" target="_self" rel="noopener nofollow">Read about each and every component of kubernetes</a>
<br><a data-href="K8s Service Types" href="https://notes.sarangwandile.xyz/devops/for-reading/k8s-service-types.html" class="internal-link" target="_self" rel="noopener nofollow">K8s Service Types</a>
<br><a data-href="k8s Namespace" href="https://notes.sarangwandile.xyz/devops/for-reading/k8s-namespace.html" class="internal-link" target="_self" rel="noopener nofollow">k8s Namespace</a>
<br><a data-href="k8s Volume" href="https://notes.sarangwandile.xyz/devops/for-reading/k8s-volume.html" class="internal-link" target="_self" rel="noopener nofollow">k8s Volume</a>

<br><br><br>Links in this website are subject to change.
These notes are not complete and subject to grow, modify and update in time to time, so its very likely that your shared page specific links might not work as that page itself either moved to different folder or replaced or renamed.
<br><br>Powered by <a href="https://obsidian.md" style="color: #7c3aed; text-decoration: none; margin-left: 0.25rem;" rel="noopener noreferrer" target="_blank">Obsidian</a>]]></description><link>https://notes.sarangwandile.xyz/index.html</link><guid isPermaLink="false">Index.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 16 Feb 2025 14:47:10 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/images/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Mysql APP From Scratch]]></title><description><![CDATA[ 
 <br><br># Update system packages
sudo yum update -y

# Install Apache web server
sudo yum install -y httpd

# Install PHP and MySQL
sudo amazon-linux-extras enable php7.4
sudo yum clean metadata
sudo yum install -y php php-devel php-mysqlnd httpd mysql-server

# Start and enable services
sudo systemctl start httpd
sudo systemctl enable httpd
sudo systemctl start mysqld
sudo systemctl enable mysqld

# Secure MySQL installation
sudo mysql_secure_installation
<br><br>-- Create database
CREATE DATABASE user_registration;
USE user_registration;

-- Create users table
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    student_name VARCHAR(100),
    address VARCHAR(255),
    email VARCHAR(100) UNIQUE,
    username VARCHAR(50) UNIQUE,
    password VARCHAR(255)
);
<br><br>signin.html<br>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;title&gt;Sign In&lt;/title&gt;
    &lt;style&gt;
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f4f4f4;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            width: 300px;
        }
        input {
            width: 100%;
            padding: 10px;
            margin: 10px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            width: 100%;
            padding: 10px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        .signup-link {
            text-align: center;
            margin-top: 15px;
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="container"&gt;
        &lt;h2&gt;Sign In&lt;/h2&gt;
        &lt;form action="login.php" method="post"&gt;
            &lt;input type="text" name="username" placeholder="Username" required&gt;
            &lt;input type="password" name="password" placeholder="Password" required&gt;
            &lt;button type="submit"&gt;Sign In&lt;/button&gt;
        &lt;/form&gt;
        &lt;div class="signup-link"&gt;
            &lt;p&gt;Don't have an account? &lt;a href="signup.html"&gt;Sign Up&lt;/a&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
<br><br>signup.html<br>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;title&gt;Sign Up&lt;/title&gt;
    &lt;style&gt;
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f4f4f4;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            width: 300px;
        }
        input {
            width: 100%;
            padding: 10px;
            margin: 10px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            width: 100%;
            padding: 10px;
            background-color: #28a745;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="container"&gt;
        &lt;h2&gt;Sign Up&lt;/h2&gt;
        &lt;form action="register.php" method="post"&gt;
            &lt;input type="text" name="student_name" placeholder="Student Name" required&gt;
            &lt;input type="text" name="address" placeholder="Address" required&gt;
            &lt;input type="email" name="email" placeholder="Email" required&gt;
            &lt;input type="text" name="username" placeholder="Username" required&gt;
            &lt;input type="password" name="password" placeholder="Password" required&gt;
            &lt;button type="submit"&gt;Register&lt;/button&gt;
        &lt;/form&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
<br><br>register.php<br>&lt;?php
session_start();
$host = 'localhost';
$db_username = 'root';  // Change this to your MySQL username
$db_password = 'your_mysql_password';  // Change to your MySQL password
$database = 'user_registration';

// Create connection
$conn = new mysqli($host, $db_username, $db_password, $database);

// Check connection
if ($conn-&gt;connect_error) {
    die("Connection failed: " . $conn-&gt;connect_error);
}

if ($_SERVER["REQUEST_METHOD"] == "POST") {
    $student_name = $conn-&gt;real_escape_string($_POST['student_name']);
    $address = $conn-&gt;real_escape_string($_POST['address']);
    $email = $conn-&gt;real_escape_string($_POST['email']);
    $username = $conn-&gt;real_escape_string($_POST['username']);
    $password = password_hash($_POST['password'], PASSWORD_BCRYPT);

    // Check if username or email already exists
    $check_query = "SELECT * FROM users WHERE username = ? OR email = ?";
    $stmt = $conn-&gt;prepare($check_query);
    $stmt-&gt;bind_param("ss", $username, $email);
    $stmt-&gt;execute();
    $result = $stmt-&gt;get_result();

    if ($result-&gt;num_rows &gt; 0) {
        echo "Username or email already exists!";
    } else {
        // Insert new user
        $sql = "INSERT INTO users (student_name, address, email, username, password) VALUES (?, ?, ?, ?, ?)";
        $stmt = $conn-&gt;prepare($sql);
        $stmt-&gt;bind_param("sssss", $student_name, $address, $email, $username, $password);

        if ($stmt-&gt;execute()) {
            $_SESSION['username'] = $username;
            header("Location: profile.php");
        } else {
            echo "Error: " . $stmt-&gt;error;
        }
    }
    $stmt-&gt;close();
}
$conn-&gt;close();
?&gt;
<br><br>Login.php<br>&lt;?php
session_start();
$host = 'localhost';
$db_username = 'root';
$db_password = 'your_mysql_password';
$database = 'user_registration';

$conn = new mysqli($host, $db_username, $db_password, $database);

if ($conn-&gt;connect_error) {
    die("Connection failed: " . $conn-&gt;connect_error);
}

if ($_SERVER["REQUEST_METHOD"] == "POST") {
    $username = $conn-&gt;real_escape_string($_POST['username']);
    $password = $_POST['password'];

    $sql = "SELECT * FROM users WHERE username = ?";
    $stmt = $conn-&gt;prepare($sql);
    $stmt-&gt;bind_param("s", $username);
    $stmt-&gt;execute();
    $result = $stmt-&gt;get_result();

    if ($result-&gt;num_rows == 1) {
        $user = $result-&gt;fetch_assoc();
        if (password_verify($password, $user['password'])) {
            $_SESSION['username'] = $username;
            header("Location: profile.php");
        } else {
            echo "Incorrect password!";
        }
    } else {
        echo "Account not registered!";
    }
    $stmt-&gt;close();
}
$conn-&gt;close();
?&gt;
<br><br>Profile.php<br>&lt;?php
session_start();
if (!isset($_SESSION['username'])) {
    header("Location: signin.html");
    exit();
}

$host = 'localhost';
$db_username = 'root';
$db_password = 'your_mysql_password';
$database = 'user_registration';

$conn = new mysqli($host, $db_username, $db_password, $database);

$username = $_SESSION['username'];
$sql = "SELECT * FROM users WHERE username = ?";
$stmt = $conn-&gt;prepare($sql);
$stmt-&gt;bind_param("s", $username);
$stmt-&gt;execute();
$result = $stmt-&gt;get_result();
$user = $result-&gt;fetch_assoc();
?&gt;

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Profile&lt;/title&gt;
    &lt;style&gt;
        body { font-family: Arial, sans-serif; max-width: 400px; margin: 0 auto; padding: 20px; }
        .profile-section { background: #f4f4f4; padding: 20px; border-radius: 5px; }
        input { width: 100%; margin: 10px 0; padding: 5px; }
        button { margin-top: 10px; padding: 10px; }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="profile-section"&gt;
        &lt;h2&gt;Profile&lt;/h2&gt;
        &lt;form action="update_profile.php" method="post"&gt;
            &lt;label&gt;Name: &lt;input type="text" name="student_name" value="&lt;?php echo $user['student_name']; ?&gt;"&gt;&lt;/label&gt;
            &lt;label&gt;Email: &lt;input type="email" name="email" value="&lt;?php echo $user['email']; ?&gt;"&gt;&lt;/label&gt;
            &lt;label&gt;New Password: &lt;input type="password" name="new_password"&gt;&lt;/label&gt;
            &lt;button type="submit"&gt;Update Profile&lt;/button&gt;
        &lt;/form&gt;
        &lt;a href="logout.php"&gt;Logout&lt;/a&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
<br><br>update_profile.php<br>&lt;?php
session_start();
if (!isset($_SESSION['username'])) {
    header("Location: signin.html");
    exit();
}

$host = 'localhost';
$db_username = 'root';
$db_password = 'your_mysql_password';
$database = 'user_registration';

$conn = new mysqli($host, $db_username, $db_password, $database);

$username = $_SESSION['username'];
$student_name = $conn-&gt;real_escape_string($_POST['student_name']);
$email = $conn-&gt;real_escape_string($_POST['email']);

$update_sql = "UPDATE users SET student_name = ?, email = ?";
$params = [$student_name, $email];
$types = "ss";

if (!empty($_POST['new_password'])) {
    $new_password = password_hash($_POST['new_password'], PASSWORD_BCRYPT);
    $update_sql .= ", password = ?";
    $params[] = $new_password;
    $types .= "s";
}

$update_sql .= " WHERE username = ?";
$params[] = $username;
$types .= "s";

$stmt = $conn-&gt;prepare($update_sql);
$stmt-&gt;bind_param($types, ...$params);

if ($stmt-&gt;execute()) {
    header("Location: profile.php");
} else {
    echo "Error updating profile: " . $stmt-&gt;error;
}

$stmt-&gt;close();
$conn-&gt;close();
?&gt;
<br><br>logout.php<br>&lt;?php
session_start();
session_destroy();
header("Location: signin.html");
exit();
?&gt;
<br>Deployment Steps:<br>
<br>Install all these files in /var/www/html/ directory
<br>Set proper file permissions: sudo chmod 755 /var/www/html/*
<br>Restart Apache: sudo systemctl restart httpd
<br>Security Notes:<br>
<br>Replace your_mysql_password with a strong password
<br>Configure firewall to allow HTTP/HTTPS traffic
<br>Use HTTPS in production
<br>Implement additional input validation
<br>Requirements Covered:<br>
<br>MySQL installed on EC2
<br>Simple registration and login system
<br>Profile update functionality
<br>Basic error handling
<br>Password hashing for security
<br><br><br>FROM php:7.4-apache

# Install system dependencies and PHP extensions
RUN docker-php-ext-install mysqli pdo pdo_mysql

# Copy application files
COPY . /var/www/html/

# Set proper permissions
RUN chown -R www-data:www-data /var/www/html \
    &amp;&amp; chmod -R 755 /var/www/html

# Enable Apache mod_rewrite
RUN a2enmod rewrite

# Expose port 80
EXPOSE 80

<br>docker build -t your-dockerhub-username/php-user-registration:v1 .
docker push your-dockerhub-username/php-user-registration:v1
]]></description><link>https://notes.sarangwandile.xyz/development/mysql-app-from-scratch.html</link><guid isPermaLink="false">Development/Mysql APP From Scratch.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 26 Jan 2025 13:32:27 GMT</pubDate></item><item><title><![CDATA[2025-01-07 kubernetes]]></title><description><![CDATA[ 
 <br><a data-href="Kubernetes Architecture" href="https://notes.sarangwandile.xyz/devops/notes/kubernetes-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Kubernetes Architecture</a><br>
<img alt="kubernetes-cluster.svg" src="https://notes.sarangwandile.xyz/lib/media/kubernetes-cluster.svg"><br><br>Prerequisites<br>
<br>kubectl
<br>eksctl
<br>aws
<br>Steps<br>
<br>create ec2 instance
<br>add admin role on it
<br>install Prerequisites on it
<br>Install kubectl binary from <a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank">official website</a><br># for x86_64
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# for arm64
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
<br>Install eksctl binary from its <a data-tooltip-position="top" aria-label="https://eksctl.io/installation" rel="noopener nofollow" class="external-link" href="https://eksctl.io/installation" target="_blank">official website</a><br># for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz

sudo mv /tmp/eksctl /usr/local/bin
<br>Write this script into script.sh file and run it with bash script.sh<br>Check if commands working properly:<br># see kubectl version
kubectl version

# see eksctl version
eksctl version
<br>and run Create Cluster command<br>eksctl create cluster --name clustername --node-type t2.medium --nodes 2 --region=us-east-1
<br>To See nodes<br>kubectl get nodes
<br>To See pods<br>kubectl get pods -A
<br>write pod.yaml manifest and apply<br>kubectl apply -f pod.yaml
<br>get info about pods<br>kubectl describe pod nginx
<br>Read about:<br>Manifest
YAML

POD
Repl set
Deployments
Services
	Load balancer
	Node pod
	cluster ip
config
	secrets
	configMap
Namespace
Resources
Resource Quota
Volume
Ingress
HPA
<br>Pod.yaml<br>apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image:
    name:
    ports:
      container-port: 80
<br>s comes - comes<br>
- means array<br>cluster version updates comes in every 4 month<br>
minimum n-2 version compatible for client to support with latest version of kubernetes<br><br><br>
<br><a data-href="Task 6 How to set permanent alias" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-6-how-to-set-permanent-alias.html" class="internal-link" target="_self" rel="noopener nofollow">Task 6 How to set permanent alias</a>
<br><a data-href="Task 7 Create LAMP Server" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-7-create-lamp-server.html" class="internal-link" target="_self" rel="noopener nofollow">Task 7 Create LAMP Server</a>
<br><br><a rel="noopener nofollow" class="external-link" href="https://www.perplexity.ai/" target="_blank">https://www.perplexity.ai/</a><br>
<a rel="noopener nofollow" class="external-link" href="https://claude.ai/" target="_blank">https://claude.ai/</a><br>
<a rel="noopener nofollow" class="external-link" href="https://chatgpt.com" target="_blank">https://chatgpt.com</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-07-kubernetes.html</link><guid isPermaLink="false">Daily Notes/2025-01-07 kubernetes.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 08 Jan 2025 16:58:03 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/kubernetes-cluster.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/kubernetes-cluster.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2025-01-22 3-Microservice Tasks]]></title><description><![CDATA[ 
 <br><br>3 microservices in 3 different namespace<br>nginx: dev-nginx<br>
httpd: dev-httpd<br>
tomcat: dev-tomcat<br>Microservice 1<br>
<br>Create Docker Image nginx free css template
<br>Build it and deploy to k8s
<br>Microservice 2<br>
<br>Httpd container 
<br>Make deployment
<br>Send index.html via configMap
<br>Microservice 3<br>
<br>Deploy tomcat container
<br>Make deployment
<br>Nginx ingress controller deploy on nginx-ingress namespace<br>
and 3 end points for 3 microservices<br>&lt;subdomain.learndevops.live&gt;<br>Reading Task<br>
<br>Node affinity
<br>Anti Affinity
<br>Node Selectors
<br>Pod Affinity
<br>Taint and Tolerance
<br>Previous task twist<br>
<br>3 node
<br>one pod on each node
<br>node 1 --&gt; httpd
<br>node 2 --&gt; tomcat
<br>node 3 ---&gt; nginx
<br><a rel="noopener nofollow" class="external-link" href="https://envs.sh/gA-.yaml" target="_blank">https://envs.sh/gA-.yaml</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-22-3-microservice-tasks.html</link><guid isPermaLink="false">Daily Notes/2025-01-22 3-Microservice Tasks.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 22 Jan 2025 16:03:30 GMT</pubDate></item><item><title><![CDATA[3 Microservices Task]]></title><description><![CDATA[ 
 <br>1. Project Setup and Namespace Creation<br><br>
<br>Launch Ec2 instance
<br>Add IAM Role 
<br>Install Docker and Kubernetes Packages
<br>Create Cluster
<br><br>curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/neogym.zip

unzip neogym.zip

mv neogym-html/* html/
<br>2. Microservice 1: Nginx with Free CSS Template<br>
<br>Dockerfile (Dockerfile_nginx):
<br>FROM nginx:stable-alpine3.20-perl 
COPY ./html/ /usr/share/nginx/html/
EXPOSE 80

CMD ["/usr/bin/nginx", "-g", "daemon off;"]
<br>
<br>Build and Push Docker Image:
<br># Build Image
docker build -t nginx-free-css:latest -f Dockerfile_nginx . 
# Add tag to repo-name/image name for docker hub push
docker tag nginx-css &lt;repo-name&gt;/nginx-css
# Push to docker hub repo
docker push nginx-free-css:latest
<br>
<br>Create Kubernetes Namespaces:<br>
Create 3 Namespaces for 3 Microservices
<br>kubectl create namespace dev-nginx
kubectl create namespace dev-httpd
kubectl create namespace dev-tomcat
<br><br>
<br>Kubernetes Deployment (dev-nginx namespace):
<br>    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment
      namespace: dev-nginx
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx-free-css:latest
            ports:
            - containerPort: 80
<br>
<br>Apply Deployment:
<br>kubectl apply -f nginx-deployment.yaml -n dev-nginx
<br>3. Microservice 2: HTTP Server with ConfigMap<br>
<br>ConfigMap (dev-httpd namespace):
<br>apiVersion: v1
kind: ConfigMap
metadata:
  name: index-html
  namespace: dev-httpd
data:
  index.html: |
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    &lt;head&gt;
      &lt;title&gt;HTTP Server&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
      &lt;h1&gt;Hello from HTTP Server!&lt;/h1&gt;
    &lt;/body&gt;
    &lt;/html&gt;
<br>
<br>Deployment (dev-httpd namespace):
<br>apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment
  namespace: dev-httpd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd:latest 
        volumeMounts:
        - name: config-volume
          mountPath: /usr/local/apache2/htdocs/ 
      volumes:
      - name: config-volume
        configMap:
          name: index-html
<br>
<br>Apply ConfigMap and Deployment:
<br>kubectl apply -f index-html.yaml -n dev-httpd
kubectl apply -f httpd-deployment.yaml -n dev-httpd
<br>4. Microservice 3: Tomcat<br>
<br>Deployment (dev-tomcat namespace):
<br>apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  namespace: dev-tomcat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat:latest
        ports:
        - containerPort: 8080
<br>
<br>Apply Deployment:
<br>kubectl apply -f tomcat-deployment.yaml -n dev-tomcat
<br>5. Nginx Ingress Controller (nginx-ingress namespace)<br>
<br>Install Ingress Controller:

<br>Refer to the official Nginx Ingress Controller documentation for installation instructions.


<br>6. Ingress Configuration (nginx-ingress namespace)<br>
<br>Create Ingress resource:
<br>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: microservices-ingress
spec:
  rules:
  - host: httpd.your-domain.com 
    http:
      paths:
      - path: /
        pathType: Prefix 
        backend:
          service:
            name: httpd-service 
            port:
              number: 80
  - host: tomcat.your-domain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tomcat-service
            port:
              number: 8080
  - host: nginx.your-domain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
<br>
<br>Create Services for each Deployment:

<br>You'll need to create Services of type LoadBalancer or NodePort for each Deployment to expose them.


<br>Apply Ingress and Services:
<br>kubectl apply -f microservices-ingress.yaml -n nginx-ingress 
# Apply Service YAML files for each Deployment 
<br>7. DNS Configuration<br>
<br>Configure your DNS records to point the following domains to the Ingress Controller's IP address:

<br>httpd.your-domain.com
<br>tomcat.your-domain.com
<br>nginx.your-domain.com


<br>8. Node Assignment (Manual)<br>
<br>Node 1: Schedule the httpd-deployment to run on Node 1.
<br>Node 2: Schedule the tomcat-deployment to run on Node 2.
<br>Node 3: Schedule the nginx-deployment to run on Node 3.
<br>Note:<br>
<br>Replace placeholders like your-domain.com with actual domain names.
<br>Adjust paths, ports, and other configurations as per your requirements.
<br>This is a basic example. You may need to adjust the configurations based on your specific needs and security requirements.
<br>Consider using a more robust service discovery mechanism like Kubernetes Service.
<br>For production environments, use a more advanced deployment strategy like rolling updates or blue/green deployments.
]]></description><link>https://notes.sarangwandile.xyz/devops/k8s-tasks/3-microservices-task.html</link><guid isPermaLink="false">DevOps/K8S Tasks/3 Microservices Task.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 23 Jan 2025 07:26:07 GMT</pubDate></item><item><title><![CDATA[K8s Task 1]]></title><description><![CDATA[ 
 <br><br><br>
<br>Launch Ec2 instance
<br>Add IAM Role 
<br>Install Docker and Kubernetes Packages
<br>Create Cluster
<br><br>curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/neogym.zip

unzip neogym.zip

mv neogym-html/* html/
<br><br>
<br>Create Dockerfile
<br>FROM nginx:stable-alpine3.20-perl 
COPY ./html/ /usr/share/nginx/html/
EXPOSE 80

CMD ["/usr/bin/nginx", "-g", "daemon off;"]
<br>
<br>Build Image and Push it to Docker hub
<br>docker build . -t nginx-css

docker tag nginx-css &lt;repo-name&gt;/nginx-css

docker push &lt;repo-name&gt;/nginx-css
<br><br># Create namespace
kubectl create ns dev-nginx
<br>Deployment File<br>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-css
  namespace: dev-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-css
  template:
    metadata:
      labels:
        app: nginx-css
    spec:
      containers:
      - name: nginx
        image: archsarangx/nginx-css
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: dev-nginx
spec:
  selector:
    app: nginx-css
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
<br>Installing Helm<br><a rel="noopener nofollow" class="external-link" href="https://helm.sh" target="_blank">https://helm.sh</a>]]></description><link>https://notes.sarangwandile.xyz/devops/k8s-tasks/k8s-task-1.html</link><guid isPermaLink="false">DevOps/K8S Tasks/K8s Task 1.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 22 Jan 2025 07:16:41 GMT</pubDate></item><item><title><![CDATA[2025-01-21 Creating PVC]]></title><description><![CDATA[ 
 <br><br><br>
<br>Add admin role (administrator access)
<br><br>Install kubectl package<br>curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
<br>and eksctl <br>ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz

sudo mv /tmp/eksctl /usr/local/bin
<br>Write this script into script.sh file and run it with bash script.sh<br>Check if commands working properly:<br># see kubectl version
kubectl version

# see eksctl version
eksctl version
<br><br>eksctl create cluster --name clustername --node-type t2.medium --nodes 2 --region=us-east-1
<br><br><br>oidc_id=$(aws eks describe-cluster --name irondome --region=us-west-2 --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
echo $oidc_id
<br>You will get something like this: 04B9B8B86AAB8AF93259FBD40DFF0D4C<br>
‚ò†Ô∏èDont copy it it just for illustration.‚ò†Ô∏è<br><br>aws iam list-open-id-connect-providers --region=us-west-2 | grep $oidc_id | cut -d "/" -f4 
<br><br>eksctl utils associate-iam-oidc-provider --cluster irondome --approve --region=us-west-2
<br><br>eksctl create iamserviceaccount \
        --name ebs-csi-controller-sa \
        --namespace kube-system \
        --cluster irondome \
        --role-name AmazonEKS_EBS_CSI_DriverRole \
        --role-only \
        --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
        --approve  --region=us-west-2
<br><br><br>eksctl utils describe-addon-versions --kubernetes-version 1.30 --region=us-west-2 --name aws-ebs-csi-driver
<br><br>eksctl create addon --cluster irondome --name aws-ebs-csi-driver --version latest \
      --service-account-role-arn arn:aws:iam::970547378605:role/AmazonEKS_EBS_CSI_DriverRole --force --region=us-west-2
<br><br>Create a Pod definition file that:<br>
<br>Start an nginx container
<br>Mount an EBS volume (claimed through ebs-claim) at /data
<br>Store data persistently in the EBS volume even if the pod is deleted
<br>apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
<br>create claim.yaml<br>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: gp2
  resources:
    requests:
      storage: 4Gi
<br>This definition requests a 4GB persistent volume of type "gp2" (likely an EBS volume) that can be mounted by only one node at a time.<br>Practical Walkthrough Video<br>
<a rel="noopener nofollow" class="external-link" href="https://drive.google.com/file/d/1lSwdeRuENgKLNIaqMpFCM9SosavuoSDb/view?usp=sharing" target="_blank">https://drive.google.com/file/d/1lSwdeRuENgKLNIaqMpFCM9SosavuoSDb/view?usp=sharing</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-21-creating-pvc.html</link><guid isPermaLink="false">Daily Notes/2025-01-21 Creating PVC.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 22 Jan 2025 16:21:28 GMT</pubDate></item><item><title><![CDATA[2025-01-14]]></title><description><![CDATA[ 
 <br>Volumes<br>
env<br><br>Create namespace<br>kubectl create ns prod --dry-run=client -o yaml &gt; ns.yaml
<br>namespace creates on across the cluster<br><br>
<br>host
<br>addons (realtime use) --&gt; CSI Drivers EBS volume
<br>CSI ebs drivers<br>
pod<br>
persistence volume claim<br>service account<br>
storage account<br>kubectl get sa
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-14.html</link><guid isPermaLink="false">Daily Notes/2025-01-14.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 14 Jan 2025 04:34:28 GMT</pubDate></item><item><title><![CDATA[2025-01-20]]></title><description><![CDATA[ 
 <br><br>Difference between POD, Deployment and container<br><br>Controller: - Ensures desired state is always present in the cluster.<br>
Pod:- A wrapper around container that maintain desired state.<br>
ReplicaSet - A controller that carries autohealing behaviour.<br>Login to kubernetes cluster with minkube<br>
minikube ssh<br>Deplyoyment - Is an abstraction.<br>Controller is golang application which ensures specific behaviour is implemented.<br>
eg. - ReplicaSet behaviour ensures desired replicas are always present.<br>Autohealing - spawns the containers with different ip addresses<br>
hence we create service<br>svc ---- 1. load balancer<br>
|-------- 2. Service Discovery<br>
|                    |------- labels and selector<br>
|-------- 3. expsoing to the world<br>autohealing without service ----<br>
deployment<br>
pod ---- 172.16.3.4     ----&gt; user 1<br>
rs1----     172.16.3.5  ----&gt; user 2<br>
rs2  ---    172.16.3.6 ----&gt; user 3<br>but if one pod gets deleted auto healing will create another pod with different ip address and user not knowing new ip address unable to access it<br>hence service comes in handy<br>Service (eg. load balancer) will have single ip and all users will get ip of service and service will distribute traffic among pods and rs.<br>but when pods gets respawn with new ip how does service knows its ip<br>
for that service has this feature called service discovery <br>how service does service discovery<br>
unlike manually keeping track of ip addresses which can change everytime service use labels and selectors mechanism.<br>
service maintains service discovery through labels and selectors.<br>
label will be common for all the app and service will only be watching for the labels.<br>Service is also use for exposing your application to outside world.<br>
service is allow application to access outiside k8s cluster.<br>You can create service of 3 types <br>Service.YAML<br>
|_ _ _ _  1. Cluster IP (inside accessible) [ you get discovery and lb]<br>
| _ _ _  2. Node Port (inside organisation or network\ access worker node )<br>
| _ _ _ _ 3. Load balancer ( service is accessible to external world)<br>load balancer only work for cloud based k8s like EKS<br>load balancer ----&gt; amazon.com<br>
Node Port ---&gt; VPC and nodes<br>
CLUSTER IP ---&gt; Cluster Network ]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-20.html</link><guid isPermaLink="false">Daily Notes/2025-01-20.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 20 Jan 2025 09:13:47 GMT</pubDate></item><item><title><![CDATA[2025-01-13]]></title><description><![CDATA[ 
 <br>ConfigMap - non confidential information<br>
Secrets - confidential information<br><br><br>Create configMap Command<br>kubectl create cm nginx-cm --from-file=index.html
<br>generate configmap definition file<br>kubectl create cm nginx-cm --from-file=index.html --dry-run=client -o yaml &gt; configmap.yaml
<br>Enter into pod shell<br>kubectl exec -it mypod --bash
<br>Port forward<br>kubectl port-forward service/myservice 80:80
<br><br>Three types of secret<br>
<br>docker-registry : Create a secret for use with a Docker registry
<br>generic : Create a secret from a local file, directory, or literal value
<br>tls : Create a TLS secret
<br>Generate Secret.yaml file<br>kubectl create secret generic --from-literal=idli=chutney --dry-run=client -o yaml &gt; secret.yaml
<br>Decode secret<br>echo "Y2hhdG55" | base64 --decode
<br>Expose pod external port<br>kubectl expose pod nginx-secret --port=80 -o yaml --dry-run=client &gt; svc.yaml
<br><br>Tommorrow:<br>
<br><a data-href="k8s Namespace" href="https://notes.sarangwandile.xyz/devops/for-reading/k8s-namespace.html" class="internal-link" target="_self" rel="noopener nofollow">k8s Namespace</a>
<br><a data-href="k8s Volume" href="https://notes.sarangwandile.xyz/devops/for-reading/k8s-volume.html" class="internal-link" target="_self" rel="noopener nofollow">k8s Volume</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-13.html</link><guid isPermaLink="false">Daily Notes/2025-01-13.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 13 Jan 2025 05:53:07 GMT</pubDate></item><item><title><![CDATA[k8s Namespace]]></title><description><![CDATA[ 
 <br>In Kubernetes,&nbsp;namespaces&nbsp;provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/overview/working-with-objects/#kubernetes-objects" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a>&nbsp;(e.g. Deployments, Services, etc.)&nbsp;and not for cluster-wide objects&nbsp;(e.g. StorageClass, Nodes, PersistentVolumes, etc.).<br><br>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.<br>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.<br>Namespaces are a way to divide cluster resources between multiple users (via&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/policy/resource-quotas/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank">resource quota</a>).<br>It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a>&nbsp;to distinguish resources within the same namespace.<br><br>For a production cluster, consider&nbsp;not&nbsp;using the&nbsp;default&nbsp;namespace. Instead, make other namespaces and use those.<br><br>Kubernetes starts with four initial namespaces:<br>default<br>Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.<br>kube-node-lease<br>This namespace holds&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/architecture/leases/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/architecture/leases/" target="_blank">Lease</a>&nbsp;objects associated with each node. Node leases allow the kubelet to send&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/architecture/nodes/#node-heartbeats" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-heartbeats" target="_blank">heartbeats</a>&nbsp;so that the control plane can detect node failure.<br>kube-public<br>This namespace is readable by&nbsp;all&nbsp;clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.<br>kube-system<br>The namespace for objects created by the Kubernetes system.<br><br>Creation and deletion of namespaces are described in the&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/" target="_blank">Admin Guide documentation for namespaces</a>.<br><br>Avoid creating namespaces with the prefix&nbsp;kube-, since it is reserved for Kubernetes system namespaces.<br><br>You can list the current namespaces in a cluster using:<br>kubectl get namespace
<br>NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
<br><br>To set the namespace for a current request, use the&nbsp;--namespace&nbsp;flag.<br>For example:<br>kubectl run nginx --image=nginx --namespace=&lt;insert-namespace-name-here&gt;
kubectl get pods --namespace=&lt;insert-namespace-name-here&gt;
<br><br>You can permanently save the namespace for all subsequent kubectl commands in that context.<br>kubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;
# Validate it
kubectl config view --minify | grep namespace:
<br><br>When you create a&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/services-networking/service/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank">Service</a>, it creates a corresponding&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank">DNS entry</a>. This entry is of the form&nbsp;&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local, which means that if a container only uses&nbsp;&lt;service-name&gt;, it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).<br>As a result, all namespace names must be valid&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names" target="_blank">RFC 1123 DNS labels</a>.<br><br>By creating namespaces with the same name as&nbsp;<a data-tooltip-position="top" aria-label="https://data.iana.org/TLD/tlds-alpha-by-domain.txt" rel="noopener nofollow" class="external-link" href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt" target="_blank">public top-level domains</a>, Services in these namespaces can have short DNS names that overlap with public DNS records. Workloads from any namespace performing a DNS lookup without a&nbsp;<a data-tooltip-position="top" aria-label="https://datatracker.ietf.org/doc/html/rfc1034#page-8" rel="noopener nofollow" class="external-link" href="https://datatracker.ietf.org/doc/html/rfc1034#page-8" target="_blank">trailing dot</a>&nbsp;will be redirected to those services, taking precedence over public DNS.<br>To mitigate this, limit privileges for creating namespaces to trusted users. If required, you could additionally configure third-party security controls, such as&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank">admission webhooks</a>, to block creating any namespace with the name of&nbsp;<a data-tooltip-position="top" aria-label="https://data.iana.org/TLD/tlds-alpha-by-domain.txt" rel="noopener nofollow" class="external-link" href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt" target="_blank">public TLDs</a>.<br><br>Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However namespace resources are not themselves in a namespace. And low-level resources, such as&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/architecture/nodes/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/architecture/nodes/" target="_blank">nodes</a>&nbsp;and&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank">persistentVolumes</a>, are not in any namespace.<br>To see which Kubernetes resources are and aren't in a namespace:<br># In a namespace
kubectl api-resources --namespaced=true

# Not in a namespace
kubectl api-resources --namespaced=false
<br><br>FEATURE STATE:&nbsp;Kubernetes 1.22 [stable]<br>The Kubernetes control plane sets an immutable&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a>&nbsp;kubernetes.io/metadata.name&nbsp;on all namespaces. The value of the label is the namespace name.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/k8s-namespace.html</link><guid isPermaLink="false">DevOps/For Reading/k8s Namespace.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 13 Jan 2025 05:35:30 GMT</pubDate></item><item><title><![CDATA[k8s Volume]]></title><description><![CDATA[ 
 <br>On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. First, when a container crashes kubelet will restart it, but the files will be lost - the container starts with a clean state. Second, when running containers together in a&nbsp;Pod&nbsp;it is often necessary to share files between those containers. The Kubernetes&nbsp;Volume&nbsp;abstraction solves both of these problems.<br><br>Docker also has a concept of&nbsp;<a data-tooltip-position="top" aria-label="https://docs.docker.com/userguide/dockervolumes/" rel="noopener nofollow" class="external-link" href="https://docs.docker.com/userguide/dockervolumes/" target="_blank">volumes</a>, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another container. Lifetimes are not managed and until very recently there were only local-disk-backed volumes. Docker now provides volume drivers, but the functionality is very limited for now (e.g. as of Docker 1.7 only one volume driver is allowed per container and there is no way to pass parameters to volumes).<br>A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many type of volumes, and a Pod can use any number of them simultaneously.<br>At its core, a volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.<br>To use a volume, a pod specifies what volumes to provide for the pod (the&nbsp;spec.volumes&nbsp;field) and where to mount those into containers(the&nbsp;spec.containers.volumeMounts&nbsp;field).<br>A process in a container sees a filesystem view composed from their Docker image and volumes. The&nbsp;<a data-tooltip-position="top" aria-label="https://docs.docker.com/userguide/dockerimages/" rel="noopener nofollow" class="external-link" href="https://docs.docker.com/userguide/dockerimages/" target="_blank">Docker image</a>&nbsp;is at the root of the filesystem hierarchy, and any volumes are mounted at the specified paths within the image. Volumes can not mount onto other volumes or have hard links to other volumes. Each container in the Pod must independently specify where to mount each volume.<br><br>Kubernetes supports several types of Volumes:<br>
<br>emptyDir
<br>hostPath
<br>gcePersistentDisk
<br>awsElasticBlockStore
<br>nfs
<br>iscsi
<br>flocker
<br>glusterfs
<br>rbd
<br>cephfs
<br>gitRepo
<br>secret
<br>persistentVolumeClaim
<br>downwardAPI
<br>azureFileVolume
<br>azureDisk
<br>vsphereVolume
<br>Quobyte
<br>PortworxVolume
<br>ScaleIO
<br>We welcome additional contributions.<br><br>An&nbsp;emptyDir&nbsp;volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the pod can all read and write the same files in the&nbsp;emptyDir&nbsp;volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the&nbsp;emptyDir&nbsp;is deleted forever. NOTE: a container crashing does&nbsp;NOT&nbsp;remove a pod from a node, so the data in an&nbsp;emptyDir&nbsp;volume is safe across container crashes.<br>Some uses for an&nbsp;emptyDir&nbsp;are:<br>
<br>scratch space, such as for a disk-based merge sort
<br>checkpointing a long computation for recovery from crashes
<br>holding files that a content-manager container fetches while a webserver container serves the data
<br>By default,&nbsp;emptyDir&nbsp;volumes are stored on whatever medium is backing the machine - that might be disk or SSD or network storage, depending on your environment. However, you can set the&nbsp;emptyDir.medium&nbsp;field to&nbsp;"Memory"&nbsp;to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on machine reboot and any files you write will count against your container's memory limit.<br><br>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
<br><br>A&nbsp;hostPath&nbsp;volume mounts a file or directory from the host node's filesystem into your pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.<br>For example, some uses for a&nbsp;hostPath&nbsp;are:<br>
<br>running a container that needs access to Docker internals; use a&nbsp;hostPath&nbsp;of&nbsp;/var/lib/docker
<br>running cAdvisor in a container; use a&nbsp;hostPath&nbsp;of&nbsp;/dev/cgroups
<br>Watch out when using this type of volume, because:<br>
<br>pods with identical configuration (such as created from a podTemplate) may behave differently on different nodes due to different files on the nodes
<br>when Kubernetes adds resource-aware scheduling, as is planned, it will not be able to account for resources used by a&nbsp;hostPath
<br>the directories created on the underlying hosts are only writable by root. You either need to run your process as root in a&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/security-context" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/security-context" target="_blank">privileged container</a>&nbsp;or modify the file permissions on the host to be able to write to a&nbsp;hostPath&nbsp;volume
<br><br>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
<br><br>A&nbsp;gcePersistentDisk&nbsp;volume mounts a Google Compute Engine (GCE)&nbsp;<a data-tooltip-position="top" aria-label="http://cloud.google.com/compute/docs/disks" rel="noopener nofollow" class="external-link" href="http://cloud.google.com/compute/docs/disks" target="_blank">Persistent Disk</a>&nbsp;into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of a PD are preserved and the volume is merely unmounted. This means that a PD can be pre-populated with data, and that data can be "handed off" between pods.<br>Important: You must create a PD using&nbsp;gcloud&nbsp;or the GCE API or UI before you can use it<br>There are some restrictions when using a&nbsp;gcePersistentDisk:<br>
<br>the nodes on which pods are running must be GCE VMs
<br>those VMs need to be in the same GCE project and zone as the PD
<br>A feature of PD is that they can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a PD with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, PDs can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.<br>Using a PD on a pod controlled by a ReplicationController will fail unless the PD is read-only or the replica count is 0 or 1.<br><br>Before you can use a GCE PD with a pod, you need to create it.<br>gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk
<br><br>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    # This GCE PD must already exist.
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4
<br><br>An&nbsp;awsElasticBlockStore&nbsp;volume mounts an Amazon Web Services (AWS)&nbsp;<a data-tooltip-position="top" aria-label="http://aws.amazon.com/ebs/" rel="noopener nofollow" class="external-link" href="http://aws.amazon.com/ebs/" target="_blank">EBS Volume</a>&nbsp;into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be "handed off" between pods.<br>Important: You must create an EBS volume using&nbsp;aws ec2 create-volume&nbsp;or the AWS API before you can use it<br>There are some restrictions when using an awsElasticBlockStore volume:<br>
<br>the nodes on which pods are running must be AWS EC2 instances
<br>those instances need to be in the same region and availability-zone as the EBS volume
<br>EBS only supports a single EC2 instance mounting a volume
<br><br>Before you can use an EBS volume with a pod, you need to create it.<br>aws ec2 create-volume --availability-zone eu-west-1a --size 10 --volume-type gp2
<br>Make sure the zone matches the zone you brought up your cluster in. (And also check that the size and EBS volume type are suitable for your use!)<br><br>apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-ebs
      name: test-volume
  volumes:
  - name: test-volume
    # This AWS EBS volume must already exist.
    awsElasticBlockStore:
      volumeID: &lt;volume-id&gt;
      fsType: ext4
<br><br>An&nbsp;nfs&nbsp;volume allows an existing NFS (Network File System) share to be mounted into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of an&nbsp;nfs&nbsp;volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be "handed off" between pods. NFS can be mounted by multiple writers simultaneously.<br>Important: You must have your own NFS server running with the share exported before you can use it<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/nfs" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/nfs" target="_blank">NFS example</a>&nbsp;for more details.<br><br>An&nbsp;iscsi&nbsp;volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of an&nbsp;iscsi&nbsp;volume are preserved and the volume is merely unmounted. This means that an iscsi volume can be pre-populated with data, and that data can be "handed off" between pods.<br>Important: You must have your own iSCSI server running with the volume created before you can use it<br>A feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/iscsi" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/iscsi" target="_blank">iSCSI example</a>&nbsp;for more details.<br><br><a data-tooltip-position="top" aria-label="https://clusterhq.com/flocker" rel="noopener nofollow" class="external-link" href="https://clusterhq.com/flocker" target="_blank">Flocker</a>&nbsp;is an open-source clustered container data volume manager. It provides management and orchestration of data volumes backed by a variety of storage backends.<br>A&nbsp;flocker&nbsp;volume allows a Flocker dataset to be mounted into a pod. If the dataset does not already exist in Flocker, it needs to be first created with the Flocker CLI or by using the Flocker API. If the dataset already exists it will be reattached by Flocker to the node that the pod is scheduled. This means data can be "handed off" between pods as required.<br>Important: You must have your own Flocker installation running before you can use it<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/flocker" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/flocker" target="_blank">Flocker example</a>&nbsp;for more details.<br><br>A&nbsp;glusterfs&nbsp;volume allows a&nbsp;<a data-tooltip-position="top" aria-label="http://www.gluster.org/" rel="noopener nofollow" class="external-link" href="http://www.gluster.org/" target="_blank">Glusterfs</a>&nbsp;(an open source networked filesystem) volume to be mounted into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of a&nbsp;glusterfs&nbsp;volume are preserved and the volume is merely unmounted. This means that a glusterfs volume can be pre-populated with data, and that data can be "handed off" between pods. GlusterFS can be mounted by multiple writers simultaneously.<br>Important: You must have your own GlusterFS installation running before you can use it<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/glusterfs" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/glusterfs" target="_blank">GlusterFS example</a>&nbsp;for more details.<br><br>An&nbsp;rbd&nbsp;volume allows a&nbsp;<a data-tooltip-position="top" aria-label="http://ceph.com/docs/master/rbd/rbd/" rel="noopener nofollow" class="external-link" href="http://ceph.com/docs/master/rbd/rbd/" target="_blank">Rados Block Device</a>&nbsp;volume to be mounted into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of a&nbsp;rbd&nbsp;volume are preserved and the volume is merely unmounted. This means that a RBD volume can be pre-populated with data, and that data can be "handed off" between pods.<br>Important: You must have your own Ceph installation running before you can use RBD<br>A feature of RBD is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, RBD volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/rbd" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/rbd" target="_blank">RBD example</a>&nbsp;for more details.<br><br>A&nbsp;cephfs&nbsp;volume allows an existing CephFS volume to be mounted into your pod. Unlike&nbsp;emptyDir, which is erased when a Pod is removed, the contents of a&nbsp;cephfs&nbsp;volume are preserved and the volume is merely unmounted. This means that a CephFS volume can be pre-populated with data, and that data can be "handed off" between pods. CephFS can be mounted by multiple writers simultaneously.<br>Important: You must have your own Ceph server running with the share exported before you can use it<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/cephfs/" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/cephfs/" target="_blank">CephFS example</a>&nbsp;for more details.<br><br>A&nbsp;gitRepo&nbsp;volume is an example of what can be done as a volume plugin. It mounts an empty directory and clones a git repository into it for your pod to use. In the future, such volumes may be moved to an even more decoupled model, rather than extending the Kubernetes API for every such use case.<br>Here is an example for gitRepo volume:<br>apiVersion: v1
kind: Pod
metadata:
  name: server
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /mypath
      name: git-volume
  volumes:
  - name: git-volume
    gitRepo:
      repository: "git@somewhere:me/my-git-repository.git"
      revision: "22f1d8406d464b0c0874075539c1f2e96c253775"
<br><br>A&nbsp;secret&nbsp;volume is used to pass sensitive information, such as passwords, to pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly.&nbsp;secret&nbsp;volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.<br>Important: You must create a secret in the Kubernetes API before you can use it<br>Secrets are described in more detail&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/secrets" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/secrets" target="_blank">here</a>.<br><br>A&nbsp;persistentVolumeClaim&nbsp;volume is used to mount a&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes" target="_blank">PersistentVolume</a>&nbsp;into a pod. PersistentVolumes are a way for users to "claim" durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment.<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes/" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes/" target="_blank">PersistentVolumes example</a>&nbsp;for more details.<br><br>A&nbsp;downwardAPI&nbsp;volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files.<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/downward-api/volume/" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/downward-api/volume/" target="_blank"><code></code>&nbsp;volume example</a>downwardAPI&nbsp;for more details.<br><br>A&nbsp;FlexVolume&nbsp;enables users to mount vendor volumes into a pod. It expects vendor drivers are installed in the volume plugin path on each kubelet node. This is an alpha feature and may change in future.<br>More details are in&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/flexvolume/README.md" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/flexvolume/README.md" target="_blank">here</a><br><br>A&nbsp;AzureFileVolume&nbsp;is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0) into a Pod.<br>More details can be found&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/azure_file/README.md" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/azure_file/README.md" target="_blank">here</a><br><br>A&nbsp;AzureDiskVolume&nbsp;is used to mount a Microsoft Azure&nbsp;<a data-tooltip-position="top" aria-label="https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/" rel="noopener nofollow" class="external-link" href="https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/" target="_blank">Data Disk</a>&nbsp;into a Pod.<br>More details can be found&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/azure_disk/README.md" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/azure_disk/README.md" target="_blank">here</a><br><br>Prerequisite: Kubernetes with vSphere Cloud Provider configured. For cloudprovider configuration please refer&nbsp;<a data-tooltip-position="top" aria-label="http://kubernetes.io/docs/getting-started-guides/vsphere/" rel="noopener nofollow" class="external-link" href="http://kubernetes.io/docs/getting-started-guides/vsphere/" target="_blank">vSphere getting started guide</a>.<br>A&nbsp;vsphereVolume&nbsp;is used to mount a vSphere VMDK Volume into your Pod. The contents of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.<br>Important: You must create VMDK using one of the following method before using with POD.<br><br>
<br>Create using vmkfstools.
<br>First ssh into ESX and then use following command to create vmdk,<br>    vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
<br>
<br>Create using vmware-vdiskmanager.
<br>  vmware-vdiskmanager -c -t 0 -s 40GB -a lsilogic myDisk.vmdk
<br><br>apiVersion: v1
kind: Pod
metadata:
  name: test-vmdk
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-vmdk
      name: test-volume
  volumes:
  - name: test-volume
    # This VMDK volume must already exist.
    vsphereVolume:
      volumePath: "[DatastoreName] volumes/myDisk"
      fsType: ext4
<br>More examples can be found&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/vsphere" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/vsphere" target="_blank">here</a>.<br><br>A&nbsp;Quobyte&nbsp;volume allows an existing&nbsp;<a data-tooltip-position="top" aria-label="http://www.quobyte.com/" rel="noopener nofollow" class="external-link" href="http://www.quobyte.com/" target="_blank">Quobyte</a>&nbsp;volume to be mounted into your pod.<br>Important: You must have your own Quobyte setup running with the volumes created before you can use it<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/quobyte" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/quobyte" target="_blank">Quobyte example</a>&nbsp;for more details.<br><br>A&nbsp;PortworxVolume&nbsp;is an elastic block storage layer that runs hyperconverged with Kubernetes. Portworx fingerprints storage in a server, tiers based on capabilities, and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.<br>A&nbsp;PortworxVolume&nbsp;can be dynamically created through Kubernetes or it can also be pre-provisioned and referenced inside a Kubernetes pod. Here is an example pod referencing a pre-provisioned PortworxVolume:<br>apiVersion: v1
kind: Pod
metadata:
  name: test-portworx-volume-pod
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /mnt
      name: pxvol
  volumes:
  - name: pxvol
    # This Portworx volume must already exist.
    portworxVolume:
      volumeID: "pxvol"
      fsType: "&lt;fs-type&gt;"
<br>Important: Make sure you have an existing PortworxVolume with name&nbsp;pxvol&nbsp;before using it in the pod<br>More details and examples can be found&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/portworx/README.md" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/portworx/README.md" target="_blank">here</a><br><br>ScaleIO is a software-based storage platform that can use existing hardware to create clusters of scalable shared block networked storage. The ScaleIO volume plugin allows deployed pods to access existing ScaleIO volumes (or it can dynamically provision new volumes for persistent volume claims, see&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes/#scaleio" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/persistent-volumes/#scaleio" target="_blank">ScaleIO Persistent Volumes</a>).<br>Important: You must have an existing ScaleIO cluster already setup and running with the volumes created before you can use them<br>The following is an example pod configuration with ScaleIO:<br>apiVersion: v1
kind: Pod
metadata:
  name: pod-0
spec:
  containers:
  - image: gcr.io/google_containers/test-webserver
    name: pod-0
    volumeMounts:
    - mountPath: /test-pd
      name: vol-0
  volumes:
  - name: vol-0
    scaleIO:
      gateway: https://localhost:443/api
      system: scaleio
      volumeName: vol-0
      secretRef:
        name: sio-secret
      fsType: xfs
<br>For further detail, plese the see the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/scaleio" rel="noopener nofollow" class="external-link" href="https://github.com/kubernetes/kubernetes/tree/%7B%7Bpage.githubbranch%7D%7D/examples/volumes/scaleio" target="_blank">ScaleIO examples</a>.<br><br>Sometimes, it is useful to share one volume for multiple uses in a single pod. The&nbsp;volumeMounts.subPath&nbsp;property can be used to specify a sub-path inside the referenced volume instead of its root.<br>Here is an example of a pod with a LAMP stack (Linux Apache Mysql PHP) using a single, shared volume. The HTML contents are mapped to its&nbsp;html&nbsp;folder, and the databases will be stored in its&nbsp;mysql&nbsp;folder:<br>apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
<br><br>The storage media (Disk, SSD, etc.) of an&nbsp;emptyDir&nbsp;volume is determined by the medium of the filesystem holding the kubelet root dir (typically&nbsp;/var/lib/kubelet). There is no limit on how much space an&nbsp;emptyDir&nbsp;or&nbsp;hostPath&nbsp;volume can consume, and no isolation between containers or between pods.<br>In the future, we expect that&nbsp;emptyDir&nbsp;and&nbsp;hostPath&nbsp;volumes will be able to request a certain amount of space using a&nbsp;<a data-tooltip-position="top" aria-label="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/compute-resources" rel="noopener nofollow" class="external-link" href="https://unofficial-kubernetes.readthedocs.io/en/latest/docs/user-guide/compute-resources" target="_blank">resource</a>&nbsp;specification, and to select the type of media to use, for clusters that have several media types.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/k8s-volume.html</link><guid isPermaLink="false">DevOps/For Reading/k8s Volume.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 13 Jan 2025 05:56:59 GMT</pubDate></item><item><title><![CDATA[Agile Development]]></title><description><![CDATA[ 
 <br><br>Agile Manifesto
The agile manifesto outlines 4 values and 12 principles for teams, but‚Äîdecades later‚Äîis it still relevant? Find out
<br>Scrum
In scrum, a product is built in a series of fixed-length iterations called sprints, giving agile teams a framework for shipping software on a regular cadence. Learn how the scrum methodology impacts traditional project management.
<br>Kanban
Kanban is a popular agile framework that requires real-time communication of team's capacity and full transparency of work. Learn how the kanban methodology for agile software development can benefit for your team.
<br><br>The Agile methodology is a project management approach that involves breaking the project into phases and emphasizes continuous collaboration and improvement. Teams follow a cycle of planning, executing, and evaluating.<br>Whereas the traditional "waterfall" approach has one discipline contribute to the project, then "throw it over the wall" to the next contributor, agile calls for collaborative cross-functional teams. Open communication, collaboration, adaptation, and trust amongst team members are at the heart of agile. Although the project lead or product owner typically prioritizes the work to be delivered, the team takes the lead on deciding how the work will get done, self-organizing around granular tasks and assignments.<br>Agile isn't defined by a set of ceremonies or specific development techniques. Rather, agile is a group of methodologies that demonstrate a commitment to tight feedback cycles and continuous improvement.<br><img alt="Agile development example | Atlassian Agile Coach" src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The original&nbsp;<a data-tooltip-position="top" aria-label="http://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="http://agilemanifesto.org/" target="_blank">Agile Manifesto</a>&nbsp;didn't prescribe two-week iterations or an ideal team size. It simply laid out a set of core values that put people first. The way you and your team live those values today ‚Äì whether you do scrum by the book, or blend elements of kanban and XP ‚Äì is entirely up to you.<br><br>Teams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year's worth of plans. "Just enough" planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.<br>But it's not just a numbers game‚Äîfirst and foremost, it's about people. As described by the Agile Manifesto, authentic human interactions are more important than rigid processes. Collaborating with customers and teammates is more important than predefined arrangements. And delivering a working solution to the customer's problem is more important than hyper-detailed documentation.<br>An agile team unites under a shared vision, then brings it to life the way they know is best. Each team sets their own standards for quality, usability, and completeness. Their "definition of done" then informs how fast they'll churn the work out. Although it can be scary at first, company leaders find that when they put their trust in an agile team, that team feels a greater sense of ownership and rises to meet (or exceed) management's expectations.<br><br>The publication of the Agile Manifesto in 2001 marks the birth of agile as a methodology. Since then, many agile frameworks have emerged such as scrum,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" target="_blank">lean</a>, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way.&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/scrum" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/scrum" target="_blank">Scrum</a>&nbsp;and XP are favored by software development teams, while&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>&nbsp;is a darling among service-oriented teams like IT or human resources.<br>Today, many agile teams combine practices from a few different frameworks, spiced up with practices unique to the team. Some teams adopt some agile rituals&nbsp;(like regular stand-ups, retros, backlogs, etc.), while others created a&nbsp;new agile practice (<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" target="_blank">agile marketing teams</a>&nbsp;who adhere to the&nbsp;Agile Marketing Manifesto).<br><img alt="Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:8631b273-6575-4586-9a9a-f5eb84d4b46f/illustrations-spot-Code%20Release%202.svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The agile teams of tomorrow will value their own effectiveness over adherence to doctrine. Openness, trust, and autonomy are emerging as the cultural currency for companies who want to attract the best people and get the most out of them. Such companies are already proving that practices can vary across teams, as long as they're guided by the right principles.<br><br>The way each team practices agile should be unique to their needs and culture. Indeed, no two teams inside Atlassian have identical agile practices.<br>Although many of our teams organize their work in sprints, estimate in story points, and prioritize their backlogs, we're not die-hard practitioners of scrum. Or kanban. Or any other trademarked methodology. Instead, we give each team the autonomy to cherry-pick the practices that will make them most effective. And we encourage you to take a similar approach.<br>For example, if you're on a queue-oriented team like IT, kanban provides a solid foundation for your agile practice. But nothing should stop you from sprinkling in a few scrum practices like demo sessions with stakeholders or regular retrospectives.<br>The key to doing agile right is&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" target="_blank">embracing a mindset of continuous improvement</a>. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don't.<br><img alt="Atlassian on agile | Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:358e6b8e-5eec-428f-8d2f-c9aedb962263/illustrations-spot-hero-Status%20Page%20Light-1115x898@2x.png?cdnVersion=2472" referrerpolicy="no-referrer">]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/agile-development.html</link><guid isPermaLink="false">AWS/For Reading/Agile Development.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Agile Vs DevOps]]></title><description><![CDATA[ 
 <br>Agile and DevOps have shaped the way software is developed today. They‚Äôve become so widely adapted and revered as to permeate beyond the world of software development into shaping project management and org charts in businesses of all stripes.<br>DevOps and agile can be tricky to define, and the lines between the two often seem to blur.<br>
<br>At a basic level, DevOps is the combination of two teams (software development and IT operations) to create a more powerful, efficient software development process.
<br>Agile is a series of methodologies around iterative development designed to make tasks smaller and more manageable and increase collaboration.
<br>But what are the deeper differences between DevOps and agile? Is DevOps&nbsp;better&nbsp;than agile? Or can DevOps and agile be implemented together? In this post, we‚Äôll dive into some common agile and DevOps FAQs.<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" target="_blank">What is the difference between DevOps and agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" target="_blank">What is agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" target="_blank">What are the benefits of agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" target="_blank">How can a company be agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" target="_blank">What is DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" target="_blank">Why combine software development and IT operations?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" target="_blank">What are some DevOps concepts and key terms?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" target="_blank">What are the benefits of DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" target="_blank">Is DevOps better than agile? Is DevOps a replacement for agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">What is CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" target="_blank">What are the benefits of CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" target="_blank">What are some common DevOps tools?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" target="_blank">How can I learn more about DevOps?</a>
<br><br>Agile and DevOps are both used in the development of software. While they‚Äôre both designed to improve the software development process, they seek to do so by taking different approaches. But they‚Äôre not mutually exclusive. (More on that below.)<br>Agile is essentially about giving software developers a shared set of principles to drive decision-making and allow for more responsiveness to change.<br>DevOps is about a culture change intended to drive collaboration between software developers and IT operations.<br>It's often said that DevOps is agile applied beyond the software development team.<br><br><img alt="Post-COVID DevOps" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank"><strong></strong>
</a>Post-COVID DevOps: Accelerating the Future<br>How has COVID affected ‚Äî or even accelerated ‚Äî DevOps best practices for engineering teams?&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank">Watch this free, on-demand webinar</a>&nbsp;panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.<br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank">Watch Now</a><br><br><br>Agile is an iterative software development approach that&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" target="_blank">focuses on collaboration and quick, rapid releases</a>. It's a set of values and principles that can be used to help drive decisions in software development.&nbsp;<br>When it comes to agile, it‚Äôs tough to define it more concisely than the original micro&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/" target="_blank">manifesto</a>&nbsp;itself, which was written back in 2001. (No TL;DR version needed. It's only 68 words.) It states:<br>We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:<br>
<br>Individuals and interactions&nbsp;over processes and tools
<br>Working software&nbsp;over comprehensive documentation
<br>Customer collaboration&nbsp;over contract negotiation
<br>Responding to change&nbsp;over following a plan
<br>That is, while there is value in the items on the right, we value the items on the left more<br>The manifest is paired with&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/principles.html" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/principles.html" target="_blank">12 agile principles</a>&nbsp;to help make better decisions.&nbsp;<br>Copying agile approaches and investing in agile tools doesn‚Äôt make your team agile. It‚Äôs not just about doing two-week iterations or building out smaller teams. In agile, the&nbsp;what&nbsp;a team does is less important than the&nbsp;why&nbsp;they do it.<br>Agile (as the name implies) is about the flexibility and adaptability to build software with ever-changing needs and toss set-in-stone plans out the window.<br>This includes better connecting the dev team with end-users. (You could sort of think of that a bit like the&nbsp;<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=m4OvQIGDg4I" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=m4OvQIGDg4I" target="_blank">‚ÄúWhat would you say you do here?‚Äù scene</a>&nbsp;from&nbsp;Office Space.)<br><br>
<br>Agility&nbsp;-&nbsp; More quickly respond to market changes or customer feedback.
<br>Quality&nbsp;- A focus on testing and sustained customer involvement means the chances of a product‚Äôs overall quality being high are greater.
<br>Collaboration&nbsp;- Agile is about people. By placing value on human interactions over processes and ‚Äúthat‚Äôs just the way it‚Äôs done,‚Äù organizations are able to let employees act guided by their experience and a shared set of values rather than being micro-managed or shackled to detailed documentation.
<br>Ownership&nbsp;- The trust required from leadership to have agile teams can create an increased feeling of ownership.
<br>Customer satisfaction&nbsp;- With a focus on finding and fixing problems quickly and a direct line between customers and developers, customers are more likely to be satisfied and come back for more.
<br>Predictability&nbsp;- By doing away with big plans that are set in stone and often detached from reality, organizations can get a better picture of what‚Äôs going well and what‚Äôs not working right away rather than months down the road when it‚Äôs too late to do anything to correct it.
<br>Increase productivity&nbsp;- Regularly used planning tools like Kanban and Scrum help teams keep tabs of work and progress toward goals.
<br><br>Many organizations are used to working in a waterfall model. A waterfall model is a linear approach is a sequence of events that starts with a heavy upfront investment of time and resources in scoping out requirements and budgeting before moving into development, testing, and production.<br>Moving this paradigm to agile and&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/overview/agile-at-scale" rel="noopener nofollow" class="external-link" href="https://acloud.guru/overview/agile-at-scale" target="_blank">running agile at scale</a>&nbsp;can be difficult and complex. This isn‚Äôt helped by the fact that ‚Äúagile‚Äù is less of a thing you do and more of a basis for decision-making. For organizations guided by process above all else, this can require a major shift and definitely takes some backing from management.<br><br>With a name that sounds like a covert military team and the kind of goofy capitalization you‚Äôve come to expect from tech terminology, DevOps combines software development and IT operations. Take the "dev" from software development and the "ops" from IT operations and you get this portmanteau, which is the fun-to-say term that describes mashing together of multiple words, like spork, phablet, brunch, jorts, and bromance.<br>DevOps isn't just a process. It‚Äôs a shift in workplace culture. It‚Äôs a collaboration between teams. Doubling down on automation and installing all of the right software won‚Äôt get you there alone. Like agile, people are the key component.<br>Interested in upscaling or beginning your journey with DevOps? A Cloud Guru‚Äôs&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/learning-paths/devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/learning-paths/devops" target="_blank">DevOps Learning Paths</a>&nbsp;offers custom courses fit for beginners and advanced gurus!<br>AWS offers the following definition of the DevOps model:<br>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization‚Äôs ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.<br><br><img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/06/cost-optimization-blog-header.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank"><strong></strong></a>Automating AWS Cost Optimization<br>
AWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this&nbsp;<a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">free, on-demand webinar</a>, you'll get an overview of AWS cost-optimization tools and strategies.<br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">Watch Now</a><br><br><br>How did these two groups get combined into one? And why would you want to combine development and IT operations?&nbsp;<br>Way back when (around 2007 or so), IT operations teams were separate from development teams. Then, people in the development and operations communities realized there was some issues in the way these teams were structured and how they interacted (or didn‚Äôt interact).<br>
<br>Development and IT operations teams had different objectives
<br>Development and operations teams had different performance indicators
<br>Development and operations teams were siloed physically in different rooms, buildings, or offices
<br>So they started talking. This gave birth to DevOps and the&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" target="_blank">DevOps engineer</a>.&nbsp;<br>Patrick Debois, often called the Godfather of DevOps, brought these groups together at&nbsp;<a data-tooltip-position="top" aria-label="https://devopsdays.org/" rel="noopener nofollow" class="external-link" href="https://devopsdays.org/" target="_blank">devopsdays</a>&nbsp;in 2009 where they discussed ways to bridge the gap between the two fields. Now, thousands of enterprises have adapted or are working toward adapting these practices.<br>These new approaches basically make ops everyone‚Äôs job to a degree, which makes&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" target="_blank">the future of ops jobs</a>&nbsp;very interesting.&nbsp;<br><br>
<br>Container&nbsp;- A lightweight, standalone, executable piece of software. It includes everything needed to run that software.
<br>Continuous delivery (CD)&nbsp;- The ongoing and manual or automatic release of software to production. It's aimed around new cycles. Some orgs release a new version with any changes.
<br>Continuous integration (CI)&nbsp;- The ongoing programming, building, and testing of code. This is done before turning it over to the continuous delivery (CD) system for deployment. With CD, it makes up&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">CI/CD</a>.
<br>Infrastructure as Code (IaC)&nbsp;- Defining infrastructure you want to use with programming code that can be understood by cloud services. These cloud services then create the infrastructure for you based on this code. This allows you to define standards and reuse code, which saves time.
<br>Microservices&nbsp;- Application architecture that is broken into multiple small pieces. Containers are often used to deploy microservices.
<br>Open source&nbsp;- Computer software code released under a license for free, like&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/linux-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/linux-this-month" target="_blank">Linux</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/kubernetes-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/kubernetes-this-month" target="_blank">Kubernetes</a>.
<br>Pipeline&nbsp;- A set of connected processes where the output of one process is the input for the next.
<br>Serverless&nbsp;- Running a service or microservice on cloud-based infrastructure without worrying about the servers running your code. You simply provide the code, and the cloud provider runs the code and gives you the results. See more on the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" target="_blank">Function as a Service&nbsp;(FaaS) services of AWS, Azure, and GCP</a>.
<br>Source code repository&nbsp;- A place to upload and track the history of your code, like&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">GitHub</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" target="_blank">AWS CodeCommit</a>.
<br>Unit testing&nbsp;- Breaking your application down into small parts to test that each features works.
<br><br>DevOps is all about producing higher-quality software faster and saving a lot of time and money. Here‚Äôs a more detailed breakdown of the benefits.<br>
<br>Speed&nbsp;- Release updates and new features faster, adapt to the changing market, and become more efficient.
<br>Rapid delivery&nbsp;- Increase deployment frequency and the pace of releases. Respond to customers' needs faster and build a competitive advantage.&nbsp;
<br>Reliability&nbsp;- Automatic testing is built-in. Ensures rollouts are of the highest quality and that you have less downtime because you build for stability and test before deploying.
<br>Scale&nbsp;- Implements automation. With the use of cloud and container technology, you can scale usage up and down and save money while you do so.
<br>Collaboration&nbsp;- Allow teams who used to be apart to work together. Workflows can be combined, inefficiency is reduced, and time is saved.
<br>Security&nbsp;- Allows infrastructure to be created by code, just like software. By doing this (instead of doing it manually), you can define policies to stay compliant no matter how many servers will be deployed.
<br>Want an overview of the benefits with a heavy dash of sarcasm? Check out our post&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" target="_blank">5 reasons to NOT move to DevOps</a>.<br><br>DevOps and agile can speed up the delivery of and increase the quality of software.&nbsp;Agile replaced the old-school waterfall model, but DevOps isn‚Äôt a replacement for agile.<br>DevOps exists because of agile, and the two can coexist and be used together. You don‚Äôt have to choose between DevOps and agile. Ideally, your organization will practice both.&nbsp;<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" target="_blank">top DevOps skills people are learning at ACG right now</a><br><br>Continuous integration and continuous development (or CI/CD) is a DevOps tactic ‚Äî a way to deliver apps to customers with the introduction of automation into the agile development process.&nbsp;<br>The&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" target="_blank">CI/CD pipeline</a>&nbsp;(as it's called when these practices are combined) has become an integral part of DevOps culture. A variety of tools and techniques are used for implementing such a pipeline. (More on those tools below.)<br>The CI/CD pipeline is supported by teams working in an agile way with either DevOps or a&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank">site reliability engineering (SRE)</a>&nbsp;approach.&nbsp;<br>Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" target="_blank"><em></em></a>why you need SRE skills to thrive in a serverless world&nbsp;and check out our&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank"><em></em></a>Site Reliability Engineering Concepts&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" target="_blank"><em></em></a>Google Cloud DevOps and SREs&nbsp;courses.<br><br>CI/CD pipelines smooth and speed up the flow of code from development through operations and QA into production by automating manual processes and increasing confidence in your releases.&nbsp;<br>This is the exact opposite of the waterfall release approach still practiced by many large organizations, where developers throw code ‚Äúover the wall‚Äù to ops, devs don‚Äôt get access to production, and ops doesn‚Äôt have much inside knowledge of how the code works.<br>CI/CD allows organizations to:<br>
<br>Build faster
<br>Decrease code review time
<br>Automatic
<br>Faster fault isolation
<br>Additional deployment features
<br>As mentioned before, you can be doing the right things and using the right tools and&nbsp;still&nbsp;not be agile or properly implementing DevOps. A broken and messy team can result in broken and messy CI/CD pipelines. You can almost&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" target="_blank">predict how your CI/CD pipeline looks</a>&nbsp;based on how your dev and ops teams work together.<br><br><img alt="Cloud Dictionary" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/12/Cloud-Dictionary-Resource-Image.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank"><strong></strong></a>Get the Cloud Dictionary of Pain<br>
Speaking cloud doesn‚Äôt have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">cloud guide</a>&nbsp;for succinct definitions of some of the most painful cloud terms.<br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">Get the Goods</a><br><br><br>Here are some of the most popular DevOps CI/CD tools you might use if working in an AWS cloud environment.<br>
<br>
Git&nbsp;- Free open-source version control system. It stores the entire history of your code that your developers will continue to push new code to It has a tiny footprint and fast performance. It also supports branching, allowing others to work on features separately without having developers trip over code.

<br>
GitHub&nbsp;-&nbsp;Online service to host Git repositories. GitHub is owned by Microsoft, which offers a similar DevOps tool called Azure DevOps. Whether you are a developer or not, working alone or in a team,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-azure-devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-azure-devops" target="_blank">Azure DevOps training</a>&nbsp;can help you organize the way you plan, create and deliver software. Get the lowdown on&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">Azure DevOps vs GitHub in this comparison of Microsoft DevOps tools</a>.

<br>
AWS CodeCommit&nbsp;-&nbsp;Fully managed server for hosting Git repositories. It‚Äôs secure and encrypted, highly available, and easily integrated with other AWS services.

<br>
AWS CodeBuild&nbsp;-&nbsp;Fully managed continuous integration code that complies code, runs tests, and produces software packages. When developers write code and push it into Git, CodeBuild compiles that code and produces a software package. It scales automatically and can process multiple builds concurrently.

<br>
AWS CodeDeploy&nbsp;-&nbsp;Fully managed deployment service that takes the software package (or files you updated and pushed into your Git repository) and deploys it to AWS or on-premises servers. It integrates well with AWS services and your own servers. It completely automates software deployment, eliminating error-prone manual operations.

<br>
AWS CodePipeline&nbsp;-&nbsp;Fully management continuous delivery service that helps you completely automate your release pipeline. It can automate the build, test, and deploy phases of software development and it can integrate with CodeCommit and GitHub, giving you the flexibility to use the source-control system of your choice. Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" target="_blank">automating CI/CD with AWS CodePipeline</a>.

<br>Other common DevOps tools include&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/docker-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/docker-deep-dive" target="_blank">Docker</a>, Jira,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/jenkins-fundamentals" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/jenkins-fundamentals" target="_blank">Jenkins</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/puppet-quick-start" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/puppet-quick-start" target="_blank">Puppet</a>, Chef,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/splunk-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/splunk-deep-dive" target="_blank">Splunk</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-ansible" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-ansible" target="_blank">Ansible</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/kubernetes-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/kubernetes-deep-dive" target="_blank">Kubernetes</a>, Bamboo, and&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/nagios-certified-professional-prep-course" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/nagios-certified-professional-prep-course" target="_blank">Nagios</a>.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/agile-vs-devops.html</link><guid isPermaLink="false">AWS/For Reading/Agile Vs DevOps.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Amazon Web Services In Plain English]]></title><description><![CDATA[ 
 <br>I was searching one liner meaning for aws services and I came across some interesting websites:<br>Meaning of AWS Services in one line

<br><a rel="noopener nofollow" class="external-link" href="https://adayinthelifeof.nl/2020/05/20/aws.html" target="_blank">https://adayinthelifeof.nl/2020/05/20/aws.html</a>
<br><a rel="noopener nofollow" class="external-link" href="https://expeditedsecurity.com/aws-in-plain-english/" target="_blank">https://expeditedsecurity.com/aws-in-plain-english/</a>

<br>Those I need 
EC2 - Virtual Private Servers<br>
Lambda - Programming Functions you can run but can costs a fortune<br>
S3 - File Storage (unmountable)<br>
EFS - Mountable Network Disks<br>
RDS - Managed Relational Database<br>
VPC - Virtual Private Network (consider as VLANs)<br>
DynamoDB - Large &amp; scalable non-relational database<br>
CloudFront - Content Delivery Network<br>
Route 53 - Managed domains names and records<br>
CloudWatch - Monitoring and Logs<br>
Autoscaling - Scale resources dynamically or manually<br>
CloudTrail - Spying on your users (Tracking Users activity)<br>
IAM - Users and their Permissions, policies and roles
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/amazon-web-services-in-plain-english.html</link><guid isPermaLink="false">AWS/For Reading/Amazon Web Services In Plain English.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Authentication vs. Authorization]]></title><description><![CDATA[ 
 <br><br>While often used interchangeably,&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate" target="_blank">authentication</a>&nbsp;and authorization represent fundamentally different functions. In this article, we compare and contrast the two to show how they protect applications in complementary ways.<br><br>In simple terms, authentication is the process of verifying who a user is, while authorization is the process of verifying what they have access to.<br>Comparing these processes to a real-world example, when you go through security in an airport, you show your ID to authenticate your identity. Then, when you arrive at the gate, you present your boarding pass to the flight attendant, so they can authorize you to board your flight and allow access to the plane.<br><br>Here's a quick overview of the differences between authentication and authorization:<br><br>In short, access to a resource is protected by both authentication and authorization. If you can't prove your identity, you won't be allowed into a resource. And even if you can prove your identity, if you are not authorized for that resource, you will still be denied access.<br>Auth0 has products and services for authentication, like&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate/passwordless/passwordless-with-universal-login" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate/passwordless/passwordless-with-universal-login" target="_blank">passwordless</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/secure/multi-factor-authentication" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/secure/multi-factor-authentication" target="_blank">multi-factor authentication</a>&nbsp;(MFA), and&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/authenticate/single-sign-on" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/authenticate/single-sign-on" target="_blank">Single-Sign On (SSO)</a>&nbsp;you can configure using&nbsp;Auth0 Dashboard&nbsp;or&nbsp;Management API. For authorization, Auth0 offers&nbsp;<a data-tooltip-position="top" aria-label="https://auth0.com/docs/manage-users/access-control/rbac" rel="noopener nofollow" class="external-link" href="https://auth0.com/docs/manage-users/access-control/rbac" target="_blank">role-based access control</a>&nbsp;(RBAC) or&nbsp;<a data-tooltip-position="top" aria-label="https://docs.fga.dev/fga" rel="noopener nofollow" class="external-link" href="https://docs.fga.dev/fga" target="_blank">fine grained authorization</a>&nbsp;FGA).]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/authorized-vs-authonticated.html</link><guid isPermaLink="false">AWS/For Reading/Authorized vs Authonticated.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[AWS EC2 Instance Types Grouped by Family]]></title><description><![CDATA[ 
 <br><br><br>
<br>M1 (e.g., m1.small, m1.medium, m1.large)

<br>Balanced CPU, memory, and network resources
<br>Suitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications


<br>M3 (e.g., m3.large, m3.xlarge, m3.2xlarge)

<br>Higher CPU performance and larger instance sizes
<br>Recommended for general-purpose instances with demanding CPU requirements


<br>M6g (e.g., m6g.medium, m6g.large, m6g.xlarge, m6g.2xlarge)

<br>Latest generation of general-purpose instances
<br>Provides a balance of CPU, memory, and network resources
<br>Suitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications


<br><br>
<br>C1 (e.g., c1.xlarge, c1.2xlarge, c1.4xlarge)

<br>High CPU-to-memory ratio
<br>Ideal for CPU-bound scale-out applications, such as front-end fleets for high-traffic web sites, on-demand batch processing, distributed analytics, web servers, video encoding, and high-performance science and engineering applications


<br>CC2 (e.g., cc2.8xlarge)

<br>Latest generation of compute-optimized instances
<br>Provides the lowest cost for CPU performance among all EC2 instance types
<br>Supports cluster networking and high core count (32 vCPUs)


<br><br>
<br>M2 (e.g., m2.xlarge, m2.2xlarge, m2.4xlarge)

<br>High memory-to-CPU ratio
<br>Suitable for memory-bound applications, such as high-performance databases, distributed cache, genome assembly, and larger deployments of SAP, Microsoft SharePoint, and other enterprise applications


<br>CR1 (e.g., cr1.16xlarge)

<br>Latest generation of memory-optimized instances
<br>Provides more memory (244 GiB), faster CPU (Intel Xeon E5-2670), and supports cluster networking


<br><br>
<br>HI1 (e.g., hi1.4xlarge)

<br>Optimized for very high random I/O performance and low cost per IOPS
<br>Ideal for transactional applications, such as large deployments of NoSQL databases like Cassandra and MongoDB


<br>HS1 (e.g., hs1.8xlarge)

<br>Optimized for very high storage density, low storage cost, and high sequential I/O performance
<br>Suitable for large-scale data warehouses, large always-on Hadoop clusters, and cluster file systems


<br><br>
<br>CG1 (e.g., cg1.4xlarge)

<br>High-performance computing instances with NVIDIA Tesla V100 GPUs
<br>Suitable for applications that require intense graphics processing, such as video encoding, scientific simulations, and machine learning


<br><br>
<br>T1 (e.g., t1.micro)

<br>Very low-cost instance option with a small amount of CPU resources
<br>May opportunistically increase CPU capacity in short bursts when additional cycles are available
<br>Suitable for lower-throughput applications, such as bastion hosts or administrative applications, or for low-traffic websites that require additional compute cycles from time to time


<br><br>AWS instance types support high-throughput workloads like video transcoding and scientific simulations&nbsp;through various combinations of CPU, memory, storage, and networking capacity. Here are some key features and instance types that cater to these workloads:<br>
<br>Compute-Optimized Instances: M5zn, C6g, C6gn, and Hpc6id instances are designed for applications that require extremely high single-thread performance and high throughput, such as:

<br>Gaming
<br>High Performance Computing (HPC)
<br>Simulation modeling for industries like automotive, aerospace, energy, and telecommunications


<br>Storage-Optimized Instances: I3en, H1, and I2 instances are ideal for big data processing clusters, MapReduce workloads, and distributed file systems, making them suitable for:

<br>Batch processing workloads
<br>Media transcoding
<br>High-performance web servers
<br>Scientific modeling


<br>High-Performance Computing (HPC) Instances: Hpc6a and Hpc6id instances are designed for tight-coupled, compute-intensive, and high-performance computing jobs, including:

<br>Computational fluid dynamics
<br>Weather forecasting
<br>Molecular dynamics
<br>Complex simulations and deep learning workloads


<br>GPU-Powered Instances: Vt1 instances, powered by NVIDIA V100 or T4 GPUs, are optimized for:

<br>Live transcoding video at up to 4K UHD resolutions
<br>Machine learning inference
<br>Graphics rendering
<br>Scientific simulations


<br>Network-Optimized Instances: R instances, with Enhanced Networking and EBS optimization, are suitable for workloads that require high networking throughput, such as:

<br>High-performance file systems
<br>Distributed web scale in-memory caches
<br>Real-time big data analytics
<br>Telco applications like 5G User Plane Function (UPF)


<br>AWS instance types offer a range of configurations to support high-throughput workloads, allowing you to choose the optimal instance for your specific needs. By leveraging these instance types, you can efficiently run demanding workloads like video transcoding and scientific simulations in the cloud.<br><br>When you launch an instance, the&nbsp;instance type&nbsp;that you specify determines the hardware of the host computer used for your instance. Each instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities. Select an instance type based on the requirements of the application or software that you plan to run on your instance.&nbsp;For more information about features and use cases, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance Types Details</a>.<br>Amazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such as the network and the disk subsystem, among instances. If each instance on a host computer tries to use as much of one of these shared resources as possible, each receives an equal share of that resource. However, when a resource is underused, an instance can consume a higher share of that resource while it's available.<br>Each instance type provides higher or lower minimum performance from a shared resource. For example, instance types with high I/O performance have a larger allocation of shared resources. Allocating a larger share of shared resources also reduces the variance of I/O performance. For most applications, moderate I/O performance is more than enough. However, for applications that require greater or more consistent I/O performance, consider an instance type with higher I/O performance.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#AvailableInstanceTypes" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#AvailableInstanceTypes" target="_blank">Available instance types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-specs" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-specs" target="_blank">Hardware specifications</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hypervisor-type" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hypervisor-type" target="_blank">Hypervisor type</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-virtualization-type" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-virtualization-type" target="_blank">AMI virtualization types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-types-processors" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-types-processors" target="_blank">Processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" target="_blank">Find an Amazon EC2 instance type</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-ec2-instance-type-recommendations.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-ec2-instance-type-recommendations.html" target="_blank">Get recommendations from EC2 instance type finder</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recommendations.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recommendations.html" target="_blank">Get EC2 instance recommendations from Compute Optimizer</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" target="_blank">Amazon EC2 instance type changes</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html" target="_blank">Burstable performance instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configure-gpu-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configure-gpu-instances.html" target="_blank">Performance acceleration with GPU instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html" target="_blank">Amazon EC2 Mac instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html" target="_blank">Amazon EBS-optimized instance types</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-optimize-cpu.html" target="_blank">CPU options for Amazon EC2 instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" target="_blank">AMD SEV-SNP for Amazon EC2 instances</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/processor_state_control.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/processor_state_control.html" target="_blank">Processor state control for Amazon EC2 Linux instances</a>
<br><br>Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.<br><br>Names are based on instance family, generation, processor family, capabilities, and size. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html" target="_blank">Naming conventions</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>To determine which instance types meet your requirements, such as supported Regions, compute resources, or storage resources, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-discovery.html" target="_blank">Find an Amazon EC2 instance type</a>&nbsp;and&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" target="_blank">Amazon EC2 instance type specifications</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>For detailed instance type specifications, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-instance-type-specifications.html" target="_blank">Specifications</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide. For pricing information, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/pricing/on-demand/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/pricing/on-demand/" target="_blank">Amazon EC2 On-Demand Pricing</a>.<br>To determine which instance type best meets your needs, we recommend that you launch an instance and use your own benchmark application. Because you pay by the instance second, it's convenient and inexpensive to test multiple instance types before making a decision. If your needs change, even after you make a decision, you can change the instance type later. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" target="_blank">Amazon EC2 instance type changes</a>.<br><br>Amazon EC2 supports the following hypervisors: Xen and Nitro.<br><br>
<br>General purpose:&nbsp;M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8g | T3 | T3a | T4g<br>

<br>Compute optimized:&nbsp;C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g<br>

<br>Memory optimized:&nbsp;R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6idn | R6in | R6id | R7a | R7g | R7gd | R7i | R7iz | R8g | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | X2gd | X2idn | X2iedn | X2iezn | X8g | z1d<br>

<br>Storage optimized:&nbsp;D3 | D3en | I3en | I4g | I4i | Im4gn | Is4gen<br>

<br>Accelerated computing:&nbsp;DL1 | DL2q | G4ad | G4dn | G5 | G5g | G6 | G6e | Gr6 | Inf1 | Inf2 | P3dn | P4d | P4de | P5 | P5e | Trn1 | Trn1n | VT1<br>

<br>High-performance computing:&nbsp;Hpc6a | Hpc6id | Hpc7a | Hpc7g<br>

<br>Previous generation:&nbsp;A1<br>

<br>For more information about the supported versions of Nitro hypervisor, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html#nitro-version-network-features" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html#nitro-version-network-features" target="_blank">Network feature support</a>&nbsp;in the&nbsp;Amazon EC2 Instance Types Guide.<br><br>
<br>General purpose: M1 | M2 | M3 | M4 | T1 | T2<br>

<br>Compute optimized: C1 | C3 | C4<br>

<br>Memory optimized: R3 | R4 | X1 | X1e<br>

<br>Storage optimized: D2 | H1 | I2 | I3<br>

<br>Accelerated computing: F1 | G3 | P2 | P3<br>

<br><br>The virtualization type of your instance is determined by the AMI that you use to launch it. Current generation instance types support hardware virtual machine (HVM) only. Some previous generation instance types support paravirtual (PV) and some AWS Regions support PV instances. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#virtualization_types" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html#virtualization_types" target="_blank">Virtualization types</a>.<br>For best performance, we recommend that you use an HVM AMI. In addition, HVM AMIs are required to take advantage of enhanced networking. HVM virtualization uses hardware-assist technology provided by the AWS platform. With HVM virtualization, the guest VM runs as if it were on a native hardware platform, except that it still uses PV network and storage drivers for improved performance.<br><br>EC2 instances support a variety of processors.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-processors" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#instance-hardware-processors" target="_blank">Intel processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#amd-epyc-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#amd-epyc-instances" target="_blank">AMD processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" target="_blank">AWS Graviton processors</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-trainium-instances" target="_blank">AWS Trainium</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-inferentia-instances" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#aws-inferentia-instances" target="_blank">AWS Inferentia</a>
<br><br>Amazon EC2 instances that run on Intel processors might include the following processor features. Not all instances that run on Intel processors support all of these processor features. For information about which features are available for each instance type, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance types</a>.<br>
<br>Intel AES New Instructions (AES-NI)&nbsp;‚Äî Intel AES-NI encryption instruction set improves upon the original Advanced Encryption Standard (AES) algorithm to provide faster data protection and greater security. All current generation EC2 instances support this processor feature.<br>

<br>Intel Advanced Vector Extensions (Intel AVX, Intel AVX2, and Intel AVX-512)&nbsp;‚Äî Intel AVX and Intel AVX2 are 256-bit, and Intel AVX-512 is a 512-bit instruction set extension designed for applications that are Floating Point (FP) intensive. Intel AVX instructions improve performance for applications like image and audio/video processing, scientific simulations, financial analytics, and 3D modeling and analysis. These features are only available on instances launched with HVM AMIs.<br>

<br>Intel Turbo Boost Technology&nbsp;‚Äî Intel Turbo Boost Technology processors automatically run cores faster than the base operating frequency.<br>

<br>Intel Deep Learning Boost (Intel DL Boost)&nbsp;‚Äî Accelerates AI deep learning use cases. The 2nd Gen Intel Xeon Scalable processors extend Intel AVX-512 with a new Vector Neural Network Instruction (VNNI/INT8) that significantly increases deep learning inference performance over previous generation Intel Xeon Scalable processors (with FP32) for image recognition/segmentation, object detection, speech recognition, language translation, recommendation systems, reinforcement learning, and more. VNNI may not be compatible with all Linux distributions.
  The following instances support VNNI:&nbsp;M5n,&nbsp;R5n,&nbsp;M5dn,&nbsp;M5zn,&nbsp;R5b,&nbsp;R5dn,&nbsp;D3,&nbsp;D3en, and&nbsp;C6i.&nbsp;C5&nbsp;and&nbsp;C5d&nbsp;instances support VNNI for only&nbsp;12xlarge,&nbsp;24xlarge, and&nbsp;metal&nbsp;instances.<br>

<br>Confusion can result from industry naming conventions for 64-bit CPUs. Chip manufacturer Advanced Micro Devices (AMD) introduced the first commercially successful 64-bit architecture based on the Intel x86 instruction set. Consequently, the architecture is widely referred to as AMD64 regardless of the chip manufacturer. Windows and several Linux distributions follow this practice. This explains why the internal system information on an instance running Ubuntu or Windows displays the CPU architecture as AMD64 even though the instances are running on Intel hardware.<br><br>Amazon EC2 instances that run on&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/amd/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/amd/" target="_blank">AMD EPYC</a>&nbsp;processors can help you optimize both cost and performance for your workloads. These instances might support the following processor features. Not all instances that run on AMD processors support all of these processor features. For information about which features are available for each instance type, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 Instance types</a>.<br>
<br>AMD Secure Memory Encryption (SME)<br>

<br>AMD Transparent Single Key Memory Encryption (TSME)<br>

<br>AMD Advanced Vector Extensions (AVX)<br>

<br>AMD Secure Encrypted Virtualization-Secure Nested Paging (<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sev-snp.html" target="_blank">SEV-SNP</a>)<br>

<br>Vector Neural Network Instructions (VNNI)<br>

<br>BFloat16<br>

<br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/graviton/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/graviton/" target="_blank">AWS Graviton</a>&nbsp;is a family of processors designed to deliver the best price performance for your workloads running on Amazon EC2 instances.<br>For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/graviton/getting-started" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/graviton/getting-started" target="_blank">Getting started with Graviton</a>.<br><br>Instances powered by&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/machine-learning/trainium/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/machine-learning/trainium/" target="_blank">AWS Trainium</a>&nbsp;are purpose built for high-performance, cost-effective deep learning training. You can use these instances to train natural language processing, computer vision, and recommender models used across a broad set of applications, such as speech recognition, recommendation, fraud detection, and image and video classification. Use your existing workflows in popular ML frameworks, such as PyTorch and TensorFlow.<br><br>Instances powered by&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/machine-learning/inferentia/" target="_blank">AWS Inferentia</a>&nbsp;are designed to accelerate machine learning. They provide high performance and low latency machine learning inference. These instances are optimized for deploying deep learning (DL) models for applications, such as natural language processing, object detection and classification, content personalization and filtering, and speech recognition.<br>There are a variety of ways that you can get started:<br>
<br>Use SageMaker, a fully-managed service that is the easiest way to get started with machine learning models. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html" target="_blank">Get Started with SageMaker</a>&nbsp;in the&nbsp;Amazon SageMaker Developer Guide.<br>

<br>Launch an Inf1 or Inf2 instance using the Deep Learning AMI. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia.html" target="_blank">AWS Inferentia with DLAMI</a>&nbsp;in the&nbsp;AWS Deep Learning AMIs Developer Guide.<br>

<br>Launch an Inf1 or Inf2 instance using your own AMI and install the&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/aws/aws-neuron-sdk" rel="noopener nofollow" class="external-link" href="https://github.com/aws/aws-neuron-sdk" target="_blank">AWS Neuron SDK</a>, which enables you to compile, run, and profile deep learning models for AWS Inferentia.<br>

<br>Launch a container instance using an Inf1 or Inf2 instance and an Amazon ECS-optimized AMI. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html" target="_blank">Amazon Linux 2 (Inferentia) AMIs</a>&nbsp;in the&nbsp;Amazon Elastic Container Service Developer Guide.<br>

<br>Create an Amazon EKS cluster with nodes running Inf1 instances. For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html" target="_blank">Inferentia support</a>&nbsp;in the&nbsp;Amazon EKS User Guide.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/aws-instance-types.html</link><guid isPermaLink="false">AWS/For Reading/AWS Instance Types.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[AWS Load Balancer Types]]></title><description><![CDATA[ 
 <br><br>AWS offers four types of load balancers:<br>]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/difference-between-load-balancers.html</link><guid isPermaLink="false">AWS/For Reading/Difference between Load balancers.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:19:08 GMT</pubDate></item><item><title><![CDATA[Git Fetch vs Git pull]]></title><description><![CDATA[ 
 <br><br>Git fetch and git pull are both Git commands used to retrieve update information from a remote repository. So, how do they differ? Git fetch downloads the changes from the remote repository to the local repository but does not make any changes to the current working directory. Since the changes are not merged into the local branch, you can check the changes from the remote repository without interrupting your current work. On the other hand, git pull retrieves the latest changes from the remote repository like git fetch, but it also automatically merges those changes into the current branch. In contrast to git fetch, git pull directly applies the changes from the remote repository to the local working directory.<br><br>The git fetch command retrieves the latest commit history from the remote repository, but it does not affect the local working directory. Even after fetching remote changes, they are not reflected in the local branch. It is primarily used when you want to retrieve the latest status from the remote repository and review the changes before they are reflected in the local repository. To apply the retrieved changes to the local branch, you need to manually run git merge or&nbsp;<a data-tooltip-position="top" aria-label="https://docs.gitlab.com/ee/topics/git/git_rebase.html" rel="noopener nofollow" class="external-link" href="https://docs.gitlab.com/ee/topics/git/git_rebase.html" target="_blank">git rebase</a>.<br><br>The git pull command combines&nbsp;git fetch&nbsp;and&nbsp;git merge&nbsp;(or&nbsp;git rebase) into a single command. This allows you to fetch changes from the remote repository and automatically integrate them into the current local branch.<br>While git fetch retrieves changes from the remote repository without applying them to the local branch, running git pull automatically integrates the changes from the remote repository into the local branch.<br>Git pull is suitable for quickly reflecting remote changes in the local branch, but it can lead to conflicts, so caution is needed, especially when working with multiple people.<br><br>Git fetch is a command used to retrieve the latest information from a remote repository. The retrieved information is not directly reflected in the local branch. Using git pull will reflect all remote branches, including incorrect or problematic ones, in the local branch.<br>When changes are made simultaneously on both remote and local branches, or when there are new users on the team, it is safer to use git fetch to retrieve the remote branch contents first and then perform merge or rebase.<br><br>Git pull is a command that performs more processes compared to git fetch. Git pull can perform both git fetch and additionally execute git merge or git rebase. For this reason, git pull is recommended when you want to quickly reflect changes from the remote repository in the local branch.<br><br><br>Git pull is a command that performs git fetch followed by git merge or git rebase. While git fetch does not affect the local repository, git pull automatically synchronizes changes from the remote repository with the local repository.<br><br>When executing git pull, there may be conflicts between remote and local changes. Merge conflicts are particularly likely to occur, so if conflicts arise, they need to be resolved manually. Additionally, using git pull --rebase allows you to incorporate the latest changes while performing a rebase.<br><br>Git fetch is useful for checking and retrieving the latest status of the remote repository. However, the changes retrieved are not automatically reflected in the local branch; git fetch is used to synchronize the local and remote repositories.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/git-fetch-vs-git-pull.html</link><guid isPermaLink="false">AWS/For Reading/Git Fetch vs Git pull.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 02 Jan 2025 07:06:05 GMT</pubDate></item><item><title><![CDATA[Host Tomcat Vanshit's Version]]></title><description><![CDATA[<a class="tag" href="https://notes.sarangwandile.xyz/?query=tag:git" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#git</a> 
 <br>.............................................Deploying Application..........................<br>1.Create an instance by using ec2-service and create database by RDS service.<br>
2.Create custom security group by adding SSH, custom tcp 8080 , for maria db add a port custom tcp 3306.<br>
3.Add this Security group to EC2 instance and Database instance.<br>
4.Login to instance.<br>
5.Search tomcat 9 on google and Download its zip file.<br>
6.Use curl -O <a rel="noopener nofollow" class="external-link" href="https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip" target="_blank">https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip</a>.<br>
7.unzip it<br>
8. Install java-17 package:-<br>
yum install java-17 -y<br>
9.Change directory :- cd apache-tomcat-9.0.97/bin<br>
10.bash catalina.sh start<br>
11.After this steps search with your instance public ip (For ex, 192.25.26.244:8080)<br>
12.Install git :- yum install git -y<br>
13.Run command :- <a href="https://notes.sarangwandile.xyz?query=tag:git" class="tag" target="_blank" rel="noopener nofollow">#git</a> clone <a rel="noopener nofollow" class="external-link" href="https://github.com/Pritam-Khergade/student-ui" target="_blank">https://github.com/Pritam-Khergade/student-ui</a><br>
14. 2 folders will be downloaded in bin dir.<br>
15.cd student-ui<br>
16.Install maven :- yum install maven -y<br>
17.mvn clean package.<br>
18.After cleaning package u should see build success notification.<br>
19.cd target<br>
20.Rename file :- mv studentapp-2.2-SNAPSHOT.war studentapp.war<br>
21.Move this folder in tomcat/webapps dir :-<br>
mv studentapp.war /root/apache-tomcat-9.0.97/webapps/<br>
22.Search with public ip u should see a registeration form :- 192.25.26.244:8080/studentapp<br>.................................................Database part......................................................<br>1.Install maria db in instance :-  yum install mariadb105 -y<br>
2.Connect to database :- MySQL -h van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com(endpoint) -u  -p <br>
3.After this u will get into database.<br>
4.Create DATABASE with name studentapp and tables,<br>CREATE DATABASE studentapp;<br>
use studentapp;<br>CREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,
	student_name VARCHAR(100) NOT NULL,
    student_addr VARCHAR(100) NOT NULL,
	student_age VARCHAR(3) NOT NULL,
	student_qual VARCHAR(20) NOT NULL,
	student_percent VARCHAR(10) NOT NULL,
	student_year_passed VARCHAR(10) NOT NULL,
	PRIMARY KEY (student_id)
);
<br>5.After this exit from database.<br>
6.vim /apache-tomcat-9.0.97/conf/context.xml<br>
7.Write this configuration :- <br>&lt;context&gt;
&lt;Resource name="jdbc/TestDB" auth="Container" type="javax.sql.DataSource"
           maxTotal="500" maxIdle="30" maxWaitMillis="1000"
           username="admin" password="Remote#123" driverClassName="com.mysql.jdbc.Driver"
           url="jdbc:mysql://van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com:3306/studentapp?useUnicode=yes&amp;amp;characterEncoding=utf8"/&gt;
&lt;context&gt;
<br>8.save and exit.<br>
9.After this download mysqlconnector.jar file from this link:-<br>
<a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar</a><br>
10.cd apache-tomcat-9.0.97/lib/<br>
11.Download this file in lib folder :- curl -O <a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar</a><br>
12.After this the data u entered in your form should be stored in this database.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/host-tomcat-vanshit's-version.html</link><guid isPermaLink="false">AWS/For Reading/Host Tomcat Vanshit's Version.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[How DNS Works]]></title><description><![CDATA[ 
 <br><br>The Domain Name System (DNS) is the phonebook of the Internet. Humans access information online through&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/what-is-a-domain-name/" target="_blank">domain names</a>, like nytimes.com or espn.com. Web browsers interact through&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/network-layer/internet-protocol/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/network-layer/internet-protocol/" target="_blank">Internet Protocol (IP)</a>&nbsp;addresses. DNS translates domain names to&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/what-is-my-ip-address/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/what-is-my-ip-address/" target="_blank">IP addresses</a>&nbsp;so browsers can load Internet resources.<br>Each device connected to the Internet has a unique IP address which other machines use to find the device. DNS servers eliminate the need for humans to memorize IP addresses such as 192.168.1.1 (in IPv4), or more complex newer alphanumeric IP addresses such as 2400:cb00:2048:1::c629:d7a2 (in IPv6).<br><img title="What is DNS?" alt="DNS - magnifying glass examines IP addresses, finds www.example.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png" referrerpolicy="no-referrer"><br><br>The process of DNS resolution involves converting a hostname (such as <a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a>) into a computer-friendly IP address (such as 192.168.1.1). An IP address is given to each device on the Internet, and that address is necessary to find the appropriate Internet device - like a street address is used to find a particular home. When a user wants to load a webpage, a translation must occur between what a user types into their web browser (example.com) and the machine-friendly address necessary to locate the example.com webpage.<br>In order to understand the process behind the DNS resolution, it‚Äôs important to learn about the different hardware components a DNS query must pass between. For the web browser, the DNS lookup occurs "behind the scenes" and requires no interaction from the user‚Äôs computer apart from the initial request.<br>Report<br>2023 GigaOm Radar for DNS Security<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/lp/gigaom-radar-dns-security/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/lp/gigaom-radar-dns-security/" target="_blank">Get the report</a><br>Report<br>Read the Q4 2023 DDoS Threat Landscape Report<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/lp/ddos-threat-report-2023-q4/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/lp/ddos-threat-report-2023-q4/" target="_blank">Get the report</a><br><br>
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">DNS recursor</a>&nbsp;- The recursor can be thought of as a librarian who is asked to go find a particular book somewhere in a library. The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client‚Äôs DNS query.
<br>Root nameserver&nbsp;- The&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/glossary/dns-root-server/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/glossary/dns-root-server/" target="_blank">root server</a>&nbsp;is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations.
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">TLD nameserver</a>&nbsp;- The top level domain server (<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/top-level-domain/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/top-level-domain/" target="_blank">TLD</a>) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is ‚Äúcom‚Äù).
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-server-types/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-server-types/" target="_blank">Authoritative nameserver</a>&nbsp;- This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.
<br>Fast &amp; Secure DNS<br>Free DNS included with any Cloudflare plan<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/plans/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/plans/" target="_blank">Start for free</a><br><br>Both concepts refer to servers (groups of servers) that are integral to the DNS infrastructure, but each performs a different role and lives in different locations inside the pipeline of a DNS query. One way to think about the difference is the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" target="_blank">recursive</a>&nbsp;resolver is at the beginning of the DNS query and the authoritative nameserver is at the end.<br><br>The recursive resolver is the computer that responds to a recursive request from a client and takes the time to track down the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/" target="_blank">DNS record</a>. It does this by making a series of requests until it reaches the authoritative DNS nameserver for the requested record (or times out or returns an error if no record is found). Luckily, recursive DNS resolvers do not always need to make multiple requests in order to track down the records needed to respond to a client;&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/what-is-caching/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/what-is-caching/" target="_blank">caching</a>&nbsp;is a data persistence process that helps short-circuit the necessary requests by serving the requested resource record earlier in the DNS lookup.<br><img title="DNS Record Request Sequence - Recursive Resolver" alt="DNS Record Request Sequence - DNS Recursive Resolver gets request from client" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/3NOmAzkfPG8FTA8zLc7Li8/8efda230b212c0de2d3bbcb408507b1e/dns_record_request_sequence_recursive_resolver.png" referrerpolicy="no-referrer"><br><br>Put simply, an authoritative DNS server is a server that actually holds, and is responsible for, DNS resource records. This is the server at the bottom of the DNS lookup chain that will respond with the queried resource record, ultimately allowing the web browser making the request to reach the IP address needed to access a website or other web resources. An authoritative nameserver can satisfy queries from its own data without needing to query another source, as it is the final source of truth for certain DNS records.<br><img title="DNS Record Request Sequence - Authoritative Nameserver" alt="DNS Record Request Sequence - DNS query reaches authoritative nameserver for cloudflare.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/6Cxvsc4NOvmU4pPkKbkDmP/a7588a4c8a3c187e9175a40fa1b3d548/dns_record_request_sequence_authoritative_nameserver.png" referrerpolicy="no-referrer"><br>It‚Äôs worth mentioning that in instances where the query is for a subdomain such as foo.example.com or&nbsp;<a data-tooltip-position="top" aria-label="https://blog.cloudflare.com/" rel="noopener nofollow" class="external-link" href="https://blog.cloudflare.com/" target="_blank">blog.cloudflare.com</a>, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain‚Äôs&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-cname-record/" target="_blank">CNAME record</a>.<br><img title="DNS Record Request Sequence - CNAME record for subdomain" alt="DNS Record Request Sequence - DNS query to CNAME record for subdomain blog.cloudflare.com" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1O1o3jhs0ztWsD00k8RLIJ/f33c1793a7e21cb92678c1f35ef1b245/dns_record_request_sequence_cname_subdomain.png" referrerpolicy="no-referrer"><br>There is a key difference between many DNS services and the one that Cloudflare provides. Different DNS recursive resolvers such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/cloudflare-vs-google-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/cloudflare-vs-google-dns/" target="_blank">Google DNS</a>, OpenDNS, and providers like Comcast all maintain data center installations of DNS recursive resolvers. These resolvers allow for quick and easy queries through optimized clusters of DNS-optimized computer systems, but they are fundamentally different than the nameservers hosted by Cloudflare.<br>Cloudflare maintains infrastructure-level nameservers that are integral to the functioning of the Internet. One key example is the&nbsp;<a data-tooltip-position="top" aria-label="https://blog.cloudflare.com/f-root/" rel="noopener nofollow" class="external-link" href="https://blog.cloudflare.com/f-root/" target="_blank">f-root server network</a>&nbsp;which Cloudflare is partially responsible for hosting. The F-root is one of the root level DNS nameserver infrastructure components responsible for the billions of Internet requests per day. Our&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/glossary/anycast-network/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/glossary/anycast-network/" target="_blank">Anycast network</a>&nbsp;puts us in a unique position to handle large volumes of DNS traffic without service interruption.<br><br>For most situations, DNS is concerned with a domain name being translated into the appropriate IP address. To learn how this process works, it helps to follow the path of a DNS lookup as it travels from a web browser, through the DNS lookup process, and back again. Let's take a look at the steps.<br>Note: Often DNS lookup information will be cached either locally inside the querying computer or remotely in the DNS infrastructure. There are typically 8 steps in a DNS lookup. When DNS information is cached, steps are skipped from the DNS lookup process which makes it quicker. The example below outlines all 8 steps when nothing is cached.<br><br>
<br>A user types ‚Äòexample.com‚Äô into a web browser and the query travels into the Internet and is received by a DNS recursive resolver.
<br>The resolver then queries a DNS root nameserver (.).
<br>The root server then responds to the resolver with the address of a Top Level Domain (TLD) DNS server (such as .com or .net), which stores the information for its domains. When searching for example.com, our request is pointed toward the .com TLD.
<br>The resolver then makes a request to the .com TLD.
<br>The TLD server then responds with the IP address of the domain‚Äôs nameserver, example.com.
<br>Lastly, the recursive resolver sends a query to the domain‚Äôs nameserver.
<br>The IP address for example.com is then returned to the resolver from the nameserver.
<br>The DNS resolver then responds to the web browser with the IP address of the domain requested initially.
<br>Once the 8 steps of the DNS lookup have returned the IP address for example.com, the browser is able to make the request for the web page:<br>
<br>The browser makes a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/" target="_blank">HTTP</a>&nbsp;request to the IP address.
<br>The server at that IP returns the webpage to be rendered in the browser (step 10).
<br><img title="Complete DNS Lookup and Webpage Query" alt="Complete DNS Lookup and Webpage Query - 10 steps" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/1NzaAqpEFGjqTZPAS02oNv/bf7b3f305d9c35bde5c5b93a519ba6d5/what_is_a_dns_server_dns_lookup.png" referrerpolicy="no-referrer"><br><br>The DNS resolver is the first stop in the DNS lookup, and it is responsible for dealing with the client that made the initial request. The resolver starts the sequence of queries that ultimately leads to a URL being translated into the necessary IP address.<br>Note: A typical uncached DNS lookup will involve both recursive and iterative queries.<br>It's important to differentiate between a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/what-is-recursive-dns/" target="_blank">recursive DNS</a>&nbsp;query and a recursive DNS resolver. The query refers to the request made to a DNS resolver requiring the resolution of the query. A DNS recursive resolver is the computer that accepts a recursive query and processes the response by making the necessary requests.<br><img title="DNS Recursive Query" alt="DNS recursive query goes from DNS client to DNS recursive resolver" src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/rOXBgctX2gaXNDqP5ktek/7086a97e00525159c6bd9318819c2287/dns_recursive_query.png" referrerpolicy="no-referrer"><br><br>In a typical DNS lookup three types of queries occur. By using a combination of these queries, an optimized process for DNS resolution can result in a reduction of distance traveled. In an ideal situation cached record data will be available, allowing a DNS name server to return a non-recursive query.<br><br>
<br>Recursive query&nbsp;- In a recursive query, a DNS client requires that a DNS server (typically a DNS recursive resolver) will respond to the client with either the requested resource record or an error message if the resolver can't find the record.
<br>Iterative query&nbsp;- in this situation the DNS client will allow a DNS server to return the best answer it can. If the queried DNS server does not have a match for the query name, it will return a referral to a DNS server authoritative for a lower level of the domain namespace. The DNS client will then make a query to the referral address. This process continues with additional DNS servers down the query chain until either an error or timeout occurs.
<br>Non-recursive query&nbsp;- typically this will occur when a DNS resolver client queries a DNS server for a record that it has access to either because it's authoritative for the record or the record exists inside of its cache. Typically, a DNS server will cache DNS records to prevent additional bandwidth consumption and load on upstream servers.
<br><br>The purpose of caching is to temporarily stored data in a location that results in improvements in performance and reliability for data requests. DNS caching involves storing data closer to the requesting client so that the DNS query can be resolved earlier and additional queries further down the DNS lookup chain can be avoided, thereby improving load times and reducing bandwidth/CPU consumption. DNS data can be cached in a variety of locations, each of which will store DNS records for a set amount of time determined by a&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/" target="_blank">time-to-live (TTL)</a>.<br><br>Modern web browsers are designed by default to cache DNS records for a set amount of time. The purpose here is obvious; the closer the DNS caching occurs to the web browser, the fewer processing steps must be taken in order to check the cache and make the correct requests to an IP address. When a request is made for a DNS record, the browser cache is the first location checked for the requested record.<br>In Chrome, you can see the status of your DNS cache by going to chrome://net-internals/#dns.<br><br>The operating system level DNS resolver is the second and last local stop before a DNS query leaves your machine. The process inside your operating system that is designed to handle this query is commonly called a ‚Äústub resolver‚Äù or DNS client. When a stub resolver gets a request from an application, it first checks its own cache to see if it has the record. If it does not, it then sends a DNS query (with a recursive flag set), outside the local network to a DNS recursive resolver inside the Internet service provider (ISP).<br>When the recursive resolver inside the ISP receives a DNS query, like all previous steps, it will also check to see if the requested host-to-IP-address translation is already stored inside its local persistence layer.<br>The recursive resolver also has additional functionality depending on the types of records it has in its cache:<br>
<br>If the resolver does not have the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/" target="_blank">A records</a>, but does have the&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/learning/dns/dns-records/dns-ns-record/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/dns/dns-records/dns-ns-record/" target="_blank">NS records</a>&nbsp;for the authoritative nameservers, it will query those name servers directly, bypassing several steps in the DNS query. This shortcut prevents lookups from the root and .com nameservers (in our search for example.com) and helps the resolution of the DNS query occur more quickly.
<br>If the resolver does not have the NS records, it will send a query to the TLD servers (.com in our case), skipping the root server.
<br>In the unlikely event that the resolver does not have records pointing to the TLD servers, it will then query the root servers. This event typically occurs after a DNS cache has been purged.
<br>Learn about what differentiates&nbsp;<a data-tooltip-position="top" aria-label="https://www.cloudflare.com/dns/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/dns/" target="_blank">Cloudflare DNS</a>&nbsp;from other DNS providers.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/how-dns-works.html</link><guid isPermaLink="false">AWS/For Reading/How DNS Works.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://cf-assets.www.cloudflare.com/slt3lc6tev37/5exJlPlwAT2kQCITQhrIi9/1f771294e218b64c0490e83968075766/what_is_dns.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[IAM Policies]]></title><description><![CDATA[ 
 <br><br><br><br>Based on the provided search results, IAM (Identity and Access Management) in AWS offers four types of policies:<br>
<br>Identity-Based Policies: Attached to an identity (user, group, or role), these policies define the permissions for that specific identity. They are used to control access to AWS resources and services.
<br>Resource-Based Policies: Attached to a specific resource (e.g., S3 bucket, EC2 instance), these policies define the permissions for that resource. They specify which identities have access to the resource and under what conditions.
<br>Service Control Policies (SCPs): These policies control the actions that can be performed by IAM users and roles across AWS services. They are used to enforce organizational compliance and governance.
<br>Permissions Boundary Policies: These policies define the maximum possible permissions that an IAM role can have. They are used to restrict the permissions granted to roles created by CI/CD pipelines or other automated processes.
<br>Here's a summary of the policies and their uses:<br><br>Note that Managed Policies and Customer Managed Policies are not separate policy types, but rather categories of policies. Managed Policies are created and managed by AWS, while Customer Managed Policies are created and managed by users in their AWS accounts. Inline Policies are embedded within an identity or resource, but are not a distinct policy type.<br>For more information, refer to the AWS documentation and the provided search results.<br><br>Based on the provided search results, AWS Identity and Access Management (IAM) supports the following types of users:<br>
<br>IAM User: An IAM user is an entity you create in AWS. This user is an identity within your AWS account that‚Äôs granted specific custom permissions. IAM users have a name and password used to sign in to the AWS Management Console.
<br>Root User: The root user is the initial sign-in identity for an AWS account. It has complete access to all AWS services and resources in the account. However, AWS strongly recommends not using the root user for everyday tasks and instead creating IAM users with limited permissions.
<br>IAM Identity Center User: IAM Identity Center users are members of AWS Organizations and can be granted access to multiple AWS accounts and applications through the AWS access portal. They can use their corporate credentials to sign in if their company has integrated Active Directory or another identity provider with IAM Identity Center.
<br>Federated Identity: With web identity federation, you can receive an authentication token and exchange it for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Federated identities sign in using an external identity provider.
<br>AWS Builder ID User: AWS Builder ID users are a type of user that represents an individual and allows access to AWS services and tools without an AWS account. They sign in to specific AWS services or tools using their AWS Builder ID profile.
<br>Note that these types of users are not mutually exclusive, and you may use a combination of them in your AWS environment. For example, you might create IAM users for your workforce and use federated identities for external partners or customers.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/iam-policies.html</link><guid isPermaLink="false">AWS/For Reading/IAM Policies.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Load Balancer]]></title><description><![CDATA[ 
 <br>Last Updated :&nbsp;04 Oct, 2024<br>When a website becomes extremely popular, the traffic on that website increases, and the load on a single server also increases. The concurrent traffic overloads the single server and the website becomes slower for the users. To meet the request of these high volumes of data and to return the correct response in a fast and reliable manner, we need to scale the server. This can be done by adding more servers to the network and distributing all the requests across these servers.&nbsp;<br><img alt="load-balancer-copy-(1)" src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp" referrerpolicy="no-referrer"><br>Important Topics for the Load Balancer ‚Äì System Design Interview Question<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-is-a-load-balancer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-is-a-load-balancer" target="_blank">What is a Load Balancer?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-will-happen-if-there-is-no-load-balancer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#what-will-happen-if-there-is-no-load-balancer" target="_blank">What will happen if there is No Load Balancer?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-load-balancer-works" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-load-balancer-works" target="_blank">How Load Balancer Works?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#where-are-load-balancers-typically-placed" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#where-are-load-balancers-typically-placed" target="_blank">Where Are Load Balancers Typically Placed?</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#types-of-load-balancers" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#types-of-load-balancers" target="_blank">Types of Load Balancers</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#load-balancing-algorithms" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#load-balancing-algorithms" target="_blank">Load Balancing Algorithms</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-to-use-load-balancing-during-system-design-interviews" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/?ref=header_ind#how-to-use-load-balancing-during-system-design-interviews" target="_blank">How to Use Load Balancing During System Design Interviews?</a>
<br><br>A load balancer is a networking device or software application that distributes and balances the incoming traffic among the servers to provide high availability, efficient utilization of servers, and high performance.<br>This article outlines various strategies for implementing load balancing and its importance in high-availability systems. To build a solid foundation in system design, the&nbsp;<a data-tooltip-position="top" aria-label="https://gfgcdn.com/tu/S4c/" rel="noopener nofollow" class="external-link" href="https://gfgcdn.com/tu/S4c/" target="_blank">*<strong></strong></a>*System Design Course**&nbsp;will help you understand the underlying principles and techniques for designing robust and scalable systems.<br>
<br>Load balancers are highly used in cloud computing domains, data centers, and large-scale web applications where traffic flow needs to be managed.&nbsp;
<br>The primary goal of using a load balancer is, not to overburden with huge incoming traffic which may lead to server crashes or high latency.
<br><br>Before understanding how a load balancer works, let‚Äôs understand what problem will occur without the load balancer through an example.<br>
Consider a scenario where an application is running on a single server and the client connects to that server directly without load balancing.<br>

<br><img alt="lb1" src="https://media.geeksforgeeks.org/wp-content/uploads/20240213114428/lb1.webp" referrerpolicy="no-referrer"><br>There are two main problems with this model:<br>
<br>**Single Point of Failure:**&nbsp;

<br>If the server goes down or something happens to the server the whole application will be interrupted and it will become unavailable for the users for a certain period. It will create a bad experience for users which is unacceptable for service providers.


<br>**Overloaded Servers:**&nbsp;

<br>There will be a limitation on the number of requests that a web server can handle. If the business grows and the number of requests increases the server will be overloaded.&nbsp;
<br>To solve the increasing number of requests we need to add a few more servers and we need to distribute the requests to the cluster of servers.&nbsp;


<br><br>Lets understand how Load Balancer works through the above discussed example:<br>To solve the above issue and to distribute the number of requests we can add a load balancer in front of the web servers and allow our services to handle any number of requests by adding any number of web servers in the network.&nbsp;<br>
<br>We can spread the request across multiple servers.&nbsp;
<br>For some reason, if one of the servers goes offline the service will be continued.
<br>Also, the latency on each request will go down because each server is not bottlenecked on RAM/Disk/CPU anymore.
<br><img alt="lb2" src="https://media.geeksforgeeks.org/wp-content/uploads/20240213114444/lb2.webp" referrerpolicy="no-referrer"><br>Load balancers minimize server response time and maximize throughput. Load balancer ensures high availability and reliability by sending requests only to online servers Load balancers do continuous health checks to monitor the server‚Äôs capability of handling the request. Depending on the number of requests or demand load balancers add or remove the number of servers.<br><br>Below is the image where a load balancer can be placed‚Ä¶<br><img alt="Where-are-Load-Balancer-placed-copy-(1)" src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140708/Where-are-Load-Balancer-placed-copy-(1).webp" referrerpolicy="no-referrer"><br>
<br>In between the client application/user and the server
<br>In between the server and the application/job servers
<br>In between the application servers and the cache servers
<br>In between the cache servers the database servers
<br><br><br>There are mainly three typers of load balancers based on configurations:<br><br>Software load balancers are applications or components that run on general-purpose servers. They are implemented in software, making them flexible and adaptable to various environments.<br><br>As the name suggests we use a physical appliance to distribute the traffic across the cluster of network servers. These load balancers are also known as Layer 4-7 Routers and these are capable of handling all kinds of HTTP, HTTPS, TCP, and UDP traffic.<br>
<br>These load balancers are expensive to acquire and configure, which is the reason a lot of service providers use them only as the first entry point for user requests.
<br>Later the internal software load balancers are used to redirect the data behind the infrastructure wall.&nbsp;
<br><br>A virtual load balancer is a type of load balancing solution implemented as a virtual machine (VM) or software instance within a virtualized environment ,such as data centers utilizing virtualization technologies like VMware, Hyper-V, or KVM.. It plays a crucial role in distributing incoming network traffic across multiple servers or resources to ensure efficient utilization of resources, improve response times, and prevent server overload.<br><br>There are mainly three typers of load balancers based on functions:<br><br>Layer-4 load balancers operate at the transport layer of the OSI model. They make forwarding decisions based on information available in network layer protocols (such as IP addresses and port numbers).&nbsp;<br><br>Layer-7 load balancers operate at the application layer of the OSI model. They can make load balancing decisions based on content, including information such as URLs, HTTP headers, or cookies.&nbsp;&nbsp;<br><br>GSLB stands for Global Server Load Balancer.&nbsp;This type of load balancer goes beyond the traditional local load balancing and is designed for distributing traffic across multiple data centers or geographically distributed servers. It takes into account factors such as server proximity, server health, and geographic location to intelligently distribute traffic across multiple locations.<br>**Further Read:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/layer-4l4-layer-7l7-and-gslb-load-balancers/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/layer-4l4-layer-7l7-and-gslb-load-balancers/" target="_blank">Layer-4 vs. Layer-7 vs. GSLB</a><br><br>We need a load-balancing algorithm to decide which request should be redirected to which backend server. The different system uses different ways to select the servers from the load balancer. Companies use varieties of load-balancing algorithm techniques depending on the configuration. Some of the common load-balancing algorithms are given below:<br><br>The Round Robin algorithm is a simple static load balancing approach in which requests are distributed across the servers in a sequential or rotational manner.&nbsp;It is easy to implement but it doesn‚Äôt consider the load already on a server so there is a risk that one of the servers receives a lot of requests and becomes overloaded.<br><br>The Weighted Round Robin algorithm is also a static load balancing approach which is much similar to the round-robin technique. The only difference is, that each of the resources in a list is provided a weighted score.&nbsp;Depending on the weighted score the request is distributed to these servers.&nbsp;<br><br>The Source IP Hash cLoad Balancing Algorithm is a static method used in network load balancing to distribute incoming requests among a set of servers based on the hash value of the source IP address. This algorithm aims to ensure that requests originating from the same source IP address are consistently directed to the same server.<br><br>The Least Connections algorithm is a dynamic load balancing approach that assigns new requests to the server with the fewest active connections. The idea is to distribute incoming workloads in a way that minimizes the current load on each server, aiming for a balanced distribution of connections across all available resources.&nbsp;<br><br>The Least Response method is a dynamic load balancing approach that aims to minimize response times by directing new requests to the server with the quickest response time.&nbsp;<br><br>In your system design interview, you‚Äôll be asked some sort of scalability question where you‚Äôll have to explain how load balancers help distribute the traffic and how it ensures scalability and availability of services in your application. The overall concept that you need to keep in mind from this article is‚Ä¶<br>
<br>A load balancer enables elastic scalability which improves the performance and throughput of data. It allows you to keep many copies of data (redundancy) to ensure the availability of the system. In case a server goes down or fails you‚Äôll have the backup to restore the services.&nbsp;
<br>Load balancers can be placed at any software layer.
<br>Many companies use both hardware and software to implement load balancers, depending on the different scale points in their system.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/load-balancer.html</link><guid isPermaLink="false">AWS/For Reading/Load Balancer.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20240214140633/load-balancer-copy-(1).webp"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Managed policies and inline policies]]></title><description><![CDATA[ 
 <br><br>When you set the permissions for an identity in IAM, you must decide whether to use an AWS managed policy, a customer managed policy, or an inline policy. The following topics provide more information about each of the types of identity-based policies and when to use them.<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies" target="_blank">AWS managed policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies" target="_blank">Customer managed policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#inline-policies" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#inline-policies" target="_blank">Inline policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-choosing-managed-or-inline.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-choosing-managed-or-inline.html" target="_blank">Choose between managed policies and inline policies</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-convert-inline-to-managed.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-convert-inline-to-managed.html" target="_blank">Convert an inline policy to a managed policy</a>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-deprecated.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-deprecated.html" target="_blank">Deprecated AWS managed policies</a>
<br><br>An&nbsp;AWS managed policy&nbsp;is a standalone policy that is created and administered by AWS. A&nbsp;standalone policy&nbsp;means that the policy has its own Amazon Resource Name (ARN) that includes the policy name. For example,&nbsp;arn:aws:iam::aws:policy/IAMReadOnlyAccess&nbsp;is an AWS managed policy. For more information about ARNs, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" target="_blank">IAM ARNs</a>. For a list of AWS managed policies for AWS services, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/policy-list.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/policy-list.html" target="_blank">AWS managed policies</a>.<br>AWS managed policies make it convenient for you to assign appropriate permissions to users, IAM groups, and roles. It is faster than writing the policies yourself, and includes permissions for many common use cases.<br>You cannot change the permissions defined in AWS managed policies. AWS occasionally updates the permissions defined in an AWS managed policy. When AWS does this, the update affects all principal entities (IAM users, IAM groups, and IAM roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API calls become available for existing services. For example, the AWS managed policy called&nbsp;ReadOnlyAccess&nbsp;provides read-only access to all AWS services and resources. When AWS launches a new service, AWS updates the&nbsp;ReadOnlyAccess&nbsp;policy to add read-only permissions for the new service. The updated permissions are applied to all principal entities that the policy is attached to.<br>Full access AWS managed policies: These define permissions for service administrators by granting full access to a service. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonDynamoDBFullAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonDynamoDBFullAccess.html" target="_blank">AmazonDynamoDBFullAccess</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/IAMFullAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/IAMFullAccess.html" target="_blank">IAMFullAccess</a><br>

<br>Power-user AWS managed policies: These provide full access to AWS services and resources, but do not allow managing users and IAM groups. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSCodeCommitPowerUser.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSCodeCommitPowerUser.html" target="_blank">AWSCodeCommitPowerUser</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSKeyManagementServicePowerUser.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSKeyManagementServicePowerUser.html" target="_blank">AWSKeyManagementServicePowerUser</a><br>

<br>Partial-access AWS managed policies: These provide specific levels of access to AWS services without allowing&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_understand-policy-summary-access-level-summaries.html#access_policies_access-level" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_understand-policy-summary-access-level-summaries.html#access_policies_access-level" target="_blank">permissions management</a>&nbsp;access level permissions. Examples include:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonMobileAnalyticsWriteOnlyAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonMobileAnalyticsWriteOnlyAccess.html" target="_blank">AmazonMobileAnalyticsWriteOnlyAccess</a><br>

<br><a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEC2ReadOnlyAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEC2ReadOnlyAccess.html" target="_blank">AmazonEC2ReadOnlyAccess</a><br>

<br>Job function AWS managed policies: These policies align closely with commonly used job functions in the IT industry and facilitate granting permissions for these job functions. One key advantage of using job function policies is that they are maintained and updated by AWS as new services and API operations are introduced. For example, the&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AdministratorAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AdministratorAccess.html" target="_blank">AdministratorAccess</a>&nbsp;job function provides full access and permissions delegation to every service and resource in AWS. We recommend that you use this policy only for the account administrator. For power users that require full access to every service except limited access to IAM and Organizations, use the&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/PowerUserAccess.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/aws-managed-policy/latest/reference/PowerUserAccess.html" target="_blank">PowerUserAccess</a>&nbsp;job function. For a list and descriptions of the job function policies, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html" target="_blank">AWS managed policies for job functions</a>.<br>The following diagram illustrates AWS managed policies. The diagram shows three AWS managed policies:&nbsp;AdministratorAccess,&nbsp;PowerUserAccess, and&nbsp;AWSCloudTrail_ReadOnlyAccess. Notice that a single AWS managed policy can be attached to principal entities in different AWS accounts, and to different principal entities in a single AWS account.<br><img alt="Diagram of AWS managed policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png" referrerpolicy="no-referrer"><br><br>You can create standalone policies in your own AWS account that you can attach to principal entities (IAM users, IAM groups, and IAM roles). You create these&nbsp;customer managed policies&nbsp;for your specific use cases, and you can change and update them as often as you like. Like AWS managed policies, when you attach a policy to a principal entity, you give the entity the permissions that are defined in the policy. When you update permissions in the policy, the changes are applied to all principal entities that the policy is attached to.<br>A great way to create a customer managed policy is to start by copying an existing AWS managed policy. That way you know that the policy is correct at the beginning and all you need to do is customize it to your environment.<br>The following diagram illustrates customer managed policies. Each policy is an entity in IAM with its own&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns" target="_blank">Amazon Resource Name (ARN)</a>&nbsp;that includes the policy name. Notice that the same policy can be attached to multiple principal entities‚Äîfor example, the same&nbsp;DynamoDB-books-app&nbsp;policy is attached to two different IAM roles.<br>For more information, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html" target="_blank">Define custom IAM permissions with customer managed policies</a><br><img alt="Diagram of customer managed policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-customer-managed-policies.diagram.png" referrerpolicy="no-referrer"><br><br>An inline policy is a policy created for a single IAM identity (a user, user group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity. You can create a policy and embed it in an identity, either when you create the identity or later. If a policy could apply to more than one entity, it‚Äôs better to use a managed policy.<br>The following diagram illustrates inline policies. Each policy is an inherent part of the user, group, or role. Notice that two roles include the same policy (the&nbsp;DynamoDB-books-app&nbsp;policy), but they are not sharing a single policy. Each role has its own copy of the policy.<br><img alt="Diagram of inline policies" src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-inline-policies.diagram.png" referrerpolicy="no-referrer"><br>]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/managed-policies-and-inline-policies.html</link><guid isPermaLink="false">AWS/For Reading/Managed policies and inline policies.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/policies-aws-managed-policies.diagram.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Types of Network Protocols and Their Uses]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;22 May, 2024<br>Network protocols are a set of rules that are responsible for the communication of data between various devices in the network. These protocols define guidelines and conventions for transmitting and receiving data, ensuring efficient and reliable data communication.<br><br>A network protocol is a set of rules that govern data communication between different devices in the network. It determines what is being communicated, how it is being communicated, and when it is being communicated. It permits connected devices to communicate with each other, irrespective of internal and structural differences.<br><br>It is essential to understand how devices communicate over a network by recognizing network protocols. The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/open-systems-interconnection-model-osi/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/open-systems-interconnection-model-osi/" target="_blank">Open Systems Interconnection (OSI),</a>&nbsp;the most widely used model, illustrates how computer systems interact with one another over a network. The communication mechanism between two network devices is shown by seven different layers in the OSI model. Every layer in the OSI model works based on different network protocols. At every layer, one or more protocols are there for network communication. To enable network-to-network connections, the Internet Protocol (IP), for instance, routes data by controlling information like the source and destination addresses of data packets. It is known as a network layer protocol.<br><br>In most cases, communication across a network like the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/internet-and-its-services/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/internet-and-its-services/" target="_blank">Internet</a>&nbsp;uses the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/layers-of-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/layers-of-osi-model/" target="_blank">OSI model</a>. The OSI model has a total of seven layers. Secured connections, network management, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-and-communication/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-and-communication/" target="_blank">network communication</a>&nbsp;are the three main tasks that the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/elements-of-network-protocol/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/elements-of-network-protocol/" target="_blank">network protocol</a>&nbsp;performs. The purpose of protocols is to link different devices.<br>The protocols can be broadly classified into three major categories:<br>
<br>Network Communication
<br>Network Management
<br>Network Security
<br><br>Communication protocols are really important for the functioning of a network. They are so crucial that it is not possible to have computer networks without them. These protocols formally set out the rules and formats through which data is transferred. These protocols handle syntax, semantics, error detection, synchronization, and authentication. Below mentioned are some network communication protocol:<br><br>It is a layer 7 protocol that is designed for transferring a hypertext between two or more systems.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/http-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/http-full-form/" target="_blank">HTTP</a>&nbsp;works on a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/client-server-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/client-server-model/" target="_blank">client-server model</a>, most of the data sharing over the web is done through using HTTP.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/tcp-ip-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/tcp-ip-model/" target="_blank">TCP</a>&nbsp;layouts a reliable stream delivery by using sequenced acknowledgment. It is a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-oriented-service/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-oriented-service/" target="_blank">connection-oriented</a>&nbsp;protocol i.e., it establishes a connection between applications before sending any&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-data/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-data/" target="_blank">data</a>. It is used for communicating over a network. It has many applications such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/email-protocols/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/email-protocols/" target="_blank">emails</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" target="_blank">FTP</a>, streaming media, etc.<br><br>It is a connectionless protocol that lay-out a basic but unreliable message service. It adds no&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/flow-control-in-data-link-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/flow-control-in-data-link-layer/" target="_blank">flow control</a>, reliability, or&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-error-recovery/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-error-recovery/" target="_blank">error-recovery</a>&nbsp;functions.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" target="_blank">UPD</a>&nbsp;is functional in cases where reliability is not required. It is used when we want faster transmission, for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-broadcast-and-multicast/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-broadcast-and-multicast/" target="_blank">multicasting and broadcasting</a>&nbsp;connections, etc.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/border-gateway-protocol-bgp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/border-gateway-protocol-bgp/" target="_blank">BGP</a>&nbsp;is a routing protocol that controls how packets pass through the router in an independent system one or more networks run by a single organization and connect to different networks. It connects the endpoints of a&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/lan-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/lan-full-form/" target="_blank">LAN</a>&nbsp;with other LANs and it also connects endpoints in different LANs to one another.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works/" target="_blank">ARP</a>&nbsp;is a protocol that helps in mapping logical addresses to the physical addresses acknowledged in a local network. For mapping and maintaining a correlation between these logical and physical addresses a table known as ARP cache is used.<br><br>It is a protocol through which data is sent from one host to another over the internet. It is used for addressing and routing data packets so that they can reach their destination.<br><br>it‚Äôs a protocol for network management and it‚Äôs used for the method of automating the process of configuring devices on IP networks. A&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/dynamic-host-configuration-protocol-dhcp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/dynamic-host-configuration-protocol-dhcp/" target="_blank">DHCP</a>&nbsp;server automatically assigns an&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-an-ip-address/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-an-ip-address/" target="_blank">IP address</a>&nbsp;and various other configurational changes to devices on a network so they can communicate with other IP networks. it also allows devices to use various services such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-time-protocol-ntp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-time-protocol-ntp/" target="_blank">NTP,</a>&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" target="_blank">DNS</a>, or any other protocol based on&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/differences-between-tcp-and-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/differences-between-tcp-and-udp/" target="_blank">TCP or UDP</a>.<br><br>These protocols assist in describing the procedures and policies that are used in monitoring, maintaining, and managing the computer network. These protocols also help in communicating these requirements across the network to ensure stable communication. Network management protocols can also be used for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-to-troubleshoot-common-http-error-codes/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-to-troubleshoot-common-http-error-codes/" target="_blank">troubleshooting</a>&nbsp;connections between a host and a client.<br><br>It is a layer 3 protocol that is used by network devices to forward operational information and error messages.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/internet-control-message-protocol-icmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/internet-control-message-protocol-icmp/" target="_blank">ICMP</a>&nbsp;is used for reporting congestions, network errors, diagnostic purposes, and timeouts.<br><br>It is a layer 7 protocol that is used for managing nodes on an IP network. There are three main components in the SNMP protocol i.e.,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" target="_blank">SNMP</a>&nbsp;agent, SNMP manager, and managed device. SNMP agent has the local knowledge of management details, it translates those details into a form that is compatible with the SNMP manager. The manager presents data acquired from SNMP agents, thus helping in monitoring network glitches, and network performance, and troubleshooting them.<br><br>It is a type of file retrieval protocol that provides downloadable files with some description for easy management, retrieving, and searching of files. All the files are arranged on a remote computer in a stratified manner. Gopher is an old protocol and it is not much used nowadays.<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp/" target="_blank">FTP</a>&nbsp;is a Client/server protocol that is used for moving files to or from a host computer, it allows users to download&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-program-and-file/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-program-and-file/" target="_blank">files, programs</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/web-pages/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/web-pages/" target="_blank">web pages</a>, and other things that are available on other services.<br><br>It is a protocol that a local mail client uses to get email messages from a remote email server over a TCP/IP connection. Email servers hosted by ISPs also use the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-pop3-post-office-protocol-version-3/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-pop3-post-office-protocol-version-3/" target="_blank">POP3</a>&nbsp;protocol to hold and receive emails intended for their users. Eventually, these users will use email client software to look at their mailbox on the remote server and to download their emails. After the email client downloads the emails, they are generally deleted from the servers.<br><br>It is a protocol that allows the user to connect to a remote computer program and to use it i.e., it is designed for remote connectivity.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-to-telnet/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-to-telnet/" target="_blank">Telnet</a>&nbsp;creates a connection between a host machine and a remote endpoint to enable a remote session.<br><br>These protocols secure the data in passage over a network. These protocols also determine how the network secures data from any unauthorized attempts to extract or review data. These protocols make sure that no unauthorized devices, users, or services can access the network data. Primarily, these protocols depend on encryption to secure data.<br><br>It is a network security protocol mainly used for protecting sensitive data and securing internet connections. SSL allows both server-to-server and client-to-server communication. All the data transferred through&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/secure-socket-layer-ssl/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/secure-socket-layer-ssl/" target="_blank">SSL</a>&nbsp;is encrypted thus stopping any unauthorized person from accessing it.<br><br>It is the secured version of HTTP. this protocol ensures secure communication between two computers where one sends the request through the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/browser-developer-tools/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/browser-developer-tools/" target="_blank">browser</a>&nbsp;and the other fetches the data from the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/web-server-and-its-type/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/web-server-and-its-type/" target="_blank">web server</a>.<br><br>It is a security protocol designed for&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/data-security/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/data-security/" target="_blank">data security</a>&nbsp;and privacy over the internet, its functionality is encryption, checking the integrity of data i.e., whether it has been tampered with or not, and authentication. It is generally used for encrypted communication between servers and web apps, like a web browser loading a website, it can also be used for encryption of messages, emails, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/voice-over-internet-protocol-voip/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/voice-over-internet-protocol-voip/" target="_blank">VoIP</a>.<br><br><br>
<br>ICMP protocol is used to retrieve message from the mail server. By using ICMP mail user can view and manage mails on his system.
<br><br>
<br>SIP is used in video, voice, and messaging application. This protocol is used to initiating, Managing, Terminating the session between two users while they are communicating.
<br><br>
<br>This protocol is used to forward audio, video over IP network. This protocol is used with SIP protocol to send audio, video at real-time.
<br><br>
<br>RAP is used in network management. It helps to user for accessing the nearest router for communication. RAP is less efficient as compared to&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-network-management-protocol-snmp/" target="_blank">SNMP</a>.
<br><br>
<br>It is used to implement VPN ( Virtual Private Network ). PPTP protocol append PPP frame in IP datagram for transmission through IP based network.
<br><br>
<br>TFTP is the simplified version of FTP. TFTP is also used to transfer file over internet
<br><br>
<br>RLP is used to assign the resource such as server, printer, or other devices over the internet to the user. It is used to locate the resource to the client for broadcast query.
<br><br><br>
Network protocol is a set of rules that shows how data is transferred between various devices connected to the same network.
<br><br>
The protocol used while accessing the internet are TCP and UDP.
<br><br>
IP Multicasting is defined as the types of group communication in which data is sent simultaneously to multiple computers.
<br><br>
Important protocols of transport layer include-

<br>Transmission Control Protocol (TCP).
<br>User Datagram Protocol (UDP).
<br>Stream Control Transmission Protocol (SCTP).

<br><br>
Some important protocols of Application Layer include-

<br>Hyper Text Transfer Protocol (HTTP).
<br>File transfer Protocol (FTP).
<br>Simple Mail Transfer protocol (SMTP).
<br>Domain Name System (DNS).

<br><br>
Full form of DHCP is Dynamic Host Configuration Protocol.
<br><br>
Function of DHCP is to assign IP address to device on a network automatically.
<br><br>
A <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/virtual-lan-vlan/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/virtual-lan-vlan/" target="_blank">virtual local area network (VLAN)</a> is a virtualized link that unites various network nodes and devices from several LANs into a single logical network.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/network-protocols.html</link><guid isPermaLink="false">AWS/For Reading/Network Protocols.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[What is OSI Model? ‚Äì Layers of OSI Model]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;11 Nov, 2024<br>The&nbsp;**OSI (Open Systems Interconnection)**&nbsp;Model is a set of rules that explains how different computer systems communicate over a network. OSI Model was developed by the&nbsp;**International Organization for Standardization (ISO)**. The OSI Model consists of 7 layers and each layer has specific functions and responsibilities.<br>This layered approach makes it easier for different devices and technologies to work together. OSI Model provides a clear structure for data transmission and managing network issues. The OSI Model is widely used as a reference to understand how network systems function.<br>In this article, we will discuss the OSI Model and each layer of the OSI Model in detail. We will also discuss the flow of data in the OSI Model and how the&nbsp;**OSI Model**&nbsp;is different from the&nbsp;**TCP/IP Model.**<br><img alt="OSI-Model" src="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif" referrerpolicy="no-referrer"><br>OSI Model<br>For those preparing for competitive exams like GATE, a strong understanding of networking concepts, including the OSI model, is crucial. To deepen your knowledge in this area and other key computer science topics, consider enrolling in the&nbsp;<a data-tooltip-position="top" aria-label="https://gfgcdn.com/tu/R89/" rel="noopener nofollow" class="external-link" href="https://gfgcdn.com/tu/R89/" target="_blank">*<strong></strong></a>*GATE CS Self-Paced course**&nbsp;. This course offers comprehensive coverage of the syllabus, helping you build a solid foundation for your exam preparation.<br><br>There are 7 layers in the OSI Model and each layer has its specific role in handling data. All the layers are mentioned below:<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/physical-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/physical-layer-in-osi-model" target="_blank">Physical Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/data-link-layer" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/data-link-layer" target="_blank">Data Link Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/network-layer-in-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/network-layer-in-osi-model/" target="_blank">Network Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/transport-layer-in-osi-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/transport-layer-in-osi-model/" target="_blank">Transport Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/session-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/session-layer-in-osi-model" target="_blank">Session Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/presentation-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/presentation-layer-in-osi-model" target="_blank">Presentation Layer</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/application-layer-in-osi-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/application-layer-in-osi-model" target="_blank">Application Layer</a>
<br><br>The lowest layer of the OSI reference model is the&nbsp;**Physical Layer**. It is responsible for the actual physical connection between the devices. The physical layer contains information in the form of&nbsp;**bits.**&nbsp;Physical Layer is responsible for transmitting individual bits from one node to the next. When receiving data, this layer will get the signal received and convert it into 0s and 1s and send them to the Data Link layer, which will put the frame back together. Common physical layer devices are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-network-hub-and-how-it-works/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-network-hub-and-how-it-works/" target="_blank">Hub</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/repeaters-in-computer-network/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/repeaters-in-computer-network/" target="_blank">Repeater</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-modem-and-router/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-modem-and-router/" target="_blank">Modem</a>, and&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/types-of-ethernet-cable/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/types-of-ethernet-cable/" target="_blank">Cables</a>.<br><img alt="physical-layer-in-OSI" src="https://media.geeksforgeeks.org/wp-content/uploads/20241015103017414021/physical-layer-768.png" referrerpolicy="no-referrer"><br>Physical Layer<br><br>
<br>**Bit Synchronization:**&nbsp;The physical layer provides the synchronization of the bits by providing a clock. This clock controls both sender and receiver thus providing synchronization at the bit level.
<br>**Bit Rate Control:**&nbsp;The Physical layer also defines the transmission rate i.e. the number of bits sent per second.
<br>**Physical Topologies:**&nbsp;Physical layer specifies how the different, devices/nodes are arranged in a network i.e.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-bus-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-bus-topology/" target="_blank">bus topology</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-star-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantages-and-disadvantages-of-star-topology/" target="_blank">star topology</a>, or&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/advantage-and-disadvantage-of-mesh-topology/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/advantage-and-disadvantage-of-mesh-topology/" target="_blank">mesh topology</a>.
<br>**Transmission Mode:**&nbsp;Physical layer also defines how the data flows between the two connected devices. The various transmission modes possible are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-simplex-half-duplex-and-full-duplex-transmission-modes/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-simplex-half-duplex-and-full-duplex-transmission-modes/" target="_blank">Simplex, half-duplex and full-duplex</a>.
<br><br>The data link layer is responsible for the node-to-node delivery of the message. The main function of this layer is to make sure data transfer is error-free from one node to another, over the physical layer. When a packet arrives in a network, it is the responsibility of the DLL to transmit it to the Host using its&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/mac-address-in-computer-network" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/mac-address-in-computer-network" target="_blank">MAC address</a>. Packet in the Data Link layer is referred to as&nbsp;**Frame.**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-switch-and-bridge/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-switch-and-bridge/" target="_blank">Switches and Bridges</a>&nbsp;are common Data Link Layer devices.<br>The Data Link Layer is divided into two sublayers:<br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/logical-link-control-llc-protocol-data-unit" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/logical-link-control-llc-protocol-data-unit" target="_blank">Logical Link Control (LLC)</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-of-mac-address-in-computer-network" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-of-mac-address-in-computer-network" target="_blank">Media Access Control (MAC)</a>
<br>The packet received from the Network layer is further divided into frames depending on the frame size of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/nic-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/nic-full-form/" target="_blank">*<strong></strong></a>*NIC(Network Interface Card)**. DLL also encapsulates Sender and Receiver‚Äôs MAC address in the header.<br>The Receiver‚Äôs MAC address is obtained by placing an&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/how-address-resolution-protocol-arp-works" target="_blank">ARP(Address Resolution Protocol)</a>&nbsp;request onto the wire asking ‚ÄúWho has that IP address?‚Äù and the destination host will reply with its MAC address.<br><br>
<br>**Framing:**&nbsp;Framing is a function of the data link layer. It provides a way for a sender to transmit a set of bits that are meaningful to the receiver. This can be accomplished by attaching special bit patterns to the beginning and end of the frame.
<br>**Physical Addressing:**&nbsp;After creating frames, the Data link layer adds physical addresses (**MAC addresses**) of the sender and/or receiver in the header of each frame.
<br>**Error Control:**&nbsp;The data link layer provides the mechanism of error control in which it detects and retransmits damaged or lost frames.
<br>**Flow Control:**&nbsp;The data rate must be constant on both sides else the data may get corrupted thus, flow control coordinates the amount of data that can be sent before receiving an acknowledgment.
<br>**Access Control:**&nbsp;When a single communication channel is shared by multiple devices, the MAC sub-layer of the data link layer helps to determine which device has control over the channel at a given time.
<br><br>The network layer works for the transmission of data from one host to the other located in different networks. It also takes care of packet routing i.e. selection of the shortest path to transmit the packet, from the number of routes available. The sender and receiver‚Äôs&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-an-ip-address" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-an-ip-address" target="_blank">IP address</a>&nbsp;are placed in the header by the network layer. Segment in the Network layer is referred to as&nbsp;**Packet.**&nbsp;Network layer is implemented by networking devices such as&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-router-and-switch/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-router-and-switch/" target="_blank">routers and switches</a>.<br><br>
<br>**Routing:**&nbsp;The network layer protocols determine which route is suitable from source to destination. This function of the network layer is known as routing.
<br>**Logical Addressing:**&nbsp;To identify each device inter-network uniquely, the network layer defines an addressing scheme. The sender and receiver‚Äôs IP addresses are placed in the header by the network layer. Such an address distinguishes each device uniquely and universally.
<br><br>The transport layer provides services to the application layer and takes services from the network layer. The data in the transport layer is referred to as&nbsp;**Segments**. It is responsible for the end-to-end delivery of the complete message. The transport layer also provides the acknowledgment of the successful data transmission and re-transmits the data if an error is found. Protocols used in Transport Layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-transmission-control-protocol-tcp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-transmission-control-protocol-tcp/" target="_blank">TCP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/user-datagram-protocol-udp/" target="_blank">UDP</a>&nbsp;&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-netbios-enumeration/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-netbios-enumeration/" target="_blank">NetBIOS</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/pptp-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/pptp-full-form/" target="_blank">PPTP</a>.<br>**At the sender‚Äôs side**, the transport layer receives the formatted data from the upper layers, performs&nbsp;**Segmentation**, and also implements&nbsp;**Flow and error control**&nbsp;to ensure proper data transmission. It also adds Source and Destination&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-ports-in-networking" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-ports-in-networking" target="_blank">port number</a>&nbsp;in its header and forwards the segmented data to the Network Layer.<br>
<br>Generally, this destination port number is configured, either by default or manually. For example, when a web application requests a web server, it typically uses port number 80, because this is the default port assigned to web applications. Many applications have default ports assigned.
<br>**At the Receiver‚Äôs side,**&nbsp;Transport Layer reads the port number from its header and forwards the Data which it has received to the respective application. It also performs sequencing and reassembling of the segmented data.<br><br>
<br>**Segmentation and Reassembly:**&nbsp;This layer accepts the message from the (session) layer, and breaks the message into smaller units. Each of the segments produced has a header associated with it. The transport layer at the destination station reassembles the message.
<br>**Service Point Addressing:**&nbsp;To deliver the message to the correct process, the transport layer header includes a type of address called service point address or port address. Thus by specifying this address, the transport layer makes sure that the message is delivered to the correct process.
<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-oriented-service" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-oriented-service" target="_blank">Connection-Oriented Service</a>
<br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/connection-less-service" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/connection-less-service" target="_blank">Connectionless Service</a>
<br><br>Session Layer in the OSI Model is responsible for the establishment of connections, management of connections, terminations of sessions between two devices. It also provides authentication and security. Protocols used in the Session Layer are NetBIOS, PPTP.<br><br>
<br>**Session Establishment, Maintenance, and Termination:**&nbsp;The layer allows the two processes to establish, use, and terminate a connection.
<br>**Synchronization:**&nbsp;This layer allows a process to add checkpoints that are considered synchronization points in the data. These synchronization points help to identify the error so that the data is re-synchronized properly, and ends of the messages are not cut prematurely and data loss is avoided.
<br>**Dialog Controller:**&nbsp;The session layer allows two systems to start communication with each other in half-duplex or full-duplex.
<br>**Example**<br>Let us consider a scenario where a user wants to send a message through some Messenger application running in their browser. The ‚Äú**Messenger**‚Äù here acts as the application layer which provides the user with an interface to create the data. This message or so-called&nbsp;**Data**&nbsp;is compressed, optionally encrypted (if the data is sensitive), and converted into bits (0‚Äôs and 1‚Äôs) so that it can be transmitted.<br><img alt="Communication in Session Layer" src="https://media.geeksforgeeks.org/wp-content/uploads/20230405124947/communication.jpg" referrerpolicy="no-referrer"><br>Communication in Session Layer<br><br>The presentation layer is also called the&nbsp;**Translation layer**. The data from the application layer is extracted here and manipulated as per the required format to transmit over the network. Protocols used in the Presentation Layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-jpeg-and-png/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-jpeg-and-png/" target="_blank">JPEG</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/mpeg-full-form/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/mpeg-full-form/" target="_blank">MPEG</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/what-is-a-gif-file/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-a-gif-file/" target="_blank">GIF</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-secure-socket-layer-ssl-and-transport-layer-security-tls/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-secure-socket-layer-ssl-and-transport-layer-security-tls/" target="_blank">TLS/SSL</a>, etc.<br><br>
<br>**Translation:**&nbsp;For example,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-ascii-and-ebcdic" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/difference-between-ascii-and-ebcdic" target="_blank">ASCII to EBCDIC</a>.
<br>**Encryption/ Decryption:**&nbsp;Data encryption translates the data into another form or code. The encrypted data is known as the ciphertext and the decrypted data is known as plain text. A key value is used for encrypting as well as decrypting data.
<br>**Compression:**&nbsp;Reduces the number of bits that need to be transmitted on the network.
<br><br>At the very top of the OSI Reference Model stack of layers, we find the Application layer which is implemented by the network applications. These applications produce the data to be transferred over the network. This layer also serves as a window for the application services to access the network and for displaying the received information to the user. Protocols used in the Application layer are&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/simple-mail-transfer-protocol-smtp/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/simple-mail-transfer-protocol-smtp/" target="_blank">SMTP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/file-transfer-protocol-ftp-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/file-transfer-protocol-ftp-in-application-layer/" target="_blank">FTP</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/domain-name-system-dns-in-application-layer/" target="_blank">DNS</a>, etc.<br><img alt="application-layer-in-OSI" src="https://media.geeksforgeeks.org/wp-content/uploads/20241015103102290937/application-layer.gif" referrerpolicy="no-referrer"><br>Application Layer<br><br>The main functions of the application layer are given below.<br>
<br>**Network Virtual Terminal(NVT):**&nbsp;It allows a user to log on to a remote host.
<br>**File Transfer Access and Management(FTAM):**&nbsp;This application allows a user to access files in a remote host, retrieve files in a remote host, and manage or control files from a remote computer.
<br>**Mail Services:**&nbsp;Provide email service.
<br>**Directory Services:**&nbsp;This application provides distributed database sources and access for global information about various objects and services.
<br><br>When we transfer information from one device to another, it travels through 7 layers of OSI model. First data travels down through 7 layers from the sender‚Äôs end and then climbs back 7 layers on the receiver‚Äôs end.<br>Data flows through the OSI model in a step-by-step process:<br>
<br>**Application Layer:**&nbsp;Applications create the data.
<br>**Presentation Layer:**&nbsp;Data is formatted and encrypted.
<br>**Session Layer:**&nbsp;Connections are established and managed.
<br>**Transport Layer:**&nbsp;Data is broken into segments for reliable delivery.
<br>**Network Layer**: Segments are packaged into packets and routed.
<br>**Data Link Layer:**&nbsp;Packets are framed and sent to the next device.
<br>**Physical Layer:**&nbsp;Frames are converted into bits and transmitted physically.
<br>Each layer adds specific information to ensure the data reaches its destination correctly, and these steps are reversed upon arrival.<br><img alt="Data Flow in OSI model" src="https://media.geeksforgeeks.org/wp-content/uploads/20210220204638/cn1.png" referrerpolicy="no-referrer"><br>We can understand how data flows through OSI Model with the help of an example mentioned below.<br>Let us suppose,&nbsp;**Person A**&nbsp;sends an e-mail to his friend&nbsp;**Person B**.<br>**Step 1: Person A**&nbsp;interacts with e-mail application like&nbsp;**Gmail**,&nbsp;**outlook**, etc. Writes his email to send. (This happens at&nbsp;**Application Layer**).<br>**Step 2: At Presentation Layer,**&nbsp;Mail application prepares for data transmission like encrypting data and formatting it for transmission.<br>**Step 3: At Session Layer,**&nbsp;There is a connection established between the sender and receiver on the internet.<br>**Step 4: At Transport Layer**, Email data is broken into smaller segments. It adds sequence number and error-checking information to maintain the reliability of the information.<br>**Step 5: At Network Layer,**&nbsp;Addressing of packets is done in order to find the best route for transfer.<br>**Step 6: At Data Link Layer, d**ata packets are encapsulated into frames, then MAC address is added for local devices and then it checks for error using error detection.<br>**Step 7: At Physical Layer,**&nbsp;Frames are transmitted in the form of electrical/ optical signals over a physical network medium like ethernet cable or WiFi.<br>After the email reaches the receiver i.e.&nbsp;**Person B**, the process will reverse and decrypt the e-mail content. At last, the email will be shown on&nbsp;**Person B**&nbsp;email client.<br><br><br><br>The OSI Model matters because it provides the user a clear structure of ‚Äúhow the data moves in the network?‚Äù. As the OSI Model consists of 7 layers, each layer has its specific role, and due to which it helps in understanding, identifying and solving the complex network problems easily by focusing on one of the layers not the entire network.<br>As the modern Internet does not prefer the OSI Model, but still, the OSI Model is still very helpful for solving network problems. It helps people understanding network concepts very easily.<br><br><br><img alt="OSI-vs-TCP/IP" src="https://media.geeksforgeeks.org/wp-content/uploads/20240615134832/OSI-vs-TCP.png" referrerpolicy="no-referrer"><br>OSI vs TCP/IP<br><br>The OSI Model defines the communication of a computing system into 7 different layers. Its advantages include:<br>
<br>It divides network communication into 7 layers which makes it easier to understand and troubleshoot.
<br>It standardizes network communications, as each layer has fixed functions and protocols.
<br>Diagnosing network problems is easier with the&nbsp;**OSI model.**
<br>It is easier to improve with advancements as each layer can get updates separately.
<br><br>
<br>The OSI Model has seven layers, which can be complicated and hard to understand for beginners.
<br>In real-life networking, most systems use a simpler model called the Internet protocol suite (TCP/IP), so the OSI Model is not always directly applicable.
<br>Each layer in the OSI Model adds its own set of rules and operations, which can make the process more time-consuming and less efficient.
<br>The OSI Model is more of a theoretical framework, meaning it‚Äôs great for understanding concepts but not always practical for implementation.
<br><br>In conclusion, the OSI (Open Systems Interconnection) model helps us understand how data moves in networks. It consists of seven distinct layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. Each layer has specific responsibilities and interacts with the layers directly above and below it. Since it is a conceptual model, but the OSI framework is still widely used to troubleshoot and understand networking issues.<br><br><br>
No, OSI layers do not work independently. Each layer depends on the services provided by the layer below it and, in turn, provides services to the layer above it. This layered approach ensures that data is transmitted smoothly from the source to the destination.
<br><br>
By breaking down communication into layers, the OSI Model helps network administrators isolate problems more easily.
<br><br>
If a particular OSI layer fails, data transmission may be disrupted or fail entirely. Network administrator will check layer by layer to identify and resolve the issue, make sure that each layer is functioning correctly or not.
<br><br>
The Domain Name System (DNS) operates at Layer 7 (Application Layer). It translates domain names into IP addresses, facilitating communication between users and services across the network.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/osi-model.html</link><guid isPermaLink="false">AWS/For Reading/OSI Model.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20241111182857579134/OSI-Model.gif"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Software Development Life Cycle (SDLC)]]></title><description><![CDATA[ 
 <br><br>Last Updated :&nbsp;20 Nov, 2024<br>**Software development life cycle (SDLC) is a structured process that is used to design, develop, and test good-quality software.**&nbsp;SDLC, or software development life cycle, is a methodology that defines the entire procedure of software development step-by-step. The&nbsp;**goal of the SDLC life cycle model**&nbsp;is to deliver high-quality, maintainable software that meets the user‚Äôs requirements. SDLC in software engineering models outlines the plan for each stage so that each stage of the software development model can perform its task efficiently to deliver the software at a low cost within a given time frame that meets users requirements. In this article we will see Software Development Life Cycle (SDLC) in detail.<br><br>**SDLC is a process followed for software building within a software organization.**&nbsp;SDLC consists of a precise plan that describes how to develop, maintain, replace, and enhance specific software. The life cycle defines a method for improving the quality of software and the all-around development process. &nbsp;<br><img alt="SDLC" src="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg" referrerpolicy="no-referrer"><br><br>SDLC specifies the task(s) to be performed at various stages by a software engineer or developer. It ensures that the end product is able to meet the customer‚Äôs expectations and fits within the overall budget. Hence, it‚Äôs vital for a software developer to have prior knowledge of this software development process.&nbsp;SDLC is a collection of these six stages, and the stages of SDLC are as follows:<br><img alt="Stages of the Software Development Life Cycle Model SDLC" src="https://media.geeksforgeeks.org/wp-content/uploads/20231220112830/6-Stages-of-Software-Development-Life-Cycle.jpg" referrerpolicy="no-referrer"><br>Software Development Life Cycle Model SDLC Stages<br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" target="_blank">*<strong></strong></a>*SDLC Model**&nbsp;**involves six phases or stages**&nbsp;while developing any software.<br><br>Planning is a crucial step in everything, just as in&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-development" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-development" target="_blank">software development</a>. In this same stage,&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/activities-involved-in-software-requirement-analysis" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/activities-involved-in-software-requirement-analysis" target="_blank">requirement analysis</a>&nbsp;is also performed by the developers of the organization. This is attained from customer inputs, and sales department/market surveys.&nbsp;<br>The information from this analysis forms the building blocks of a basic project. The quality of the project is a result of planning. Thus, in this stage, the basic project is designed with all the available information.<br><img alt="Stage 1" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094307/1.jpg" referrerpolicy="no-referrer"><br>Stage-1 : Planning and Requirement Analysis<br><br>In this stage, all the requirements for the target software are specified. These requirements get approval from customers, market analysts, and stakeholders.&nbsp;<br>
This is fulfilled by utilizing SRS (Software Requirement Specification). This is a sort of document that specifies all those things that need to be defined and created during the entire project cycle.&nbsp;<br><img alt="Stage-2: Defining Requirements" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094400/2.jpg" referrerpolicy="no-referrer"><br>Stage-2 : Defining Requirements<br><br><a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-requirement-specification-srs-format" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-requirement-specification-srs-format" target="_blank">SRS</a>&nbsp;is a reference for software designers to come up with the best architecture for the software. Hence, with the requirements defined in SRS, multiple designs for the product architecture are present in the Design Document Specification (DDS).&nbsp;<br>This DDS is assessed by market analysts and stakeholders. After evaluating all the possible factors, the most practical and logical design is chosen for development.<br><img alt="Stage-3: Designing Architecture" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094458/3.jpg" referrerpolicy="no-referrer"><br>Stage 3: Design<br><br>At this stage, the fundamental development of the product starts. For this, developers use a specific programming code as per the design in the DDS. Hence, it is important for the coders to follow the protocols set by the association. Conventional programming tools like compilers, interpreters, debuggers, etc. are also put into use at this stage. Some popular languages like C/C++, Python, Java, etc. are put into use as per the software regulations.&nbsp;<br><img alt="Stage-4: Developing Product" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094538/4.jpg" referrerpolicy="no-referrer"><br>Stage 4: Development<br><br>After the development of the product, testing of the software is necessary to ensure its smooth execution. Although, minimal testing is conducted at every stage of SDLC.&nbsp;Therefore, at this stage, all the probable flaws are tracked, fixed, and retested. This ensures that the product confronts the quality requirements of SRS.&nbsp;<br>**Documentation, Training, and Support:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/overview-software-documentation" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/overview-software-documentation" target="_blank">Software documentation</a>&nbsp;is an essential part of the software development life cycle. A well-written document acts as a tool and means to information repository necessary to know about software processes, functions, and maintenance. Documentation also provides information about how to use the product. Training in an attempt to improve the current or future employee performance by increasing an employee‚Äôs ability to work through learning, usually by changing his attitude and developing his skills and understanding.&nbsp;<br><img alt="Stage-5: Product Testing and Integration" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094631/5.jpg" referrerpolicy="no-referrer"><br>Stage 5: Testing<br><br>After detailed testing, the conclusive product is released in phases as per the organization‚Äôs strategy. Then it is tested in a real industrial environment. It is important to ensure its smooth performance. If it performs well, the organization sends out the product as a whole. After retrieving beneficial feedback, the company releases it as it is or with auxiliary improvements to make it further helpful for the customers. However, this alone is not enough. Therefore, along with the deployment, the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/product-management" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/product-management" target="_blank">product‚Äôs supervision</a>.&nbsp;<br><img alt="Stage-6: Deployment and Maintenance of Products" src="https://media.geeksforgeeks.org/wp-content/uploads/20231218094709/6.jpg" referrerpolicy="no-referrer"><br>Stage 6: Deployment and Maintenance<br><br>To this day, we have&nbsp;**more than 50**&nbsp;recognized&nbsp;**SDLC models**&nbsp;in use. But&nbsp;**None of them is perfect**, and each brings its favorable aspects and disadvantages for a specific software development project or a team.<br>Here, we have listed the&nbsp;**top five**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/top-8-software-development-models-used-in-industry/?ref=" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/top-8-software-development-models-used-in-industry/?ref=" target="_blank">*<strong></strong></a>*most popular SDLC models**:<br><br>It is the fundamental model of the software development life cycle. This is a very simple model. The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-classical-waterfall-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-classical-waterfall-model" target="_blank">*<strong></strong></a>*waterfall model**&nbsp;is not in practice anymore, but it is the basis for all other SDLC models. Because of its simple structure, the waterfall model is easier to use and provides a tangible output. In the waterfall model, once a phase seems to be completed, it cannot be changed, and due to this less flexible nature, the waterfall model is not in practice anymore.&nbsp;<br><br>The agile model in SDLC was mainly designed to adapt to changing requests quickly. The main goal of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-agile-development-models" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-agile-development-models" target="_blank">*<strong></strong></a>*Agile model**&nbsp;is to facilitate quick project completion. The agile model refers to a group of development processes. These processes have some similar characteristics but also possess certain subtle differences among themselves.<br><br>In the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-iterative-waterfall-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-iterative-waterfall-model" target="_blank">*<strong></strong></a>*Iterative model**&nbsp;**in SDLC**, each cycle results in a semi-developed but deployable version; with each cycle, some requirements are added to the software, and the final cycle results in the software with the complete requirement specification.&nbsp;<br><br>The spiral model in SDLC is one of the most crucial SDLC models that provides support for risk handling. It has various spirals in its diagrammatic representation; the number of spirals depends upon the type of project. Each loop in the spiral structure indicates the Phases of the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-spiral-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-spiral-model" target="_blank">*<strong></strong></a>*Spiral model****.**&nbsp;&nbsp;<br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-engineering-sdlc-v-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-engineering-sdlc-v-model" target="_blank">*<strong></strong></a>*V-shaped model**&nbsp;**in SDLC**&nbsp;is executed in a sequential manner in V-shape. Each stage or phase of this model is integrated with a testing phase. After every development phase, a testing phase is associated with it, and the next phase will start once the previous phase is completed, i.e., development &amp; testing. It is also known as the verification or validation model.&nbsp;<br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/overview-of-big-bang-model" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/overview-of-big-bang-model" target="_blank">Big Bang model</a>&nbsp;in SDLC is a term used to describe an informal and unstructured approach to software development, where there is no specific planning, documentation, or well-defined phases.<br><br>SDLC is a method, approach, or process that is followed by a software development organization while developing any software.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/sdlc-models-types-phases-use" target="_blank">SDLC models</a>&nbsp;were introduced to follow a disciplined and systematic method while designing software. With the software development life cycle, the process of software design is divided into small parts, which makes the problem more understandable and easier to solve. SDLC comprises a detailed description or step-by-step plan for designing, developing, testing, and maintaining the software.<br>
Follow the project <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/library-management-system" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/library-management-system" target="_blank">*<strong></strong></a>*Library Management System** or <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/portfolio-website-project-software-development" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/portfolio-website-project-software-development" target="_blank">*<strong></strong></a>*E Portfolio Website** to see the use of Software Development Life Cycle in a Software Projects.
<br><br>A frequent issue in software development is the delay of security-related tasks until the testing phase, which occurs late in the software development life cycle (SDLC) and occurs after the majority of crucial design and implementation has been finished. During the testing phase, security checks may be minimal and restricted to scanning and penetration testing, which may fail to identify more complicated security flaws.<br>Security issue can be address in SDLC by following DevOps. Security is integrated throughout the whole SDLC, from build to production, through the use of DevSecOps. Everyone involved in the DevOps value chain have responsibility for security under DevSecOps.<br><br>**Developing a banking application using SDLC:**<br>
<br>**Planning and Analysis:**&nbsp;During this stage, business stakeholders‚Äô requirements about the functionality and features of banking application will be gathered by program managers and business analysts. Detailed SRS (<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/software-requirement-specification-srs-format/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/software-requirement-specification-srs-format/" target="_blank">Software Requirement Specification</a>) documentation will be produced by them. Together with business stakeholders, business analysts will analyse and approve the SRS document.
<br>**Design:**&nbsp;Developers will receive SRS documentation. Developers will read over the documentation and comprehend the specifications. Web pages will be designed by designers. High level system architecture will be prepared by developers.
<br>**Development:**&nbsp;During this stage, development will code. They will create the web pages and APIs needed to put the feature into practice.
<br>**Testing:**&nbsp;Comprehensive functional testing will be carried out. They will guarantee that the banking platform is glitch-free and operating properly.
<br>**Deployment and Maintenance:**&nbsp;The code will be made available to customers and deployed. Following this deployment, the customer can access the online banking. The same methodology will be used to create any additional features.
<br><br>Choosing the right SDLC (Software Development Life Cycle) model is essential for project success. Here are the key factors to consider:<br>
<br>**Project Requirements:**

<br>**Clear Requirements:**&nbsp;Use&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/waterfall-model/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/waterfall-model/" target="_blank">*<strong></strong></a>*Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if requirements are well-defined and unlikely to change.
<br>**Changing Requirements:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Iterative**&nbsp;models if requirements are unclear or likely to evolve.


<br>**Project Size and Complexity:**

<br>**Small Projects:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**RAD**&nbsp;for small, simple projects.
<br>**Large Projects:**&nbsp;Use&nbsp;**Agile**,&nbsp;**Spiral**, or&nbsp;**DevOps**&nbsp;for large, complex projects that need flexibility.


<br>**Team Expertise:**

<br>**Experienced Teams:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Scrum**&nbsp;if the team is familiar with iterative development.
<br>**Less Experienced Teams:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;for teams needing structured guidance.


<br>**Client Involvement:**

<br>**Frequent Client Feedback:**&nbsp;Use&nbsp;**Agile**,&nbsp;**Scrum**, or&nbsp;**RAD**&nbsp;if regular client interaction is needed.
<br>**Minimal Client Involvement:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if client involvement is low after initial planning.


<br>**Time and Budget Constraints:**

<br>**Fixed Time and Budget:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if you have strict time and budget limits.
<br>**Flexible Time and Budget:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Spiral**&nbsp;if you can adjust time and budget as needed.


<br>**Risk Management:**

<br>**High-Risk Projects:**&nbsp;Use&nbsp;**Spiral**&nbsp;for projects with significant risks and uncertainties.
<br>**Low-Risk Projects:**&nbsp;Use&nbsp;**Waterfall**&nbsp;for projects with minimal risks.


<br>**Product Release Timeline:**

<br>**Quick Release Needed:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**RAD**&nbsp;to deliver products quickly.
<br>**Longer Development Time:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;for projects with no urgent deadlines.


<br>**Maintenance and Support:**

<br>**Long-Term Maintenance:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**DevOps**&nbsp;for projects needing continuous updates and support.
<br>**Minimal Maintenance:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if little future maintenance is expected.


<br>**Stakeholder Expectations:**

<br>**High Stakeholder Engagement:**&nbsp;Use&nbsp;**Agile**&nbsp;or&nbsp;**Scrum**&nbsp;if stakeholders want ongoing involvement.
<br>**Low Stakeholder Engagement:**&nbsp;Use&nbsp;**Waterfall**&nbsp;or&nbsp;**V-Model**&nbsp;if stakeholders prefer involvement only at major milestones.


<br>**Note:**<br>

<br>**Waterfall**: Best for clear, stable projects with minimal changes.
<br>**V-Model**: Good for projects with clear requirements and a strong focus on testing.
<br>**Agile/Scrum**: Ideal for projects with changing requirements and frequent client interaction.
<br>**Spiral**: Suitable for high-risk projects with evolving requirements.
<br>**RAD**: Useful for projects needing rapid development.
<br>**DevOps**: Best for continuous integration and ongoing support

<br><br>In conclusion, we now know that the&nbsp;**Software Development Life Cycle (SDLC) in software engineering is an important framework for the better and more structured development of optimized software programs.**&nbsp;In a world full of rapid evolution in technology, SDLC phases plays a crucial role in enabling some good and innovative solutions for helping users and organizations. Also, it‚Äôs better to adapt SDLC principles to achieve software development goals effectively.<br><br><br>(A) Spiral model<br>(B) Prototyping model<br>(C) Waterfall model<br>(D) Capability maturity model<br>
**Solution:**&nbsp;The correct Answer is&nbsp;**(D)**.
<br><br><br>(A) P-3, Q-2, R-4, S-1<br>(B) P-2, Q-3, R-1, S-4<br>(C) P-3, Q-2, R-1, S-4<br>(D) P-2, Q-3, R-4, S-1<br>
**Solution:**&nbsp;The correct Answer is&nbsp;**(B)**.
<br><br><br>
The SDLC involves planning the project, gathering requirements, designing the system, coding the software, testing it for issues, deploying it to users, and maintaining it post-release. Each phase ensures the software meets user needs and functions correctly, from start to finish.
<br><br>
The main phases of SDLC include Requirements, Design, Implementation (Coding), Testing, Deployment, and Maintenance. These phases represent the stages a software project goes through from initiation to completion.
<br><br>
SDLC ensures a structured and organized approach to software development, leading to the creation of reliable and high-quality software. It helps manage resources efficiently, reduces development time, and minimizes the risk of project failure.
<br><br>
The key objectives of SDLC include delivering a high-quality product, meeting customer requirements, managing project resources effectively, minimizing risks, and providing a clear and transparent development process.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/sdlc-software-developement-lifecycle.html</link><guid isPermaLink="false">AWS/For Reading/SDLC - Software Developement LifeCycle.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20231220113035/SDLC.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Simple Definitions To give in Interview]]></title><description><![CDATA[ 
 <br><br><br>Linux is a powerful, flexible, and open-source operating system that's widely used in server environments.<br><br>kernel is the core component of an operating system. It acts as bridge between software and hardware.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/simple-definitions-to-give-in-interview.html</link><guid isPermaLink="false">AWS/For Reading/Simple Definitions To give in Interview.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[Some talks about git in reddit]]></title><description><![CDATA[ 
 <br><br>[reddit user]<br>
git&nbsp;is a program that is used to track changes in code. It's very useful and allows you to "go back in time" if something is broken and see what's changed which might have caused it to break. It's also very handy when several people work on the same code together, they can share their changes easily.
GitHub is a website, now owned by Microsoft, that you can use with git. There are other websites like it, e.g. GitLab. All of them allow you to put your changes on their servers to make it easier to share them with other people. They may also have other features like more advanced permissions so you can only share your code with specific people, or build platforms where you can automatically build your program after you've pushed new code to them.
<br>[reddit user]<br>
Just in case that is still a bit dense for you or your current computer knowledge a simpler version is:
Linus Torvalds, the man who Linux is named after (Linus + Unix = Linux) made a manager for managing the revisions of computer programming he was doing making the Linux Kernel. Every different thing he tried to do to solve for how to make something one way, or another or the third way was saved. if he had to go back and change from the third way back to the second or first as all saved unlike usual computer coding which wouldn't save that.
Making fun of himself that he needed this helpful feature he called it "git" after the British Slang for someone being a dummy. Now some people prefer to think of it as "Go-back In Time" instead of thinking of the creator of the Linux Kernel as 'dumb'.
Now Git was so useful, and like Linux, Open Source and Free, and the more it was developed by the community the more powerful, and to some people, seen as a required tool for doing programming projects.
What was shared versions of work stored locally, called versioning, would be stored in folders of single coders work, or multiple coders putting their proposed version(s) for a piece of code, or multiple people sharing and collaborating on the same code. The folders for these were called repositories. Later the finished/published pieces of code would be published to a folder which was also called a repository. When you install software on Linux or just using package managers in general, they largely use collections of published repositories either by pointing at all those final version repositories or by collecting copies of them on a central server called itself a repository, as in a repository of repositories.
Now this is where GitHub and its siblings/cousins come in. They can act as an online collection of not just the published version of the repositories, but a central place for people to collaborate with Git online, store the work away from the person's local computer alone, preventing loss of data, and even act as a place users can come to learn about and install and get help with the published program/code.
In short you use Git to help you develop code and track all the versions of your work over time.<br>
Git is so ubiquitous that the format it finishes off the code in can be used to distribute to code and update it later.
GitHub (and GitLab, SourceForge, etc.) are places where code project can be worked on and collaborated with that also distribute and help creators support users that is done with Git and Git related software.
Repositories is just a Git related Jargon for the folders that hold the different versions of a Git Project, but when you are talking Package Managers or Code Sites like GitHub, they are talking about the stored published versions of the code that people can download, install, and use.
To you the end user just think of repositories as "Folders of Code" be it programs or extensions or mods or even collections of settings and that it is just jargon.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/some-talks-about-git-in-reddit.html</link><guid isPermaLink="false">AWS/For Reading/Some talks about git in reddit.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[ssl certificate]]></title><description><![CDATA[ 
 <br><br>An SSL certificate is a digital certificate that authenticates a website's identity and enables an encrypted connection. SSL stands for Secure Sockets Layer, a security protocol that creates an encrypted link between a web server and a web browser.<br>Companies and organizations need to add SSL certificates to their websites to secure online transactions and keep customer information private and secure.<br>In short: SSL keeps internet connections secure and prevents criminals from reading or modifying information transferred between two systems. When you see a padlock icon next to the URL in the address bar, that means SSL protects the website you are visiting.<br>Since its inception about 25 years ago, there have been several versions of SSL protocol, all of which at some point ran into security troubles. A revamped and renamed version followed ‚Äî TLS (Transport Layer Security), which is still in use today. However, the initials SSL stuck, so the new version of the protocol is still usually called by the old name.<br><br>SSL works by ensuring that any data transferred between users and websites, or between two systems, remains impossible to read. It uses encryption algorithms to scramble data in transit, which prevents hackers from reading it as it is sent over the connection. This data includes potentially sensitive information such as names, addresses, credit card numbers, or other financial details.<br>The process works like this:<br>
<br>A browser or server attempts to connect to a website (i.e., a web server) secured with SSL.
<br>The browser or server requests that the web server identifies itself.
<br>The web server sends the browser or server a copy of its SSL certificate in response.
<br>The browser or server checks to see whether it trusts the SSL certificate. If it does, it signals this to the webserver.
<br>The web server then returns a digitally signed acknowledgment to start an SSL encrypted session.
<br>Encrypted data is shared between the browser or server and the webserver.
<br>This process is sometimes referred to as an "SSL handshake." While it sounds like a lengthy process, it takes place in milliseconds.<br>When a website is secured by an SSL certificate, the acronym HTTPS (which stands for HyperText Transfer Protocol Secure) appears in the URL. Without an SSL certificate, only the letters HTTP ‚Äì i.e., without the S for Secure ‚Äì will appear. A padlock icon will also display in the URL address bar. This signals trust and provides reassurance to those visiting the website.<br>To view an SSL certificate's details, you can click on the padlock symbol located within the browser bar. Details typically included within SSL certificates include:<br>
<br>The domain name that the certificate was issued for
<br>Which person, organization, or device it was issued to
<br>Which Certificate Authority issued it
<br>The Certificate Authority's digital signature
<br>Associated subdomains
<br>Issue date of the certificate
<br>The expiry date of the certificate
<br>The public key (the private key is not revealed)
<br><br>Websites need SSL certificates to keep user data secure, verify ownership of the website, prevent attackers from creating a fake version of the site, and convey trust to users.<br>If a website is asking users to sign in, enter personal details such as their credit card numbers, or view confidential information such as health benefits or financial information, then it is essential to keep the data confidential. SSL certificates help keep online interactions private and assure users that the website is authentic and safe to share private information with.<br>More relevant to businesses is the fact that an SSL certificate is required for an HTTPS web address. HTTPS is the secure form of HTTP, which means that HTTPS websites have their traffic encrypted by SSL. Most browsers tag HTTP sites ‚Äì those without SSL certificates ‚Äì as "not secure." This sends a clear signal to users that the site may not be trustworthy ‚Äì incentivizing businesses who have not done so to migrate to HTTPS.<br>An SSL certificate helps to secure information such as:<br>
<br>Login credentials
<br>Credit card transactions or bank account information
<br>Personally identifiable information ‚Äî such as full name, address, date of birth, or telephone number
<br>Legal documents and contracts
<br>Medical records
<br>Proprietary information
<br><br>There are different types of SSL certificates with different validation levels. The six main types are:<br>
<br>Extended Validation certificates (EV SSL)
<br>Organization Validated certificates (OV SSL)
<br>Domain Validated certificates (DV SSL)
<br>Wildcard SSL certificates
<br>Multi-Domain SSL certificates (MDC)
<br>Unified Communications Certificates (UCC)
<br><br>This is the highest-ranking and most expensive type of SSL certificate. It tends to be used for high profile websites which collect data and involve online payments. When installed, this SSL certificate displays the padlock, HTTPS, name of the business, and the country on the browser address bar. Displaying the website owner's information in the address bar helps distinguish the site from malicious sites. To set up an EV SSL certificate, the website owner must go through a standardized identity verification process to confirm they are authorized legally to the exclusive rights to the domain.<br><br>This version of SSL certificate has a similar assurance similar level to the EV SSL certificate since to obtain one; the website owner needs to complete a substantial validation process. This type of certificate also displays the website owner's information in the address bar to distinguish from malicious sites. OV SSL certificates tend to be the second most expensive (after EV SSLs), and their primary purpose is to encrypt the user's sensitive information during transactions. Commercial or public-facing websites must install an OV SSL certificate to ensure that any customer information shared remains confidential.<br><br>The validation process to obtain this SSL certificate type is minimal, and as a result, Domain Validation SSL certificates provide lower assurance and minimal encryption. They tend to be used for blogs or informational websites ‚Äì i.e., which do not involve data collection or online payments. This SSL certificate type is one of the least expensive and quickest to obtain. The validation process only requires website owners to prove domain ownership by responding to an email or phone call. The browser address bar only displays HTTPS and a padlock with no business name displayed.<br><br>Wildcard SSL certificates allow you to secure a base domain and unlimited sub-domains on a single certificate. If you have multiple sub-domains to secure, then a Wildcard SSL certificate purchase is&nbsp;much&nbsp;less expensive than buying individual SSL certificates for each of them. Wildcard SSL certificates have an asterisk  as part of the common name, where the asterisk represents any valid sub-domains that have the same base domain. For example, a single Wildcard certificate for website can be used to secure:<br>
<br>payments.yourdomain.com
<br>login.yourdomain.com
<br>mail.yourdomain.com
<br>download.yourdomain.com
<br>anything.yourdomain.com
<br><br>A Multi-Domain certificate can be used to secure many domains and/or sub-domain names. This includes the combination of completely unique domains and sub-domains with different TLDs (Top-Level Domains) except for local/internal ones.<br>For example:<br>
<br><a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a>
<br>example.org
<br>mail.this-domain.net
<br>example.anything.com.au
<br>checkout.example.com
<br>secure.example.org
<br>Multi-Domain certificates do not support sub-domains by default. If you need to secure both <a data-tooltip-position="top" aria-label="http://www.example.com" rel="noopener nofollow" class="external-link" href="http://www.example.com" target="_blank">www.example.com</a> and example.com with one Multi-Domain certificate, then both hostnames should be specified when obtaining the certificate.<br><br>Unified Communications Certificates (UCC) are also considered Multi-Domain SSL certificates. UCCs were initially designed to secure Microsoft Exchange and Live Communications servers. Today, any website owner can use these certificates to allow multiple domain names to be secured on a single certificate. UCC Certificates are organizationally validated and display a padlock on a browser. UCCs can be used as EV SSL certificates to give website visitors the highest assurance through the green address bar.<br>It is essential to be familiar with the different types of SSL certificates to obtain the right type of certificate for your website.<br><br>SSL certificates can be obtained directly from a Certificate Authority (CA). Certificate Authorities ‚Äì sometimes also referred to as Certification Authorities ‚Äì issue millions of SSL certificates each year. They play a critical role in how the internet operates and how transparent, trusted interactions can occur online.<br>The cost of an SSL certificate can range from free to hundreds of dollars, depending on the level of security you require. Once you decide on the type of certificate you require, you can then look for Certificate Issuers, which offer SSLs at the level you require.<br>Obtaining your SSL involves the following steps:<br>
<br>Prepare by getting your server set up and ensuring your&nbsp;<a data-tooltip-position="top" aria-label="https://www.whois.com/" rel="noopener nofollow" class="external-link" href="https://www.whois.com/" target="_blank">WHOIS</a>&nbsp;record is updated and matches what you are submitting to the Certificate Authority (it needs to show the correct company name and address, etc.)
<br>Generating a Certificate Signing Request (CSR) on your server. This is an action your hosting company can assist with.
<br>Submitting this to the Certificate Authority to validate your domain and company details
<br>Installing the certificate they provide once the process is complete.
<br>Once obtained, you need to configure the certificate on your web host or on your own servers if you host the website yourself.<br>How quickly you receive your certificate depends on what type of certificate you get and which certificate provider you procure it from. Each level of validation takes a different length of time to complete. A simple Domain Validation SSL certificate can be issued within minutes of being ordered, whereas Extended Validation can take as long as a full week.<br><img alt="Types of SSL certificates" src="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg" referrerpolicy="no-referrer"><br><br>It is possible to use one SSL certificate for multiple domains on the same server. Depending on the vendor, you can also use one SSL certificate on multiple servers. This is because of Multi-Domain SSL certificates, which we discussed above.<br>As the name implies, Multi-Domain SSL Certificates work with multiple domains. The number is left up to the specific issuing Certificate Authority. A Multi-Domain SSL Certificate is different from a Single Domain SSL Certificate, which ‚Äì again, as the name implies ‚Äì is designed to secure a single domain.<br>To make matters confusing, you may hear Multi-Domain SSL Certificates, also referred to as SAN certificates. SAN stands for Subject Alternative Name. Every multi-domain certificate has additional fields (i.e., SANs), which you can use to list additional domains that you want to cover under one certificate.<br>Unified Communications Certificates (UCCs) and Wildcard SSL Certificates also allow for multi-domains and, in the latter case, an unlimited number of subdomains.<br><br>SSL certificates do expire; they don't last forever.&nbsp;<a data-tooltip-position="top" aria-label="https://cabforum.org/" rel="noopener nofollow" class="external-link" href="https://cabforum.org/" target="_blank">The Certificate Authority/Browser Forum</a>, which serves as the de facto regulatory body for the SSL industry, states that SSL certificates should have a lifespan of&nbsp;<a data-tooltip-position="top" aria-label="https://cabforum.org/uploads/CA-Browser-Forum-BR-1.6.0.pdf" rel="noopener nofollow" class="external-link" href="https://cabforum.org/uploads/CA-Browser-Forum-BR-1.6.0.pdf" target="_blank">no more than 27 months</a>. This essentially means two years plus you can carry over up to three months if you renew with time remaining on your previous SSL certificate.<br>SSL certificates expire because, as with any form of authentication, information needs to be periodically re-validated to check it is still accurate. Things change on the internet, as companies and also websites are bought and sold. As they change hands, the information relevant to SSL certificates also changes. The purpose of the expiry period is to ensure that the information used to authenticate servers and organizations is as up-to-date and accurate as possible.<br>Previously, SSL certificates could be issued for as long as five years, which was subsequently reduced to three and most recently to two years plus a potential extra three months. In 2020, Google, Apple, and Mozilla announced&nbsp;<a data-tooltip-position="top" aria-label="https://techbeacon.com/security/google-apple-mozilla-enforce-1-year-max-security-certifications" rel="noopener nofollow" class="external-link" href="https://techbeacon.com/security/google-apple-mozilla-enforce-1-year-max-security-certifications" target="_blank">they would enforce one-year SSL certificates</a>, despite this proposal being voted down by the Certificate Authority Browser Forum. This took effect from September 2020. It is possible that in the future, the length of validity will reduce still further.<br>When an SSL certificate expires, it makes the site in question unreachable. When a user's browser arrives at a website, it checks the SSL certificate's validity within milliseconds (as part of the SSL handshake). If the SSL certificate has expired, visitors will receive a message to the effect of ‚Äî "This site is not secure. Potential risk ahead".<br>While users do have the option to proceed, it is not advisable to do so, given the cybersecurity risks involved, including the possibility of&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/resource-center/threats/malware-protection" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/resource-center/threats/malware-protection" target="_blank">malware</a>. This will significantly impact bounce rates for website owners, as users rapidly click off the homepage and go elsewhere.<br>Keeping on top of when SSL certificates expire presents a challenge for larger businesses. While smaller and&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/small-to-medium-business-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/small-to-medium-business-security" target="_blank">medium-sized businesses (SMEs)</a>&nbsp;may have one or only a few certificates to manage,&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/enterprise-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/enterprise-security" target="_blank">enterprise-level organizations</a>&nbsp;that potentially transact across markets ‚Äì with numerous websites and networks ‚Äì will have many more. At this level, allowing an SSL certificate to expire is usually the result of oversight rather than incompetence. The best way for larger businesses to stay on top of when their SSL certificates expire is by using a certificate management platform. There are various products on the market, which you can find using an online search. These allow enterprises to see and manage digital certificates across their entire infrastructure. If you do use one of these platforms, it is important to log in regularly so you can be aware of when renewals are due.<br>If you allow a certificate to expire, the certificate becomes invalid, and you will no longer be able to run secure transactions on your website. The Certification Authority (CA) will prompt you to renew your SSL certificate before the expiration date.<br>Whichever Certificate Authority or SSL service you use to obtain your SSL certificates from will send you expiration notifications at set intervals, usually starting at 90 days out. Try to ensure that these reminders are being sent to an email distribution list ‚Äî rather than a single individual, who may have left the company or moved to another role by the time the reminder is sent. Think about which stakeholders in your company are on this distribution list to ensure the right people see the reminders at the right time.<br><br>The easiest way to see if a site has an SSL certificate is by looking at the address bar in your browser:<br>
<br>If the URL begins with HTTPS instead of HTTP, that means the site is secured using an SSL certificate.
<br>Secure sites show a closed padlock emblem, which you can click on to see security details ‚Äì the most trustworthy sites will have green padlocks or address bars.
<br>Browsers also show warning signs when a connection is not secure ‚Äî such as a red padlock, a padlock which is not closed, a line going through the website's address, or a warning triangle on top of the padlock emblem.
<br><br>Only submit your personal data and online payment details to websites with EV or OV certificates. DV certificates are not suitable for eCommerce websites. You can tell if a site has an EV or OV certificate by looking at the address bar. For an EV SSL, the organization's name will be visible in the address bar itself. For an OV SSL, you can see the organization's name's details by clicking on the padlock icon. For a DV SSL, only the padlock icon is visible.<br>Read the website's privacy policy. This enables you to see how your data will be used. Legitimate companies will be transparent about how they collect your data and what they do with it.<br>Look out for trust signals or indicators on websites.<br>
As well as SSL certificates, these include reputable logos or badges which show the website meets specific security standards. Other signs that can help you determine if a site is real or not include checking for a physical address and telephone number, checking their returns or refunds policy, and making sure prices are believable and not too good to be true.<br>Stay alert to phishing scams.<br>
Sometimes cyber attackers create websites that mimic existing websites to trick people into purchasing something or logging in to their phishing site. It is possible for a&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/resource-center/preemptive-safety/phishing-prevention-tips" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/resource-center/preemptive-safety/phishing-prevention-tips" target="_blank">phishing</a>&nbsp;site to obtain an SSL certificate and therefore encrypt all the traffic that flows between you and it. A growing proportion of phishing scams occur on HTTPS sites ‚Äî deceiving users who feel reassured by the padlock icon's presence.<br>To avoid these kinds of attacks:<br>
<br>Always examine the domain of the site you are on and ensure it is spelled correctly. The URL of a fake site might differ by only one character ‚Äì e.g., amaz0n.com instead of amazon.com. If in doubt, type the domain directly into your browser to make sure you are connecting to the website you intend to visit.
<br>Never enter logins, passwords, banking credentials, or any other personal information on the site unless you are sure of its authenticity.
<br>Always consider what a particular site is offering, whether it looks suspicious, and whether you really need to register on it.
<br>Make sure your devices are well protected:&nbsp;<a data-tooltip-position="top" aria-label="https://www.kaspersky.com/internet-security" rel="noopener nofollow" class="external-link" href="https://www.kaspersky.com/internet-security" target="_blank">Kaspersky Internet Security</a>&nbsp;checks URLs against an extensive database of phishing sites, and it detects scams regardless of how "safe" the resource looks.
<br>Cybersecurity risks continue to evolve but understanding the types of SSL certificates to look out for and how to distinguish a safe site from a potentially dangerous one will help internet users avoid scams and protect their personal data from cybercriminals.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/ssl-certificate.html</link><guid isPermaLink="false">AWS/For Reading/ssl certificate.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.kaspersky.com/content/en-global/images/repository/isc/2020/what-is-a-ssl-certificate2.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Amazon S3 Storage Classes]]></title><description><![CDATA[ 
 <br><br><br>Amazon S3 offers a range of storage classes that you can choose from based on the performance, data access, resiliency, and cost requirements of your workloads. S3 storage classes are purpose-built to provide the lowest cost storage for different access patterns. S3 storage classes are ideal for virtually any use case, including those with demanding performance needs, data lakes, data residency requirements, unknown or changing access patterns, or archival storage.<br>The S3 storage classes include&nbsp;S3 Intelligent-Tiering&nbsp;for automatic cost savings for data with unknown or changing access patterns,&nbsp;S3 Standard&nbsp;for frequently accessed data,&nbsp;S3 Express One Zone&nbsp;for your most frequently accessed data,&nbsp;S3 Standard-Infrequent Access (S3 Standard-IA)&nbsp;and&nbsp;S3 One Zone-Infrequent Access (S3 One Zone-IA)&nbsp;for less frequently accessed data,&nbsp;S3 Glacier Instant Retrieval&nbsp;for archive data that needs immediate access,&nbsp;S3 Glacier Flexible Retrieval (formerly S3 Glacier)&nbsp;for rarely accessed long-term data that does not require immediate access, and&nbsp;Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive)&nbsp;for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud.<br>Amazon S3 provides the most durable storage in the cloud. Based on its unique architecture, S3 is designed to exceed 99.999999999% (11 nines) data durability. Additionally, S3 stores data redundantly across a minimum of 3 Availability Zones by default, providing built-in resilience against widespread disaster. Customers can store data in a single AZ to minimize storage cost or latency, in multiple AZs for resilience against the permanent loss of an entire data center, or in multiple AWS Regions to meet geographic resilience requirements. If you have data residency requirements that can‚Äôt be met by an existing AWS Region, you can use S3 storage classes for AWS Dedicated Local Zones or S3 on Outposts racks to store your data in a specific data perimeter.&nbsp;<br>You can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. Amazon S3 also offers capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application. S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone, and do not support S3 Lifecycle transitions.<br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes-infographic/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes-infographic/" target="_blank">View the Amazon S3 storage classes overview infographic.</a><br><br><br>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.&nbsp;<br>Key features:<br>
<br>General purpose storage for frequently accessed data
<br>Low latency and high throughput performance
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br><br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/intelligent-tiering/" target="_blank"><strong></strong></a>Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)&nbsp;is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. S3 Intelligent-Tiering delivers milliseconds latency and high throughput performance for frequently, infrequently, and rarely accessed data in the Frequent, Infrequent, and Archive Instant Access tiers. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.<br>For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier that is optimized for frequent access, a 40% lower-cost tier that is optimized for infrequent access, and a 68% lower-cost tier optimized for rarely accessed data. S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier. For data that does not require immediate retrieval, you can set up S3 Intelligent-Tiering to monitor and automatically move objects that aren‚Äôt accessed for 180 days or more to the Deep Archive Access tier to realize up to 95% in storage cost savings.<br>There are no retrieval charges in S3 Intelligent-Tiering. If an object in the Infrequent or Archive Instant Access tier is accessed later, it‚Äôs automatically moved back to the Frequent Access tier. If the object you‚Äôre retrieving is stored in the optional Deep Archive tiers, before you can retrieve the object, you must first restore a copy using RestoreObject. &nbsp;For information about restoring archived objects, see&nbsp;<a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html" target="_blank">Restoring Archived Objects</a>.&nbsp;No additional tiering charges apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class.<br>Key features:<br>
<br>Automatic cost savings for data with unknown or changing access patterns
<br>Frequent, Infrequent, and Archive Instant Access tiers have the same low-latency and high-throughput performance of S3 Standard
<br>The Infrequent Access tier saves up to 40% on storage costs
<br>The Archive Instant Access tier saves up to 68% on storage costs
<br>Opt-in asynchronous archive capabilities for objects that become rarely accessed
<br>Deep Archive Access tier has the same performance as Glacier Deep Archive and saves up to 95% for rarely accessed objects
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br>Small monthly monitoring and&nbsp;automation charge
<br>No operational overhead, no lifecycle charges, no retrieval charges, and no minimum storage duration
<br>Objects smaller than 128KB can be stored in S3 Intelligent-Tiering but will always be charged at the Frequent Access tier rates, and are not charged the monitoring and automation charge.
<br><br><br><a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/express-one-zone/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/express-one-zone/" target="_blank"><strong></strong></a>Amazon S3 Express One Zone&nbsp;is a high-performance, single-Availability Zone storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications.&nbsp;S3 Express One Zone can improve data access speeds by 10x and reduce request costs by 50% compared to S3 Standard. While you have always been able to choose a specific AWS Region to store your S3 data, with S3 Express One Zone you can select a specific AWS Availability Zone within an AWS Region to store your data. You can choose to co-locate your storage and compute resources in the same Availability Zone to further optimize performance, which helps lower compute costs and run workloads faster. With S3 Express One Zone, data is stored in a different bucket type‚Äîan Amazon S3 directory bucket‚Äîwhich supports hundreds of thousands of requests per second. Additionally, you can use S3 Express One Zone with services such as&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/sagemaker/train/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/sagemaker/train/" target="_blank">Amazon SageMaker Model Training</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/athena/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/athena/" target="_blank">Amazon Athena</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/emr/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/emr/" target="_blank">Amazon EMR</a>, and&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/glue/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/glue/" target="_blank">AWS Glue</a>&nbsp;Data Catalog to accelerate your ML and analytics workloads. With S3 Express One Zone, storage automatically scales up or down based on your consumption and need, and you no longer need to manage multiple storage systems for low-latency workloads.<br>Key features:<br>
<br>High performance storage for your most frequently accessed data
<br>Consistent single-digit millisecond request latency
<br>Improve access speeds by 10x and reduce request costs by 50% compared to S3 Standard
<br>Select an AWS Availability Zone and have the option to co-locate storage and compute resources for even lower latency, with reduced processing time and more efficient use of compute resources contributing to lower overall total cost of ownership
<br>Accelerate analytics and ML workloads with AWS service integrations
<br>Scale to handle millions of requests per minute
<br>Optimized for large datasets with many small objects
<br>Use existing Amazon S3 APIs with different bucket type ‚Äì directory buckets
<br>Designed to deliver 99.95% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br><br><br>S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. You can configure S3 storage classes at the object level, and a single bucket can contain objects stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.<br>Key features:<br>
<br>Infrequently accessed data that needs millisecond access
<br>Same low latency and high throughput performance of S3 Standard
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br><br>S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. It‚Äôs a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 Cross-Region Replication.<br>S3 One Zone-IA offers the same high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. Using similar engineering designs as S3 Regional storage classes, S3 One Zone-IA also offers 11 nines of durability, but may be susceptible to data loss in the unlikely case of the loss or damage to all or part of an AWS Availability Zone. You can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.<br>Key features:<br>
<br>Re-creatable infrequently accessed data
<br>Same low latency and high throughput performance of S3 Standard
<br>Designed to deliver 99.5% availability with an availability SLA of 99%
<br><br><br>The&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes&nbsp;are purpose-built for data archiving, and are designed to provide you with the highest performance, the most retrieval flexibility, and the lowest cost archive storage in the cloud. You can choose from three archive storage classes optimized for different access patterns and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5‚Äî12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval from 12‚Äî48 hours.<br><br>Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. With S3 Glacier Instant Retrieval, you can save up to 68% on storage costs compared to using the S3 Standard-Infrequent Access (S3 Standard-IA) storage class, when your data is accessed once per quarter. S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives. You can upload objects directly to S3 Glacier Instant Retrieval, or use S3 Lifecycle policies to transfer data from the S3 storage classes. For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/" target="_blank"><strong></strong></a>Amazon S3 Glacier Instant Retrieval page ¬ª<br>Key features:<br>
<br>Long-lived data that is accessed a few times per year with instant retrievals
<br>Data retrieval in milliseconds with the same performance as S3 Standard
<br>Designed to deliver 99.9% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99%
<br>128 KB minimum object size
<br>S3 PUT API for direct uploads to S3 Glacier Instant Retrieval, and S3 Lifecycle management for automatic migration of objects
<br><br>S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1‚Äî2 times per year and is retrieved asynchronously. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, S3 Glacier Flexible Retrieval (formerly S3 Glacier) is the ideal storage class. S3 Glacier Flexible Retrieval delivers the most flexible retrieval options that balance cost with access times ranging from minutes to hours and with free bulk retrievals. It is an ideal solution for backup, disaster recovery, offsite data storage needs, and for when some data&nbsp;occasionally need to be retrieved in minutes, and you don‚Äôt want to worry about costs. S3 Glacier Flexible Retrieval is designed for 99.999999999% (11 nines) of data durability and 99.99% availability by redundantly storing data across multiple physically separated AWS Availability Zones in a given year. For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes page ¬ª<br>Key features:<br>
<br>Backup and archive data that is rarely accessed and low cost
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br>Supports SSL for data in transit and encryption of data at rest
<br>Ideal for backup and disaster recovery use cases when large sets of data&nbsp;occasionally need to be retrieved in minutes, without concern for costs
<br>Configurable retrieval times, from minutes to hours, with free bulk retrievals
<br>S3 PUT API for direct uploads to S3 Glacier Flexible Retrieval, and S3 Lifecycle management for automatic migration of objects
<br><br>S3 Glacier Deep Archive is Amazon S3‚Äôs lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers‚Äîparticularly those in highly-regulated industries, such as financial services, healthcare, and public sectors‚Äîthat retain data sets for 7‚Äî10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. All objects stored in S3 Glacier Deep Archive are replicated and stored across at least three geographically-dispersed Availability Zones, protected by 99.999999999% of durability, and can be restored within 12 hours.&nbsp;For more information, visit the&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/storage-classes/glacier/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/glacier/" target="_blank"><strong></strong></a>Amazon S3 Glacier storage classes page ¬ª<br>Key features:<br>
<br>Archive data that is very rarely accessed and very low cost
<br>Designed to deliver 99.99% availability with an&nbsp;<a data-tooltip-position="top" aria-label="https://aws.amazon.com/s3/sla/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/sla/" target="_blank">availability SLA</a>&nbsp;of 99.9%
<br>Ideal alternative to magnetic tape libraries
<br>Retrieval time within 12 hours
<br>S3 PUT API for direct uploads to S3 Glacier Deep Archive, and S3 Lifecycle management for automatic migration of objects
<br><br><br>In AWS Dedicated Local Zones, the S3 Express One Zone and S3 One Zone-Infrequent access storage classes are purpose-built to store data in a specific data perimeter to support your data isolation and data residency use cases. Dedicated Local Zones are a type of AWS infrastructure that is fully managed by AWS, built for exclusive use by you or your community, and placed in a location or data center specified by you to help you comply with regulatory requirements. Both storage classes store data in a single Dedicated Local Zone and are supported in directory buckets. S3 supports the AWS Management Console, AWS SDKs, and S3 APIs, so you can run S3 based applications in Dedicated Local Zones. S3 Express One Zone is a high-performance storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications. Amazon S3 One Zone-Infrequent Access is designed for data that is accessed less frequently and is ideal for backups.&nbsp;<br>Key features:<br>
<br>Store S3 objects in a specific data perimeter
<br>Enforce security within a data perimeter using AWS Identity and Access Management (IAM)
<br>Audit bucket and object-level access for governance and compliance use cases with AWS CloudTrail
<br>Designed to durably and redundantly store data in a single Dedicated Local Zone
<br><br>Amazon S3 on Outposts delivers object storage to your on-premises AWS Outposts environment. Using the S3 APIs and features available in AWS Regions today, S3 on Outposts makes it easy to store and retrieve data on your Outpost, as well as secure the data, control access, tag, and report on it. S3 on Outposts provides a single Amazon S3 storage class, named 'OUTPOSTS', which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your Outposts. The S3 Outposts storage class is ideal for workloads with local data residency requirements, and to satisfy demanding performance needs by keeping data close to on-premises applications.<br>Key features:<br>
<br>Store S3 objects in your on-premises AWS Outposts environment&nbsp;
<br>S3 Object compatibility and bucket management through the S3 SDK
<br>Designed to durably and redundantly store data on your AWS Outposts rack
<br>Encryption using SSE-S3 and SSE-C
<br>Authentication and authorization using IAM&nbsp;and S3 Access Points
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/storage-classes-in-s3.html</link><guid isPermaLink="false">AWS/For Reading/Storage Classes in S3.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Subnetting in Computer Networks]]></title><description><![CDATA[ 
 <br><br>source: <a rel="noopener nofollow" class="external-link" href="https://www.javatpoint.com/subnetting-in-computer-networks" target="_blank">https://www.javatpoint.com/subnetting-in-computer-networks</a><br>In this tutorial, we will learn about Subnetting in Computer Networks Subject. First and foremost, the most crucial concept we are going to learn today is that while studying the subject of computer networks is subnetting. The most crucial idea known as Subnetting will help to lessen or disperse the pressure that the networks' heavy load causes. Let's now quickly go through the idea of subnetting for everyone.<br>Now, let us know the definition of Subnetting. But before going into the Subnetting concept let us know the overview of the concept named Subnetting in Computer Networks.<br>Subnetting is a part of Network Layer. The duty of the network layer is to divide the received message into separate components and activities. The Network layer can be called as the heart of Computer Networks.<br><br>Finding a network and delivering data to it was simpler when the IP (Internet Protocol) system was initially implemented since there were fewer individuals online. Sending a data packet to the desired machine in a network is getting more and more challenging these days due to the rise in internet users. Network performance becomes a major issue once a network is large enough to serve an enterprise.<br>In order to divide larger networks logically (firewalls, etc.) or physically (for example), an organization can employ IP subnets (smaller broadcast domains, etc.). To put it another way, routers base their routing choices on subnets. We shall learn more about these ideas in this post.<br><br>Subnetting is a combination of two words i.e. Sub and Netting. Here Sub word means Substitute and netting word means Network. The Substitute Network created for a function to happen is known as Subnetting.<br>Here, Substitute Network does not mean a new network is created. A full piece of network is broken into small pieces and each piece a different is assigned.<br>Subnet is the name given to piece of the broken network or can also be called as the Substitute network is known as Subnet. Subnets are the legal small parts of IP (Internet Protocol) Addressing process<br>Subnetting should be done in such a way that network does not gets affected. This means that we can divide the network into different parts but all when put together should perform the same task when done before splitting in to small parts.<br>Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed<br>Subnetting is a technique for creating logical sub-networks from a single physical network (subnets). A company can grow its network via subnetting without asking for a new network number from its ISP. Subnetting hides network complexity while assisting in the reduction of network traffic. Here, a network which is unique has to provide its services to many Local Area Networks i.e. (LAN). So, for this reason Subnetting is extensively used.<br>Do you know what these little subnets are? As we all know, subnetting divides networks into them. A subnet is a smaller network, also referred to as a sub network. An IP network is logically divided into several smaller network components by subnets. A subnet is used to divide a large network into a number of smaller, linked networks, which helps to minimize traffic. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed.<br>A rapid, effective, and reliable computer network is what subnetting is meant to create. Network traffic must find more effective routes as they become larger and more complicated. If all network traffic used the same path and moved through the system at once, bottlenecks and congestion would form, creating sluggish and inefficient backlogs. You may reduce the number of routers that network traffic must transit through by setting up a subnet. In order to make traffic go the shortest distance feasible inside a bigger network, an engineer will effectively create smaller mini routes.<br><br>
<br>Efficiency of the Network
<br>By removing the need for extra routers, subnetting makes network traffic simpler. This makes sure the data being transmitted can get to its destination as fast as possible, eliminating or avoiding any potential diversions that may slow it down.<br>
<br>Provides Network Security
<br>By isolating or removing vulnerable network regions and making it harder for intruders to move through a company's network, subnetting helps the network managers in reducing network-wide risks.<br>
<br>Internet Protocol (IP) Addressing Relocation
<br>Each class has a finite amount of possible host allocations; for instance, networks with more than 254 devices require a Class B allocation. Assume that you are a network administrator. Now, you have a task of allocating 150 hosts among three physical networks in three distinct cities for a Class B or C network. If so, we must either ask for additional address blocks for each network or split the single big network into small parts named subnets so that we could utilize a single address block across a number of physical networks.<br>We will learn about this concept deeper in the upcoming topics.<br>
<br>Reduction of Network Traffic
<br>Placing all of the computers on the same subnet can assist minimize network traffic if a significant amount of an organization's traffic is intended to be shared routinely among a number of devices. Without a subnet, all computers and servers on the network would be able to see data packets from every other machine.<br>
<br>Network Speed Improvement
<br>The main network is divided into smaller subnets through the process of subnetting, and the goal of these smaller, linked networks is to split the large network into a collection of smaller, less-busy networks. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network.<br>
<br>Division of IP Addresses
<br>An IP address is split into its network address and host address via subnetting.<br>The split address may then be further divided into units using the subnet mask approach, and those units can be assigned to different network devices.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png" referrerpolicy="no-referrer"><br>Here, X refers to the Host ID. This is the only thing which gets changed in the Internet Protocol Address<br>Now, we are going to learn how these subnets provide the different addresses to different devices and also the process of subnetting in computer networks. So, by this example we would easily understand the working of the Subnet.<br>We are going to learn how Subnets are formed for Internet Protocol version 4 (IPv4) Addressing.<br>The IPv4 Addressing has five different classes. They are:<br>
<br>Class A Network
<br>Class B Network
<br>Class C Network
<br>Class D Network
<br>Class E Network
<br>The total number of Internet Protocol Addresses (IP Address) gives the total number of Subnets that can be formed by using a network.<br>
<br>Class A has 24 Host ID Bits
<br>Class B has 16 Host ID Bits
<br>Class C has 8 Host ID Bits
<br>The number of usable IP Addresses that can be created is<br>The total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.<br>
Class A Network can have 224 - 2<br>
Class B Network can have 216 - 2<br>
Class C Network can have 28 - 2<br>
Class D and Class E do not contribute for IP Address creation.<br>
Class D is used for multicasting purpose<br>
Class E is used for Address Range Calculator<br>They are saved for future purposes.<br><br><br>We have arrived at the subject at hand, Subnetting, thanks to the problem of IP address waste. By taking bits from the Host ID section of the address, subnetting enables the creation of smaller networks (sub networks; subnets) within of a larger network. With the help of those borrowed bits, we can build more networks with a reduced overall size.<br>A Subnet is created from the bits taken from the Host ID.<br>To understand about this concept let take an example of a network this belongs to class C.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks2.png" referrerpolicy="no-referrer"><br>Our goal is to create to build a network. The capacity of each network must be Thirty (30) Devices. We have three networks of type Class C Network based on IPv4 Addressing.<br>Each Class C Network can provide Two Hundred and Fifty Four (254) Internet Protocol Addresses.<br>The Capacity of each device which we require is very less than the Capacity which we require.<br>So, now we divide the four networks based on the requirement. Let us see how this division happens.<br>We have four Class C Networks of imaginary Internet Protocol (IP) Addresses like:<br>
<br>Network 1 : 255.147.1.0
<br>Network 2 : 255.147.2.0
<br>Network 3 : 255.147.3.0
<br>Network 4 : 255.147.4.0
<br>We know that each network can produce 254 IP Addresses alone. This means four networks can produce 254 * 4 = 1016 (Thousand and Sixteen ) Internet Protocol Addresses can be formed. But what we require is only thirty Internet Protocol Addresses from each Network. This means we only need hundred and Twenty (120) IP Addresses only.<br>This means 1016 - 120 = 896<br>Eight Hundred and Ninety-Six Addresses created are wasted. So, we need to use the Host ID bits wisely.<br>So, by some calculation we will get to know that if we take 5 bits from each network we will be able to get 30 IP Addresses from each Network.<br>The formula for number of IP Addresses is:<br>The total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.<br>So, now we will consider 5 Host ID Bits.<br>25 - 2 = 30 Internet Protocol Addresses from each Network.<br>So, by considering we can create 30 Usable IP Addresses from each Class C Network.<br>So, now we have 3 more Host ID Bits left over unused. We also have different ways in using these remaining bits.<br><br>
<br>These remaining Host ID Bits can be used to increase the capacity of the IP Addresses to be created in future, if required.
<br>We can also create a new six subnets from each network using these three Host ID Bits.
<br>First method is usually chosen because creation of two different subnets causes wastage of IP Addresses. Let me explain this problem with the help of the above example.<br>Example:<br>The network belongs to Class C Network which has 8 Host ID Bits.<br>
In the above first created Subnet we have only used 30 IP Addresses only.<br>
In the newly created Subnet we have created only 6 IP Addresses only.<br>
This means we have used the full potential of the Class C Network. We might have used the whole 8 bits. But, this is considered as wastage of resources.<br>
This is called wastage because we have now a capacity of 36 IP Addresses to be created.<br>
But, the actual capacity of the Class C is 254 IP Addresses.<br>
This means 254 - 36 = 218 IP Addresses are wasted now because of this Host ID Bits Division.<br>
So, it is better to save the remaining Host ID Bits for future purpose rather than dividing it for these kind of resource wasting purpose.  <br><br>Subnetting, as we all know, separates the network into small subnets. While each subnet permits communication between the devices connected to it, subnets are connected together by routers. The network technology being utilized and the connectivity requirements define the size of a subnet. Each organization is responsible for selecting the number and size of the subnets it produces, within the constraints of the address space available for its use.<br>
<br>For the construction of the subnets, we usually check the MSB (Most Significant Bit) bits of the host ID and if found wrong we make it right. In order to create two network subnets, we fix one of the host's MSB (Most Significant Bit) bits in the table below. We are unable to alter network bits since doing so would alter the entire network.
<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks3.png" referrerpolicy="no-referrer"><br>We need a subnet mask to identify a subnet, which is created by substituting the number "1" for each Network ID bit and the amount of bits we reserve for Host ID to create the subnet. A data packet from the internet is intended to be forwarded to the specified subnet network using the subnet mask.<br>A part of an address should be used as the Subnet ID is also specified by the subnet mask. In order to apply the subnet mask to the whole network address, a binary AND operation is utilized. When performing an AND operation, it is assumed that the result will be "true" if both inputs are. If not, "false" is presented. This is only possible when both bits are 1.<br>The Subnet ID results from this. The Subnet ID is used by routers to choose the best route among the sub - networks.<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks4.png" referrerpolicy="no-referrer"><br>
<br>The two components that make up an IP address are the Network Prefix (sometimes called the Network ID) and the Host ID. Depending on whether the address is Class A, B, or C, either the Network Prefix or the Host ID must be separated. A Class B IPv4 address, 172.16.37.5, is seen in the image below. The Network Prefix is 172.16.0.0, and the Host ID is 37.5.
<br><img alt="Subnetting in Computer Networks" src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks5.png" referrerpolicy="no-referrer"><br>
<br>We use permutations to the amount of bits set aside to form subnets if we wish to produce subnets of varied length. Variable Length Subnet Masking is the name of this subnetting (VLSM).
<br>After setting aside some bits to indicate the subnet, the broadcast address of a subnet is computed by setting all the remaining bits of the host id to 1.The message is sent to all network hosts using the broadcast address.
<br><br>
<br>Subnetting is used to decrease the presence of Internet Protocol (IP) range.
<br>Subnets helps in stopping the devices or gadgets from occupying the whole network, only allowing the hosts to control which kind of user can have access to the important information. Simply, we can tell that network is safe just because of the subnetting concept.
<br>Subnetting concept increases the performance of the total network by deleting the repeated traffic causing errors.
<br>We can convert the whole big network into smaller networks by using the concept of subnetting as discussed earlier.
<br><br>
<br>If the number of subnets increases, then the number of routers must also increase along with the subnet increase number. This happens because each subnet has its own subnet mask, broadcast address and network address.
<br>As told earlier, if we create many subnets many IP Addresses are wasted because of the wastage of Host ID Bits
<br>The cost of the entire network is increased by subnetting, which calls for the acquisition of pricey internal routers, switches, hubs, and bridges, among other things.
<br>The complexity of the network is increased through subnetting. The subnet network must be managed by a skilled network administrator.
<br>This is all about Subnetting Concept in the subject named Computer Networks.]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/subnetting.html</link><guid isPermaLink="false">AWS/For Reading/Subnetting.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://images.javatpoint.com/tutorial/computer-network/images/subnetting-in-computer-networks.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Types of Autoscaling]]></title><description><![CDATA[ 
 <br>source: <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/amazon-web-services-scaling-amazon-ec2/" target="_blank">https://www.geeksforgeeks.org/amazon-web-services-scaling-amazon-ec2/</a><br><br>Auto Scaling is a feature in cloud computing that allows a cloud-based application to automatically adjust the resources it uses such as servers, compute instances based on demand. The goal of Auto Scaling is to ensure that the application has sufficient resources to meet performance goals and maintain availability, while also optimizing resource utilization and minimizing costs. To know the difference between Auto scaling and load balancer refer to the&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/auto-scaling-vs-load-balancer/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/auto-scaling-vs-load-balancer/" target="_blank">Auto Scaling vs Load Balancer</a>.<br><br>AWS auto-scaling is used to scale up and scale down the EC2-instance by depending on the incoming traffic. You can scale up and scale down the applications in a few minutes based on the traffic which will decrease the latency of the application to the end-users.<br><br>Dynamical scaling:&nbsp;AWS auto-scaling service doesn‚Äôt require any type of manual intervention it will automatically scale the application down and up by depending up on the incoming traffic.<br>
<br>**Pay For You Use:**&nbsp;In auto scaling the resource will be utilised in the optimised way where the demand is low the resource utilisation will be low and the demand will high the resource utilisation will increase so the AWS is going to charge you only for the amount of resources you really used.
<br>**Automatic Performance Maintenance:**&nbsp;AWS auto scaling maintains the optimal application performance with considering the workloads it will ensures that the application is running to desired level which will decrease the latency and also the capacity will be increased by based on your application
<br><br>Following are the components of AWS Scaling Components.<br>
<br>**Groups:**For scaling and managing the EC2 instances are grouped together so that they may be thought of as a single logical entity. You can mention the minimum and maximum no.of EC2 instance are required based up on the demand of the incoming traffic.
<br>**Configuration templates:**&nbsp;Configuration template or an launch template which is used by the EC2 autoscaling group for the EC2 instance. In which you can specify the Amazon Machine Image ID,keypair,security group and so on.
<br>**Scaling options: Aws Autoscaling provides no.of options some of them are mentioned as following.**

<br>Dynamic scaling
<br>Predictive scaling
<br>Scheduled scaling
<br>Manual scaling


<br><img alt="Auto-Scaling EC2" src="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png" referrerpolicy="no-referrer"><br>That‚Äôs the point where Amazon EC2 Autoscaling comes into the picture. You may use Amazon EC2 Auto Scaling in order to add or delete Amazon EC2 instances with respect to changes in your application demand. You can maintain a higher feeling of application availability by dynamically scaling your instances in and out as needed.<br><br>You can use three scaling techniques within Amazon EC2 Auto Scaling i.e. Dynamic Scaling, Predictive Scaling, and Scheduled Scaling. They are explained in detail below:<br>
<br>
**Dynamic Scaling: A**dapts to changing environments and responds with the EC2 instances as per the demand. It helps the user to follow the demand curve for the application, which ultimately helps the maintainer/user to scale the instances ahead of time. Target tracking scaling policies, for example, may be used to choose a loaded statistic for your application, such as CPU use. Alternatively, you might use Application Load Balancer‚Äôs new ‚ÄúRequest Count Per Target‚Äù measure, which is a load balancing option for the Elastic Load Balancing service. After that, Amazon EC2 Auto Scaling will modify the number of EC2 instances as needed to keep you on track.&nbsp;

<br>
**Predictive Scaling:**&nbsp;Helps you to schedule the right number of EC2 instances based on the predicted demand. You can use both dynamic and predictive scaling approaches together for faster scaling of the application. Predictive Scaling forecasts future traffic and allocates the appropriate number of EC2 instances ahead of time. Machine learning algorithms in Predictive Scaling identify changes in daily and weekly patterns and automatically update projections. In this way, the need to manually scale the instances on particular days is relieved.&nbsp;

<br>
**Scheduled Scaling:**&nbsp;As the name suggests allows you to scale your application based on the scheduled time you set. For e.g. A coffee shop owner may employ more baristas on weekends because of the increased demand and frees them on weekdays because of reduced demand.

<br>Computing power is a programmed resource in the cloud, so you may take a more flexible approach to scale your applications. When you add Amazon EC2 Auto Scaling to an application, you may create new instances as needed and terminate them when they‚Äôre no longer in use. In this way, you only pay for the instances you use, when they‚Äôre in use.<br><br>
<br>**Horizontal Scaling:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" target="_blank">Horizontal scaling</a>&nbsp;involves adding more instances to your application to handle increased demand. This can be done manually by launching additional instances, or automatically using Amazon EC2 Auto Scaling, which monitors your application‚Äôs workload and adds or removes instances based on predefined rules.
<br>**Vertical Scaling:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/horizontal-and-vertical-scaling-in-databases/" target="_blank">Vertical scaling</a>&nbsp;involves increasing the resources of existing instances, such as CPU, memory, or storage. This can be done manually by resizing instances, or automatically using Amazon EC2 Auto Scaling with launch configurations that specify instance sizes based on the workload.
<br>**Load Balancing:**&nbsp;Load balancing involves distributing incoming traffic across multiple instances to improve performance and availability.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/elastic-load-balancer-in-aws/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/elastic-load-balancer-in-aws/" target="_blank">Amazon Elastic Load Balancing (ELB)</a>&nbsp;is a service that automatically distributes incoming traffic across multiple instances in one or more Availability Zones.
<br>**Multi-Availability Zone Deployment:**&nbsp;Multi-Availability Zone (AZ) deployment involves launching instances in multiple AZs to improve availability and fault tolerance. Amazon EC2 Auto Scaling can be used to automatically launch instances in additional AZs to maintain availability in case of an AZ outage.
<br>**Containerization:**&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/containerization-using-docker/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/containerization-using-docker/" target="_blank">Containerization</a>&nbsp;involves using containers to package and deploy applications, making them more portable and easier to manage.&nbsp;<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/introduction-to-amazon-elastic-container-service-ecs/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/introduction-to-amazon-elastic-container-service-ecs/" target="_blank">Amazon Elastic Container Service (ECS)</a>&nbsp;is a service that makes it easy to run, stop, and manage Docker containers on a cluster of EC2 instances.
<br>Note
When we created the auto-scaling group, we configured the Desired capacity, Minimum capacity, maximum capacity, and CPU utilization. If CPU utilization increases by 60% in all instances, one more instance is created, and if CPU utilization decreases by 30% in all instances, one instance is terminated.
<br><br><br>
AWS auto scaling is an service provided by the AWS which is used to scale the EC2 by depending up the in coming traffic.
<br><br>
Auto scaling is mainly used to scale up and scale down the application based on the load. There are four main types of AWS autoscaling:

<br>manual scaling,
<br>scheduled scaling,
<br>dynamic scaling, and
<br>predictive scaling

<br><br>
The main components of autoscaling was mentioned below.

<br>Load Balancer.
<br>Snapshot.
<br>EC2 (Elastic Compute Cloud) Instance.
<br>Autoscaling group.

<br><br>
AWS Auto Scaling Group Terraform is a module that allows you to create and manage Auto Scaling groups using Terraform.
]]></description><link>https://notes.sarangwandile.xyz/aws/for-reading/types-of-autoscaling.html</link><guid isPermaLink="false">AWS/For Reading/Types of Autoscaling.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://media.geeksforgeeks.org/wp-content/uploads/20230828125811/Auto-Scaling---ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Amazon Web Services In Plain English]]></title><description><![CDATA[ 
 <br>I was searching one liner meaning for aws services and I came across some interesting websites:<br>Meaning of AWS Services in one line

<br><a rel="noopener nofollow" class="external-link" href="https://adayinthelifeof.nl/2020/05/20/aws.html" target="_blank">https://adayinthelifeof.nl/2020/05/20/aws.html</a>
<br><a rel="noopener nofollow" class="external-link" href="https://expeditedsecurity.com/aws-in-plain-english/" target="_blank">https://expeditedsecurity.com/aws-in-plain-english/</a>

<br>Those I need 
EC2 - Virtual Private Servers<br>
Lambda - Programming Functions you can run but can costs a fortune<br>
S3 - File Storage (unmountable)<br>
EFS - Mountable Network Disks<br>
RDS - Managed Relational Database<br>
VPC - Virtual Private Network (consider as VLANs)<br>
DynamoDB - Large &amp; scalable non-relational database<br>
CloudFront - Content Delivery Network<br>
Route 53 - Managed domains names and records<br>
CloudWatch - Monitoring and Logs<br>
Autoscaling - Scale resources dynamically or manually<br>
CloudTrail - Spying on your users (Tracking Users activity)<br>
IAM - Users and their Permissions, policies and roles
]]></description><link>https://notes.sarangwandile.xyz/aws/notes/amazon-web-services-in-plain-english.html</link><guid isPermaLink="false">AWS/Notes/Amazon Web Services In Plain English.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Aws Services Simply Explained]]></title><description><![CDATA[ 
 <br>This is the list of AWS resources required for Entry level devops interview preparation. Sometimes we struggle to explain the definition we already know it's usecase in simpler terms. So I curated this list. The ones written in Italian is my own hand crafted lines while others mostly generated from Gemini Ai.<br>
<br>
EC2 (Elastic Compute Cloud):  Like renting virtual computers in the cloud, you can run whatever software you want on them.
Its like vps or vms in the cloud, they are scalable, flexible and cheaper.

<br>
VPC is like having your own private network within the cloud. It lets you create a secure and isolated space for your resources, like your own little corner of the internet in AWS with all the required tools to create a network such as subnets, route table, internet gateway, VPNs, etc.

<br>
Load Balancer: Distributes traffic across multiple EC2 instances so your application can handle more users and is more resilient.

<br>
CloudWatch:  Monitors your AWS resources and applications, collecting metrics and logs to help you understand performance and troubleshoot issues.
Aws provide cloudwatch service to monitor and manage logs of resources to help user getting better performance and troubleshoot issues.

<br>
CloudFront: A content delivery network (CDN) that makes your websites and applications faster by caching content closer to your users.
Cloudfront is aws managed CDN to caching the content closer to users to make websites and applications faster.

<br>
S3 (Simple Storage Service):  Like a hard drive in the cloud for storing all sorts of files, from websites to backups.
Aws managed storage service like a Google drive to store all sorts of files and store backups

<br>
EFS (Elastic File System):  A network file system that lets you share files between multiple EC2 instances.

<br>
RDS (Relational Database Service): Makes it easy to set up and manage databases in the cloud, like MySQL, PostgreSQL, and Oracle.

<br>
Route 53: A scalable DNS service that translates domain names (like google.com) into IP addresses.

<br>
Lambda: Lets you run code without provisioning or managing servers; you only pay for the compute time you use.<br>
Serverless compute

<br>
SNS (Simple Notification Service): A messaging service that can send notifications to various destinations, like email, SMS, and other applications.

<br>
SQS (Simple Queue Service): Like a post office box in the cloud. You put messages in it (like tasks to be done), and other services can pick them up later. This helps keep different parts of your application loosely connected and make things more reliable.

<br>
EBS (Elastic Block Store): Think of it as a fast hard drive in the cloud. You can attach it to an EC2 instance (your virtual computer) to store data. This is great for things like databases or file storage.
Ebs is like virtual hard drive that can be attached to ec2 instances and its scalable and you can adjust performance and type according to your need.

]]></description><link>https://notes.sarangwandile.xyz/aws/notes/aws-services-simply-explained.html</link><guid isPermaLink="false">AWS/Notes/Aws Services Simply Explained.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 15:50:09 GMT</pubDate></item><item><title><![CDATA[Simple Definitions To give in Interview]]></title><description><![CDATA[ 
 <br><br><br>Linux is a powerful, flexible, and open-source operating system that's widely used in server environments.<br><br>kernel is the core component of an operating system. It acts as bridge between software and hardware.]]></description><link>https://notes.sarangwandile.xyz/aws/notes/simple-definitions-to-give-in-interview.html</link><guid isPermaLink="false">AWS/Notes/Simple Definitions To give in Interview.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item><item><title><![CDATA[‚òëÔ∏è How to store aws load balancer logs in s3 bucket]]></title><description><![CDATA[ 
 <br><br>1.&nbsp;&nbsp;&nbsp; Create 2 instances and&nbsp;add http port and also install nginx enable it .<br>
2.&nbsp;&nbsp;&nbsp; For load balancer u will need target group<br>
3.&nbsp;&nbsp;&nbsp; Go to EC2 service scroll down u will see target groups<br>
<img alt="{8799CC1A-FB3F-4C46-89AC-E9EA4FA22C1E} 1.png" src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"><br>
4.&nbsp;&nbsp;&nbsp; Create target group &gt; add instances &gt; save and create .<br>
<img alt="{122014CE-B98D-499C-83AC-AD171883DF5B}.png" src="https://notes.sarangwandile.xyz/lib/media/{122014ce-b98d-499c-83ac-ad171883df5b}.png"><br>
<img alt="{76992DE6-07D5-4F62-BF3D-8BAC91D82994}.png" src="https://notes.sarangwandile.xyz/lib/media/{76992de6-07d5-4f62-bf3d-8bac91d82994}.png"><br>
5.&nbsp;&nbsp;&nbsp; Now create load balancer (Application load balancer) &gt; configure it give name , vpc ,select availability zones, select Target Groups u created and create load balancer.<br>
<img alt="{B089AE33-8E08-44B4-BF14-89AC63880F3B}.png" src="https://notes.sarangwandile.xyz/lib/media/{b089ae33-8e08-44b4-bf14-89ac63880f3b}.png"><br>
<img alt="{CC268607-25E9-442F-BAC8-171661E1F704}.png" src="https://notes.sarangwandile.xyz/lib/media/{cc268607-25e9-442f-bac8-171661e1f704}.png"><br>
6.&nbsp;&nbsp;&nbsp; Check it your load balancer is working or not.<br>
7.&nbsp;&nbsp;&nbsp; If working , Create s3 bucket&nbsp; and go to permission &gt; &nbsp;bucket policy and edit it :-<br>
8.&nbsp;&nbsp;&nbsp; Note :- &nbsp;Find policy from this by searching on google enable access log of load balancer to s3<br><img alt="Pasted image 20241201200127.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200127.png"><br>
Policy :-<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws-us-gov:iam::elb-account-id:root"
      },
      "Action": "s3:PutObject",
      "Resource": "s3-bucket-arn"
    }
  ]
}
<br><img alt="Pasted image 20241201200139.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200139.png"><br>
9.&nbsp;&nbsp;&nbsp; Save it.<br>10.&nbsp;&nbsp;Now go to load balancer u created &gt; go to attributes section &gt;&nbsp; scroll down and u will see this monitoring sec enable access logs and add our bucket<br><img alt="Pasted image 20241201200203.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200203.png"><br>
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Save changes.<br>
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; U will see ur logs folder in ur s3 bucket.]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-how-to-store-aws-load-balancer-logs-in-s3-bucket.html</link><guid isPermaLink="false">AWS/Tasks Done/‚òëÔ∏è How to store aws load balancer logs in s3 bucket.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚òëÔ∏è Implement autoscaling on memory utilization]]></title><description><![CDATA[ 
 <br><br>
<br>Implement autoscaling on memory utilization 
<br>SNS will notification each time instance scale up and scale down
<br>In short:<br>
Implementing Autoscaling on Memory Utilization with SNS Notifications<br>
Understanding the Components:<br><br>
<br>AWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.
<br>Amazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.
<br>Amazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).
<br><br>
<br>Navigate to the SNS console.
<br>Create a new topic.
<br>Give it a descriptive name (e.g., "AutoScalingNotifications").
<br><img alt="autoscaling-notifications-created.png" src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"><br><br>
<br>Navigate to the EC2 console and select "Auto Scaling Groups".
<br>Create a new Auto Scaling group with the desired configuration:

<br>Launch Configuration: Specify the AMI, instance type, security groups, etc.
<br>Scaling Policies:

<br>Scaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.
<br>Target Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.




<br><img alt="launch-template-creation-snsautoscalinggroup123.png" src="https://notes.sarangwandile.xyz/lib/media/launch-template-creation-snsautoscalinggroup123.png"><br>Created Launch Template with this User data<br>#!/bin/bash
yum install nginx unzip -y
systemctl enable --now nginx
curl -O [https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)
unzip oxer.zip
rm oxer.zip
mv oxer* /usr/share/nginx/html
systemctl restart nginx
<br><img alt="autoscale-group-size.png" src="https://notes.sarangwandile.xyz/lib/media/autoscale-group-size.png"><br>enable sns notification and select previously created sns topic<br>
<img alt="add-notification-in-auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/add-notification-in-auto-scaling-groups.png"><br><img alt="choose-lauch-template-review.png" src="https://notes.sarangwandile.xyz/lib/media/choose-lauch-template-review.png"><br><br>
<br>Navigate to the CloudWatch console.
<br>Create an alarm for the "MemoryUtilization" metric:

<br>Metric Name: MemoryUtilization
<br>Namespace: AWS/EC2
<br>Statistic: Average
<br>Period: 5 Minutes (or as needed)
<br>Threshold: Set the desired threshold (e.g., 80%)
<br>Comparison Operator: Greater Than or Equal To
<br>Alarm Actions: Select the SNS topic created in step 1.


<br>Associate the Alarm with the Auto Scaling Group:

<br>In the Auto Scaling group settings, under "Notifications," add the CloudWatch alarm created in step 3.


<br>Create Alarm<br>
<img alt="create-alarms-cloudwatch.png" src="https://notes.sarangwandile.xyz/lib/media/create-alarms-cloudwatch.png"><br>select memory utilization metric<br>
<img alt="select-metric-cpu-utilization.png" src="https://notes.sarangwandile.xyz/lib/media/select-metric-cpu-utilization.png"><br><img alt="select-matric.png" src="https://notes.sarangwandile.xyz/lib/media/select-matric.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-implement-autoscaling-on-memory-utilization.html</link><guid isPermaLink="false">AWS/Tasks Done/‚òëÔ∏è Implement autoscaling on memory utilization.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sat, 28 Dec 2024 13:29:05 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚òëÔ∏è Task 1 - Implement autoscaling on memory utilization]]></title><description><![CDATA[ 
 <br><br>
<br>Implement autoscaling on memory utilization 
<br>SNS will notification each time instance scale up and scale down
<br>In short:<br>
Implementing Autoscaling on Memory Utilization with SNS Notifications<br>
Understanding the Components:<br><br>
<br>AWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.
<br>Amazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.
<br>Amazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).
<br><br>
<br>Navigate to the SNS console.
<br>Create a new topic.
<br>Give it a descriptive name (e.g., "AutoScalingNotifications").
<br><img alt="autoscaling-notifications-created.png" src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"><br><br>
<br>Navigate to the EC2 console and select "Auto Scaling Groups".
<br>Create a new Auto Scaling group with the desired configuration:

<br>Launch Configuration: Specify the AMI, instance type, security groups, etc.
<br>Scaling Policies:

<br>Scaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.
<br>Target Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.




<br><img alt="launch-template-creation-snsautoscalinggroup123.png" src="https://notes.sarangwandile.xyz/lib/media/launch-template-creation-snsautoscalinggroup123.png"><br>Created Launch Template with this User data<br>#!/bin/bash
yum install nginx unzip -y
systemctl enable --now nginx
curl -O [https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)
unzip oxer.zip
rm oxer.zip
mv oxer* /usr/share/nginx/html
systemctl restart nginx
<br><img alt="autoscale-group-size.png" src="https://notes.sarangwandile.xyz/lib/media/autoscale-group-size.png"><br>enable sns notification and select previously created sns topic<br>
<img alt="add-notification-in-auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/add-notification-in-auto-scaling-groups.png"><br><img alt="choose-lauch-template-review.png" src="https://notes.sarangwandile.xyz/lib/media/choose-lauch-template-review.png"><br><br>
<br>Navigate to the CloudWatch console.
<br>Create an alarm for the "MemoryUtilization" metric:

<br>Metric Name: MemoryUtilization
<br>Namespace: AWS/EC2
<br>Statistic: Average
<br>Period: 5 Minutes (or as needed)
<br>Threshold: Set the desired threshold (e.g., 80%)
<br>Comparison Operator: Greater Than or Equal To
<br>Alarm Actions: Select the SNS topic created in step 1.


<br>Associate the Alarm with the Auto Scaling Group:

<br>In the Auto Scaling group settings, under "Notifications," add the CloudWatch alarm created in step 3.


<br>Create Alarm<br>
<img alt="create-alarms-cloudwatch.png" src="https://notes.sarangwandile.xyz/lib/media/create-alarms-cloudwatch.png"><br>select memory utilization metric<br>
<img alt="select-metric-cpu-utilization.png" src="https://notes.sarangwandile.xyz/lib/media/select-metric-cpu-utilization.png"><br><img alt="select-matric.png" src="https://notes.sarangwandile.xyz/lib/media/select-matric.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-task-1-implement-autoscaling-on-memory-utilization.html</link><guid isPermaLink="false">AWS/Tasks Done/‚òëÔ∏è Task 1 - Implement autoscaling on memory utilization.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sat, 28 Dec 2024 13:29:05 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚òëÔ∏è Task 4 - How to store aws load balancer logs in s3 bucket]]></title><description><![CDATA[ 
 <br><br>1.&nbsp;&nbsp;&nbsp; Create 2 instances and&nbsp;add http port and also install nginx enable it .<br>
2.&nbsp;&nbsp;&nbsp; For load balancer u will need target group<br>
3.&nbsp;&nbsp;&nbsp; Go to EC2 service scroll down u will see target groups<br>
<img alt="{8799CC1A-FB3F-4C46-89AC-E9EA4FA22C1E} 1.png" src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"><br>
4.&nbsp;&nbsp;&nbsp; Create target group &gt; add instances &gt; save and create .<br>
<img alt="{122014CE-B98D-499C-83AC-AD171883DF5B}.png" src="https://notes.sarangwandile.xyz/lib/media/{122014ce-b98d-499c-83ac-ad171883df5b}.png"><br>
<img alt="{76992DE6-07D5-4F62-BF3D-8BAC91D82994}.png" src="https://notes.sarangwandile.xyz/lib/media/{76992de6-07d5-4f62-bf3d-8bac91d82994}.png"><br>
5.&nbsp;&nbsp;&nbsp; Now create load balancer (Application load balancer) &gt; configure it give name , vpc ,select availability zones, select Target Groups u created and create load balancer.<br>
<img alt="{B089AE33-8E08-44B4-BF14-89AC63880F3B}.png" src="https://notes.sarangwandile.xyz/lib/media/{b089ae33-8e08-44b4-bf14-89ac63880f3b}.png"><br>
<img alt="{CC268607-25E9-442F-BAC8-171661E1F704}.png" src="https://notes.sarangwandile.xyz/lib/media/{cc268607-25e9-442f-bac8-171661e1f704}.png"><br>
6.&nbsp;&nbsp;&nbsp; Check it your load balancer is working or not.<br>
7.&nbsp;&nbsp;&nbsp; If working , Create s3 bucket&nbsp; and go to permission &gt; &nbsp;bucket policy and edit it :-<br>
8.&nbsp;&nbsp;&nbsp; Note :- &nbsp;Find policy from this by searching on google enable access log of load balancer to s3<br><img alt="Pasted image 20241201200127.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200127.png"><br>
Policy :-<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws-us-gov:iam::elb-account-id:root"
      },
      "Action": "s3:PutObject",
      "Resource": "s3-bucket-arn"
    }
  ]
}
<br><img alt="Pasted image 20241201200139.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200139.png"><br>
9.&nbsp;&nbsp;&nbsp; Save it.<br>10.&nbsp;&nbsp;Now go to load balancer u created &gt; go to attributes section &gt;&nbsp; scroll down and u will see this monitoring sec enable access logs and add our bucket<br><img alt="Pasted image 20241201200203.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201200203.png"><br>
11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Save changes.<br>
12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; U will see ur logs folder in ur s3 bucket.]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚òëÔ∏è-task-4-how-to-store-aws-load-balancer-logs-in-s3-bucket.html</link><guid isPermaLink="false">AWS/Tasks Done/‚òëÔ∏è Task 4 - How to store aws load balancer logs in s3 bucket.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{8799cc1a-fb3f-4c46-89ac-e9ea4fa22c1e}-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Create 5 IAM users and 5 S3 buckets and attach them each other]]></title><description><![CDATA[ 
 <br>Task:<br>
requirement: <br>
<br>Create 5 I am users
<br>Create 5 S3 Buckets
<br>assign each user their name specific bucket
<br>no one should able touch other users bucket
<br>
<br>Created User s3-mango with default settings
<br>Created s3 bucket s3-mango-bucket with default settings<br>
after creation added following policy to bucket
<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::970547378605:user/s3-mango"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::s3-mango-bucket",
        "arn:aws:s3:::s3-mango-bucket/*"
      ]
    }
  ]
}
<br>Step 4: Assign IAM Policies to Users<br>
Example for Alice:<br>
Go to the IAM Console and open Alice.<br>
Click Add Permissions, then Attach Policies Directly.<br>
Create a custom policy for Alice:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::s3-mango-bucket",
        "arn:aws:s3:::s3-mango-bucket/*"
      ]
    }
  ]
}
<br>Attach this policy to Alice.<br>granted =AmazonS3ReadOnlyAccess= to each users<br><br>Certainly! Let's go step-by-step with specific names for users and their buckets to make it clearer.<br>Scenario<br>
We want to:<br>Create 5 users: Alice, Bob, Charlie, Dave, and Eve.<br>
Create 5 buckets: alice-bucket, bob-bucket, charlie-bucket, dave-bucket, and eve-bucket.<br>
Ensure that:<br>
Alice can access only alice-bucket.<br>
Bob can access only bob-bucket.<br>
Similarly, each user has access only to their bucket.<br>
Step 1: Create Users<br>
Manually create the following users via the IAM Console:<br>Alice<br>
Bob<br>
Charlie<br>
Dave<br>
Eve<br>
Step 2: Create Buckets<br>
Manually create the following buckets via the S3 Console:<br>alice-bucket<br>
bob-bucket<br>
charlie-bucket<br>
dave-bucket<br>
eve-bucket<br>
Step 3: Set Permissions for Each Bucket<br>
Example for Alice:<br>
Go to the S3 Console and open alice-bucket.<br>Go to the Permissions tab, then Bucket Policy.<br>Add this bucket policy:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ACCOUNT_ID:user/Alice"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::alice-bucket",
        "arn:aws:s3:::alice-bucket/*"
      ]
    }
  ]
}
<br>Replace ACCOUNT_ID with your AWS account ID.<br>Save the policy.<br>Repeat this step for:<br>Bob with bob-bucket<br>
Charlie with charlie-bucket<br>
Dave with dave-bucket<br>
Eve with eve-bucket<br>
Step 4: Assign IAM Policies to Users<br>
Example for Alice:<br>
Go to the IAM Console and open Alice.<br>
Click Add Permissions, then Attach Policies Directly.<br>
Create a custom policy for Alice:<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::alice-bucket",
        "arn:aws:s3:::alice-bucket/*"
      ]
    }
  ]
}
<br>Attach this policy to Alice.<br>
Repeat this step for:<br>Bob with bob-bucket<br>
Charlie with charlie-bucket<br>
Dave with dave-bucket<br>
Eve with eve-bucket<br>
Step 5: Test Access<br>
Log in as Alice and try to:<br>
Upload a file to alice-bucket (should work).<br>
Access bob-bucket (should fail).<br>
Repeat the process for Bob, Charlie, Dave, and Eve to ensure they can only access their respective buckets.<br>
How This Works<br>
Each user has a bucket policy on their bucket that allows only them to access it.<br>
Each user also has an IAM policy that allows them to access only their bucket.<br>
This ensures that the permissions are restricted at both the bucket level and the user level.<br>
Let me know if you need help applying this setup!<br>
creating bucket<br>
<br>bucket name: s3-mango bucket
<br>object ownership -<br>
ACLs disabled (recommended)<br>Block Public Access settings for this bucket<br>
block all public access turn on<br>
<br>
Create user =mango=<br>
password- myPassw0rd

<br>
Created policy called =my-s3-mango-bucket-policy=<br>
Permissions defined in this policy

<br> {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::s3-mango-bucket"
        }
    ]
}
<br>attached this policy to user<br><img alt="IAM-user-creating-review-and-create.png" src="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-create-5-iam-users-and-5-s3-buckets-and-attach-them-each-other.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Create 5 IAM users and 5 S3 buckets and attach them each other.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/iam-user-creating-review-and-create.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Create notification for s3 bucket activity happen of put and delete]]></title><description><![CDATA[ 
 <br><br>Whenever a new object is added or deleted in s3 bucket you will get notification via email<br><br><img alt="Create-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"><br><br><img alt="create-topic.png" src="https://notes.sarangwandile.xyz/lib/media/create-topic.png"><br>
Create an topic &gt; go to access policy and add a policy<br>&nbsp;<br>
<img alt="sns-create-topic-access-policy.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-topic-access-policy.png"><br>Click on Advanced and Paste this code.<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "SNS:Publish",
      "Resource": "arn:aws:sns:us-west-2:970547378605:sns-s3-topic123",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::snsbucket123testing"
        }
      }
    }
  ]
}
<br>Tip
In above JSON code edit "Resource"  block to replace the ARN id according to your sns topic's Arn id and aws:SourceArn to your S3 bucket's ARN id.<br>
or in last two fields just change your AWS account id and topic name<br>
"Resource": "arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;"<br>
"aws:SourceArn": "arn:aws:s3:::&lt;s3-bucket-name&gt;"
<br><br>   After Topic is created Create Subscription to get notifications <br><img alt="sns-create-subscription.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-subscription.png"><br>
Set:<br>
<br>Topic ARN: Your previously created Topic's ARN ID
<br>Protocol: Email<br>
- Endpoint: Your Gmail address<br>
Click on create subscription button
<br>After your subscription is created, you'll get a mail to confirm it.<br><img alt="email-subscription-confirmation.png" src="https://notes.sarangwandile.xyz/lib/media/email-subscription-confirmation.png"><br>
Goto your mail and click on Confirm Subscription link.<br>
<img alt="sns-subscription-mail.png" src="https://notes.sarangwandile.xyz/lib/media/sns-subscription-mail.png"><br><br>Now go to your buckets<br>
<br>
Select Your Bucket:

<br>
Choose the bucket for which you want to set up notifications.

<br>
Go to the Properties tab.<br>
<img alt="snsbucket123testing-overview.png" src="https://notes.sarangwandile.xyz/lib/media/snsbucket123testing-overview.png">

<br>
Scroll down to Event notifications and click Create event notification.<br>
<img alt="creating-snsbucket123testing-event-notification.png" src="https://notes.sarangwandile.xyz/lib/media/creating-snsbucket123testing-event-notification.png">

<br>
Click on Create event notification 

<br>
Event name: put_and_delete_event

<br>
Event types: 

<br>Select put checkbox
<br>Permanently Deleted checkbox<br>
<img alt="Pasted image 20241201210604.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201210604.png">


<br>
Scroll down u will see Destination Option 

<br>select SNS topic 
<br>choose your previously created SNS Topic 
<br>Click on Save changes<br>
<img alt="selecting-sns-topic-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-sns-topic-s3-bucket.png"><br>
Try adding new files in bucket you should get notification in your mail.<br>
<img alt="uploading-files-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/uploading-files-s3-bucket.png">


<br><img alt="sns-topic-email-notification.png" src="https://notes.sarangwandile.xyz/lib/media/sns-topic-email-notification.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-create-notification-for-s3-bucket-activity-happen-of-put-and-delete.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Create notification for s3 bucket activity happen of put and delete.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-bucket.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Get sns alert when any user launches new instance]]></title><description><![CDATA[ 
 <br><br>Whenever u r launching instance u will get a event and also u have to get details<br><br>
<br>
Create Topic

<br>Type:  Standard
<br>Name: NotifyonInstanceLaunch


<br>
Create subscription 

<br>Select Topic ARN
<br>Protocol: E-mail 
<br>Endpoint: <a data-tooltip-position="top" aria-label="mailto:archsarangx@gmail.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:archsarangx@gmail.com" target="_blank">archsarangx@gmail.com</a>
<br>check your mails and confirm subscription 

4.&nbsp;&nbsp;&nbsp; Go to Amazon Event bridge service .<br>
5.&nbsp;&nbsp;&nbsp; In buses section go to rules and Create rule<br>
<img alt="create-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"><br>
In step 2 Build event pattern scroll down to bottom<br>
In Event Pattern section add configuration like this:

<br>
Event Source: AWS Service

<br>
AWS Service: EC2

<br>
Event Type: EC2 Instance State-change Notification<br>
<img alt="event-pattern-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/event-pattern-eventbridge-rules.png"><br>
Click Next and in Step 3 Select Target page Select

<br>
Target Type: AWS Service

<br>
Select a target: SNS Topic

<br>
Topic: your topic name

<br><img alt="selecting-target-amazon-eventbridge-rules-creation.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-target-amazon-eventbridge-rules-creation.png"><br>
9.&nbsp;&nbsp;&nbsp; Create target and at last create rule<br>&nbsp;You will get a notifications while Launching and terminating instance]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-get-sns-alert-when-any-user-launches-new-instance.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Get sns alert when any user launches new instance.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Host static website on s3 bucket]]></title><description><![CDATA[ 
 <br><br>
<br>Create a bucket
<br>add website template inside bucket
<br>enable static website hosting option
<br>view the website
<br><br><img alt="create-a-bucket-button.png" src="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png"><br>
<br>Goto aws S3 bucket creation page
<br>Click on create bucket button
<br>Select Bucket Type as General Purpose
<br>Give any Bucket Name of your desire (name should be globally unique)
<br><img alt="create-bucket-s3-cdec-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket-s3-cdec-bucket.png"><br>
<br>Here I set s3-cdec-bucket
<br>Leave default setting as it is like ACLs disabled on 
<br>Clear all Block&nbsp;all&nbsp;public access option
<br>I set bucket versioning enabled
<br>Leave other setting as it is and click on create bucket
<br>Here my s3 bucket is created<br>
<img alt="created-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/created-bucket.png"><br><br>You can upload your website files if you have but I'll use free website template<br>
from <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/" target="_blank">https://www.free-css.com/</a><br>Download any template from this site and upload the extracted content from the zip over the bucket<br>I downloaded this template called browny <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/free-css-templates/page296/browny" target="_blank">https://www.free-css.com/free-css-templates/page296/browny</a><br>
<img alt="download-free-css-template-zip.png" src="https://notes.sarangwandile.xyz/lib/media/download-free-css-template-zip.png"><br>Its gave me zip file.<br>
I'll be extracting it and uploading the folder into the bucket<br><img alt="extract-template-zip.png" src="https://notes.sarangwandile.xyz/lib/media/extract-template-zip.png"><br>It contains these files<br>
<img alt="template-files-downloaded.png" src="https://notes.sarangwandile.xyz/lib/media/template-files-downloaded.png"><br>
<br>Lets upload the files into the root of the s3 bucket
<br><img alt="s3-cdec-bucket-uploading-files.png" src="https://notes.sarangwandile.xyz/lib/media/s3-cdec-bucket-uploading-files.png"><br>Click on your bucket name in s3 bucket list and click on upload button<br>
then click on add files<br>you can also drag n drop files and folder <br>Click on Add files to add files and Add folder to upload folder<br>
<img alt="upload-files.png" src="https://notes.sarangwandile.xyz/lib/media/upload-files.png"><br>After the files are shown like this finally click on upload button<br>After the files gets uploaded click on your s3 bucket name and goto properties and<br>
<img alt="s3-cdec-bucket-properties-tab.png" src="https://notes.sarangwandile.xyz/lib/media/s3-cdec-bucket-properties-tab.png"><br>and scroll down till you see Static Website Hosting option<br><img alt="static-website-hosting.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-hosting.png"><br>click on edit button and enable it<br>
after enabling it select Hosting type as host a static website<br>
<img alt="enabling-static-website-hosting.png" src="https://notes.sarangwandile.xyz/lib/media/enabling-static-website-hosting.png"><br>and specify the index document as index.html<br>
<img alt="index-document.png" src="https://notes.sarangwandile.xyz/lib/media/index-document.png"><br>
Now Static website option should be updated with the link to access our newly hosted static website link<br><img alt="static-website-enabled-success.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-enabled-success.png"><br>
Click on the Bucket website endpoint url or open it in another tab<br><br>but before this link to even work we need to specify a permission policy to be able to see our site<br>Under the permission tab click Edit bucket policy<br>
copy and paste this policy and save changes<br>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::your-bucket-name/*"
            ]
        }
    ]
}

<br>The provided JSON defines an AWS IAM policy that grants public read access to all objects in an S3 bucket named "your-bucket-name".<br>replace your-bucket-name with your bucket name<br>save changes and see the link now your site should be visible.<br>
now our website is visible over this url <a rel="noopener nofollow" class="external-link" href="http://s3-cdec-bucket.s3-website-us-west-2.amazonaws.com/" target="_blank">http://s3-cdec-bucket.s3-website-us-west-2.amazonaws.com/</a><br><img alt="static-website-success-final-image.png" src="https://notes.sarangwandile.xyz/lib/media/static-website-success-final-image.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-host-static-website-on-s3-bucket.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Host static website on s3 bucket.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-a-bucket-button.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Implement Template with Scheduled Autoscaling]]></title><description><![CDATA[ 
 <br>In this task we will be launching instance with userdata attach for setuping and starting nginx sever. We will create template out of that instance attach it to the autoscaling target groups. So when stress will be added to instance it should create another instances to balance the load.<br><br>
<br>‚úÖ Launch instance 
<br>‚úÖ Pass userdata 
<br>‚úÖ Install nginx 
<br>‚úÖ Create template from instance 
<br>‚úÖ Implement that template into auto scaling 
<br>‚úÖ Create scheduled autoscaling 
<br>‚úÖ Start instance 
<br>‚úÖ Add stress into instance 
<br>‚úÖ Upgrade the template to nginx to httpd
<br><br>
<br>Name: MyEc2Instance
<br>AMI: Amazon Linux (Free Tier)
<br>Instance Type: t2.micro (Free Tier)
<br>Select Key pair
<br>Select Common Security Group (which has 80, 22 port enabled)
<br>Click on Advanced Details and add following Userdata
<br>#!/bin/bash
sudo yum install nginx unzip -y
sudo systemctl enable --now nginx
sudo curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip
sudo unzip oxer.zip
sudo rm oxer.zip
sudo mv oxer* /usr/share/nginx/html
sudo systemctl restart nginx
<br>Click on Launch Instance.<br><br>
<br>‚úÖ Nginx page was successfully visible on root 
<br>‚úÖ And our template site is also visible on <a rel="noopener nofollow" class="external-link" href="http://35.160.157.246/oxer-html/" target="_blank">http://35.160.157.246/oxer-html/</a><br>
<img alt="welcome-to-nginx.png" src="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png">
<br><img alt="html-template-final-site.png" src="https://notes.sarangwandile.xyz/lib/media/html-template-final-site.png"><br><br>To create Launch Template from running instance<br>
Select your Instance Click Actions &gt;&gt; Image and Templates &gt;&gt; Create Templates from Instance<br>
<img alt="creating-launch-template-from-instance.png" src="https://notes.sarangwandile.xyz/lib/media/creating-launch-template-from-instance.png"><br>
Give Your template name and similar setting you gave in Step 1 and in Advanced tab make sure to add previous bash script as Userdata and create launch Template<br>
<img alt="summary-create-laumch-template.png" src="https://notes.sarangwandile.xyz/lib/media/summary-create-laumch-template.png"><br><br>In Ec2 Instance Left Sidebar scroll down to see Auto Scaling groups option<br>
<img alt="auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/auto-scaling-groups.png"><br>
<br>Click on Create AutoScaling group
<br>Give name, Select Launch Template
<br>Choose availability Zones
<br>No load balancer
<br>No VPC Lattice
<br>Set Health check grace period to 100
<br>Desired Capacity: 1
<br>Max desired capacity: 3
<br>For Autoscaling Policy Select Target Tracking Policy
<br>Metric Type: Average CPU Utillization
<br>Target Value: 30
<br>Instance Warmup: 80 Seconds<br>
Click on Create
<br>After Creation Click on Template Name and in Automatic Scaling Tab select Scheduled Actions option<br>
<img alt="scheduled-actions.png" src="https://notes.sarangwandile.xyz/lib/media/scheduled-actions.png"><br>Create Scheduled Action<br><img alt="scheduled-autoscaling.png" src="https://notes.sarangwandile.xyz/lib/media/scheduled-autoscaling.png"><br>Now it at that specific time your scheduled autoscaling will work the desired way and you'll get 2 instance launched minimum and maximum 2 if needed.]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-implement-template-with-scheduled-autoscaling.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Implement Template with Scheduled Autoscaling.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/welcome-to-nginx.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Monitoring Nginx logs with Cloudwatch]]></title><description><![CDATA[ 
 <br>A webserver like nginx can have some important logs that is crucial to the security of server. but as cloud provider its not convenient to always login manually to check the logs file yourself. to automate this task Cloudwatch can be a huge help to monitor the direct logs generated by nginx and other webserver. <br>Here we will see how to set up Logs monitoring with Cloudwatch.<br><br>
<br>install nginx 
<br>activate service
<br>check port 80 on browser to generate some logs
<br>check if these files get created
<br>sudo ls /var/log/nginx/
access.log  error.log
<br><br>
<br>create  IAM role
<br>give that role ec2-describe permission
<br>attach role to instance
<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br><br>
<br>install agent
<br>sudo yum install amazon-cloudwatch-agent -y
<br>
<br>enable agent service
<br>systemctl enable --now amazon-cloudwatch-agent
<br>
<br>Goto this directory
<br>cd /opt/aws/amazon-cloudwatch-agent/bin/
<br>
<br>create configuration file
<br>sudo ./amazon-cloudwatch-agent-config-wizard
<br>after finishing whole setup it will generate config.json file in that folder<br>Log file path:<br>/var/log/nginx/access.log
<br>config.json generated in bin folder<br>Try validating this config with amazon-cloudwatch-agent<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>Check over cloudwatch Log events should be visible like this<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png"><br><br>if you get error of collectd folder or file not found<br>
create an empty collectd file in that said location<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a>
]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖ-monitoring-nginx-logs-with-cloudwatch.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖ Monitoring Nginx logs with Cloudwatch.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖüêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2]]></title><description><![CDATA[ 
 <br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br>
Also add security group rules for port 3306 for database<br>
<img alt="{26BC9CA5-9BB0-4D39-9B13-454B992FE55F}.png" src="https://notes.sarangwandile.xyz/lib/media/{26bc9ca5-9bb0-4d39-9b13-454b992fe55f}.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br><br><a rel="noopener nofollow" class="external-link" href="https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql" target="_blank">https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql</a><br><br>CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';
grant create, alter, drop, insert, update, delete, select on demo.persons to 'sammy'@'localhost';
grant select on demo.* to 'sammy'@'localhost';           
show grants for 'sammy'@'localhost';

<br><br>Install tomcat, git, maven<br><br>Install this specific version from source<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 

<br>hit the instance ip:8080 / curl the ip<br>
You should see tomcat webpage<br>
<img alt="{BF5D483E-E314-42B8-A790-36EC904C637B}.png" src="https://notes.sarangwandile.xyz/lib/media/{bf5d483e-e314-42b8-a790-36ec904c637b}.png"><br><br>sudo yum install git -y
<br><br>sudo yum install maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br>build application<br>cd student-ui
mvn clean package
<br>this must have created .war file in target folder<br>
rename it to suitable short name and move to apache-tomcat's webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war ../apache-tomcat-9.0.97/webapps/
<br>hit the instance ip:8080/studentapp<br>
<img alt="{17627F7C-22F9-448B-B926-BE826D130D98}.png" src="https://notes.sarangwandile.xyz/lib/media/{17627f7c-22f9-448b-b926-be826d130d98}.png"><br>Upon filling the form if you get this<br>
<img alt="{9A94ABEB-EA83-48BB-A58B-36203206528C}.png" src="https://notes.sarangwandile.xyz/lib/media/{9a94abeb-ea83-48bb-a58b-36203206528c}.png"><br>
It means you have not connected RDS with this webapp<br>
lets do this<br><br>mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br><br>CREATE DATABASE studentapp;
use studentapp;
CREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,
	student_name VARCHAR(100) NOT NULL,
    student_addr VARCHAR(100) NOT NULL,
	student_age VARCHAR(3) NOT NULL,
	student_qual VARCHAR(20) NOT NULL,
	student_percent VARCHAR(10) NOT NULL,
	student_year_passed VARCHAR(10) NOT NULL,
	PRIMARY KEY (student_id)
);
<br>show database;
use studentapp;
show tables;
desc students;
<br><img alt="{E35F872A-AB74-43BC-BACD-DE202683E43E}.png" src="https://notes.sarangwandile.xyz/lib/media/{e35f872a-ab74-43bc-bacd-de202683e43e}.png"><br>Ir should look like this<br>
<img alt="{5ABCCE77-EF84-4615-957E-4DD5E23E430E}.png" src="https://notes.sarangwandile.xyz/lib/media/{5abcce77-ef84-4615-957e-4dd5e23e430e}.png"><br><br>goto conf folder in apache-tomcat source folder<br>cd ../conf
#and edit context.xml file
vi context.xml
<br>and paste this code between &lt;context&gt;&lt;/context&gt; block<br>&lt;Resource name="jdbc/TestDB" auth="Container" type="javax.sql.DataSource"
           maxTotal="500" maxIdle="30" maxWaitMillis="1000"
           username="admin" password="Passw0rd123" driverClassName="com.mysql.jdbc.Driver"
           url="jdbc:mysql://database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com:3306/studentapp?useUnicode=yes&amp;amp;characterEncoding=utf8"/&gt;
<br>make sure to edit username and password values with your own and url to your hostaname of database<br><br>copy this url - <a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar</a><br>Place the mysql connector library (mysql-connector.jar) into /lib folder of apache-tomcat directory by using curl or wget<br>cd ../lib
curl -O https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar
<br>now that library file is placed into its correct location lets restart catalina<br>
but first navigate back to bin directory where catalina.sh binary is located<br>cd ../bin
bash catalina.sh stop
bash catalina.sh start
<br>Now navigate to browser with this url<br>
ec2-instance-ip:8080/studentapp<br>now registration data should be able to passed into database<br>After the filling the registration form login to mysql database and check if the data is present in the table now.<br>mysql -h &lt;rds endpoint&gt; -u admin -p
show databases;
use studentapp;
show tables;
select * from students;
]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖüêà‚Äç‚¨õhost-tomcat-basesd-web-app-called-student-app-with-rds-and-ec2.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖüêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖCreate notification for s3 bucket activity happen of put and delete]]></title><description><![CDATA[ 
 <br><br>Whenever a new object is added or deleted in s3 bucket you will get notification via email<br><br><img alt="Create-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"><br><br><img alt="create-topic.png" src="https://notes.sarangwandile.xyz/lib/media/create-topic.png"><br>
Create an topic &gt; go to access policy and add a policy<br>&nbsp;<br>
<img alt="sns-create-topic-access-policy.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-topic-access-policy.png"><br>Click on Advanced and Paste this code.<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "SNS:Publish",
      "Resource": "arn:aws:sns:us-west-2:970547378605:sns-s3-topic123",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::snsbucket123testing"
        }
      }
    }
  ]
}
<br>Tip
In above JSON code edit "Resource"  block to replace the ARN id according to your sns topic's Arn id and aws:SourceArn to your S3 bucket's ARN id.<br>
or in last two fields just change your AWS account id and topic name<br>
"Resource": "arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;"<br>
"aws:SourceArn": "arn:aws:s3:::&lt;s3-bucket-name&gt;"
<br><br>   After Topic is created Create Subscription to get notifications <br><img alt="sns-create-subscription.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-subscription.png"><br>
Set:<br>
<br>Topic ARN: Your previously created Topic's ARN ID
<br>Protocol: Email<br>
- Endpoint: Your Gmail address<br>
Click on create subscription button
<br>After your subscription is created, you'll get a mail to confirm it.<br><img alt="email-subscription-confirmation.png" src="https://notes.sarangwandile.xyz/lib/media/email-subscription-confirmation.png"><br>
Goto your mail and click on Confirm Subscription link.<br>
<img alt="sns-subscription-mail.png" src="https://notes.sarangwandile.xyz/lib/media/sns-subscription-mail.png"><br><br>Now go to your buckets<br>
<br>
Select Your Bucket:

<br>
Choose the bucket for which you want to set up notifications.

<br>
Go to the Properties tab.<br>
<img alt="snsbucket123testing-overview.png" src="https://notes.sarangwandile.xyz/lib/media/snsbucket123testing-overview.png">

<br>
Scroll down to Event notifications and click Create event notification.<br>
<img alt="creating-snsbucket123testing-event-notification.png" src="https://notes.sarangwandile.xyz/lib/media/creating-snsbucket123testing-event-notification.png">

<br>
Click on Create event notification 

<br>
Event name: put_and_delete_event

<br>
Event types: 

<br>Select put checkbox
<br>Permanently Deleted checkbox<br>
<img alt="Pasted image 20241201210604.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201210604.png">


<br>
Scroll down u will see Destination Option 

<br>select SNS topic 
<br>choose your previously created SNS Topic 
<br>Click on Save changes<br>
<img alt="selecting-sns-topic-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-sns-topic-s3-bucket.png"><br>
Try adding new files in bucket you should get notification in your mail.<br>
<img alt="uploading-files-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/uploading-files-s3-bucket.png">


<br><img alt="sns-topic-email-notification.png" src="https://notes.sarangwandile.xyz/lib/media/sns-topic-email-notification.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖcreate-notification-for-s3-bucket-activity-happen-of-put-and-delete.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖCreate notification for s3 bucket activity happen of put and delete.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-bucket.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖTask 7 - Get sns alert when any user launches new instance]]></title><description><![CDATA[ 
 <br><br>Whenever u r launching instance u will get a event and also u have to get details<br><br>
<br>
Create Topic

<br>Type:  Standard
<br>Name: NotifyonInstanceLaunch


<br>
Create subscription 

<br>Select Topic ARN
<br>Protocol: E-mail 
<br>Endpoint: <a data-tooltip-position="top" aria-label="mailto:archsarangx@gmail.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:archsarangx@gmail.com" target="_blank">archsarangx@gmail.com</a>
<br>check your mails and confirm subscription 

4.&nbsp;&nbsp;&nbsp; Go to Amazon Event bridge service .<br>
5.&nbsp;&nbsp;&nbsp; In buses section go to rules and Create rule<br>
<img alt="create-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"><br>
In step 2 Build event pattern scroll down to bottom<br>
In Event Pattern section add configuration like this:

<br>
Event Source: AWS Service

<br>
AWS Service: EC2

<br>
Event Type: EC2 Instance State-change Notification<br>
<img alt="event-pattern-eventbridge-rules.png" src="https://notes.sarangwandile.xyz/lib/media/event-pattern-eventbridge-rules.png"><br>
Click Next and in Step 3 Select Target page Select

<br>
Target Type: AWS Service

<br>
Select a target: SNS Topic

<br>
Topic: your topic name

<br><img alt="selecting-target-amazon-eventbridge-rules-creation.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-target-amazon-eventbridge-rules-creation.png"><br>
9.&nbsp;&nbsp;&nbsp; Create target and at last create rule<br>&nbsp;You will get a notifications while Launching and terminating instance]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/‚úÖtask-7-get-sns-alert-when-any-user-launches-new-instance.html</link><guid isPermaLink="false">AWS/Tasks Done/‚úÖTask 7 - Get sns alert when any user launches new instance.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-eventbridge-rules.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[üêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2]]></title><description><![CDATA[ 
 <br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br>
Also add security group rules for port 3306 for database<br>
<img alt="{26BC9CA5-9BB0-4D39-9B13-454B992FE55F}.png" src="https://notes.sarangwandile.xyz/lib/media/{26bc9ca5-9bb0-4d39-9b13-454b992fe55f}.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br><br><a rel="noopener nofollow" class="external-link" href="https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql" target="_blank">https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql</a><br><br>CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';
grant create, alter, drop, insert, update, delete, select on demo.persons to 'sammy'@'localhost';
grant select on demo.* to 'sammy'@'localhost';           
show grants for 'sammy'@'localhost';

<br><br>Install tomcat, git, maven<br><br>Install this specific version from source<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 

<br>hit the instance ip:8080 / curl the ip<br>
You should see tomcat webpage<br>
<img alt="{BF5D483E-E314-42B8-A790-36EC904C637B}.png" src="https://notes.sarangwandile.xyz/lib/media/{bf5d483e-e314-42b8-a790-36ec904c637b}.png"><br><br>sudo yum install git -y
<br><br>sudo yum install maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br>build application<br>cd student-ui
mvn clean package
<br>this must have created .war file in target folder<br>
rename it to suitable short name and move to apache-tomcat's webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war ../apache-tomcat-9.0.97/webapps/
<br>hit the instance ip:8080/studentapp<br>
<img alt="{17627F7C-22F9-448B-B926-BE826D130D98}.png" src="https://notes.sarangwandile.xyz/lib/media/{17627f7c-22f9-448b-b926-be826d130d98}.png"><br>Upon filling the form if you get this<br>
<img alt="{9A94ABEB-EA83-48BB-A58B-36203206528C}.png" src="https://notes.sarangwandile.xyz/lib/media/{9a94abeb-ea83-48bb-a58b-36203206528c}.png"><br>
It means you have not connected RDS with this webapp<br>
lets do this<br><br>mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br><br>CREATE DATABASE studentapp;
use studentapp;
CREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,
	student_name VARCHAR(100) NOT NULL,
    student_addr VARCHAR(100) NOT NULL,
	student_age VARCHAR(3) NOT NULL,
	student_qual VARCHAR(20) NOT NULL,
	student_percent VARCHAR(10) NOT NULL,
	student_year_passed VARCHAR(10) NOT NULL,
	PRIMARY KEY (student_id)
);
<br>show database;
use studentapp;
show tables;
desc students;
<br><img alt="{E35F872A-AB74-43BC-BACD-DE202683E43E}.png" src="https://notes.sarangwandile.xyz/lib/media/{e35f872a-ab74-43bc-bacd-de202683e43e}.png"><br>Ir should look like this<br>
<img alt="{5ABCCE77-EF84-4615-957E-4DD5E23E430E}.png" src="https://notes.sarangwandile.xyz/lib/media/{5abcce77-ef84-4615-957e-4dd5e23e430e}.png"><br><br>goto conf folder in apache-tomcat source folder<br>cd ../conf
#and edit context.xml file
vi context.xml
<br>and paste this code between &lt;context&gt;&lt;/context&gt; block<br>&lt;Resource name="jdbc/TestDB" auth="Container" type="javax.sql.DataSource"
           maxTotal="500" maxIdle="30" maxWaitMillis="1000"
           username="admin" password="Passw0rd123" driverClassName="com.mysql.jdbc.Driver"
           url="jdbc:mysql://database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com:3306/studentapp?useUnicode=yes&amp;amp;characterEncoding=utf8"/&gt;
<br>make sure to edit username and password values with your own and url to your hostaname of database<br><br>copy this url - <a rel="noopener nofollow" class="external-link" href="https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar" target="_blank">https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar</a><br>Place the mysql connector library (mysql-connector.jar) into /lib folder of apache-tomcat directory by using curl or wget<br>cd ../lib
curl -O https://s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar
<br>now that library file is placed into its correct location lets restart catalina<br>
but first navigate back to bin directory where catalina.sh binary is located<br>cd ../bin
bash catalina.sh stop
bash catalina.sh start
<br>Now navigate to browser with this url<br>
ec2-instance-ip:8080/studentapp<br>now registration data should be able to passed into database<br>After the filling the registration form login to mysql database and check if the data is present in the table now.<br>mysql -h &lt;rds endpoint&gt; -u admin -p
show databases;
use studentapp;
show tables;
select * from students;
]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/üêà‚Äç‚¨õhost-tomcat-basesd-web-app-called-student-app-with-rds-and-ec2.html</link><guid isPermaLink="false">AWS/Tasks Done/üêà‚Äç‚¨õHost tomcat basesd web app called Student-app with RDS and ec2.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[ü§î Task 1 - Implement autoscaling on memory utilization]]></title><description><![CDATA[ 
 <br><br>Implement autoscaling on memory utilization trigger notification each time instance scale up and scale down with sns<br>Implementing Autoscaling on Memory Utilization with SNS Notifications<br>
Understanding the Components:<br>
<br>AWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.
<br>Amazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.
<br>Amazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).
<br><br>
<br>Navigate to the SNS console.
<br>Create a new topic.
<br>Give it a descriptive name (e.g., "AutoScalingNotifications").
<br><img alt="autoscaling-notifications-created.png" src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"><br><br>
<br>Navigate to the EC2 console and select "Auto Scaling Groups".
<br>Create a new Auto Scaling group with the desired configuration:

<br>Launch Configuration: Specify the AMI, instance type, security groups, etc.
<br>Scaling Policies:

<br>Scaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.
<br>Target Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.




<br><img alt="launch-template-creation-snsautoscalinggroup123.png" src="https://notes.sarangwandile.xyz/lib/media/launch-template-creation-snsautoscalinggroup123.png"><br>Created Launch Template with this User data<br>#!/bin/bash
yum install nginx unzip -y
systemctl enable --now nginx
curl -O [https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](https://www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)
unzip oxer.zip
rm oxer.zip
mv oxer* /usr/share/nginx/html
systemctl restart nginx
<br><img alt="autoscale-group-size.png" src="https://notes.sarangwandile.xyz/lib/media/autoscale-group-size.png"><br>enable sns notification and select previously created sns topic<br>
<img alt="add-notification-in-auto-scaling-groups.png" src="https://notes.sarangwandile.xyz/lib/media/add-notification-in-auto-scaling-groups.png"><br><img alt="choose-lauch-template-review.png" src="https://notes.sarangwandile.xyz/lib/media/choose-lauch-template-review.png"><br><br>
<br>Navigate to the CloudWatch console.
<br>Create an alarm for the "MemoryUtilization" metric:

<br>Metric Name: MemoryUtilization
<br>Namespace: AWS/EC2
<br>Statistic: Average
<br>Period: 5 Minutes (or as needed)
<br>Threshold: Set the desired threshold (e.g., 80%)
<br>Comparison Operator: Greater Than or Equal To
<br>Alarm Actions: Select the SNS topic created in step 1.


<br>Associate the Alarm with the Auto Scaling Group:

<br>In the Auto Scaling group settings, under "Notifications," add the CloudWatch alarm created in step 3.


<br>Create Alarm<br>
<img alt="create-alarms-cloudwatch.png" src="https://notes.sarangwandile.xyz/lib/media/create-alarms-cloudwatch.png"><br>select memory utilization metric<br>
<img alt="select-metric-cpu-utilization.png" src="https://notes.sarangwandile.xyz/lib/media/select-metric-cpu-utilization.png"><br><img alt="select-matric.png" src="https://notes.sarangwandile.xyz/lib/media/select-matric.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/ü§î-task-1-implement-autoscaling-on-memory-utilization.html</link><guid isPermaLink="false">AWS/Tasks Done/ü§î Task 1 - Implement autoscaling on memory utilization.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/autoscaling-notifications-created.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Create notification for s3 bucket activity happen of put and delete]]></title><description><![CDATA[ 
 <br><br>Whenever a new object is added or deleted in s3 bucket you will get notification via email<br><br><img alt="Create-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"><br><br><img alt="create-topic.png" src="https://notes.sarangwandile.xyz/lib/media/create-topic.png"><br>
Create an topic &gt; go to access policy and add a policy<br>&nbsp;<br>
<img alt="sns-create-topic-access-policy.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-topic-access-policy.png"><br>Click on Advanced and Paste this code.<br>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "SNS:Publish",
      "Resource": "arn:aws:sns:us-west-2:970547378605:sns-s3-topic123",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::snsbucket123testing"
        }
      }
    }
  ]
}
<br>Tip
In above JSON code edit "Resource"  block to replace the ARN id according to your sns topic's Arn id and aws:SourceArn to your S3 bucket's ARN id.<br>
or in last two fields just change your AWS account id and topic name<br>
"Resource": "arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;"<br>
"aws:SourceArn": "arn:aws:s3:::&lt;s3-bucket-name&gt;"
<br><br>   After Topic is created Create Subscription to get notifications <br><img alt="sns-create-subscription.png" src="https://notes.sarangwandile.xyz/lib/media/sns-create-subscription.png"><br>
Set:<br>
<br>Topic ARN: Your previously created Topic's ARN ID
<br>Protocol: Email<br>
- Endpoint: Your Gmail address<br>
Click on create subscription button
<br>After your subscription is created, you'll get a mail to confirm it.<br><img alt="email-subscription-confirmation.png" src="https://notes.sarangwandile.xyz/lib/media/email-subscription-confirmation.png"><br>
Goto your mail and click on Confirm Subscription link.<br>
<img alt="sns-subscription-mail.png" src="https://notes.sarangwandile.xyz/lib/media/sns-subscription-mail.png"><br><br>Now go to your buckets<br>
<br>
Select Your Bucket:

<br>
Choose the bucket for which you want to set up notifications.

<br>
Go to the Properties tab.<br>
<img alt="snsbucket123testing-overview.png" src="https://notes.sarangwandile.xyz/lib/media/snsbucket123testing-overview.png">

<br>
Scroll down to Event notifications and click Create event notification.<br>
<img alt="creating-snsbucket123testing-event-notification.png" src="https://notes.sarangwandile.xyz/lib/media/creating-snsbucket123testing-event-notification.png">

<br>
Click on Create event notification 

<br>
Event name: put_and_delete_event

<br>
Event types: 

<br>Select put checkbox
<br>Permanently Deleted checkbox<br>
<img alt="Pasted image 20241201210604.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241201210604.png">


<br>
Scroll down u will see Destination Option 

<br>select SNS topic 
<br>choose your previously created SNS Topic 
<br>Click on Save changes<br>
<img alt="selecting-sns-topic-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/selecting-sns-topic-s3-bucket.png"><br>
Try adding new files in bucket you should get notification in your mail.<br>
<img alt="uploading-files-s3-bucket.png" src="https://notes.sarangwandile.xyz/lib/media/uploading-files-s3-bucket.png">


<br><img alt="sns-topic-email-notification.png" src="https://notes.sarangwandile.xyz/lib/media/sns-topic-email-notification.png">]]></description><link>https://notes.sarangwandile.xyz/aws/tasks-done/create-notification-for-s3-bucket-activity-happen-of-put-and-delete.html</link><guid isPermaLink="false">AWS/Tasks Done/Create notification for s3 bucket activity happen of put and delete.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/create-bucket.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/create-bucket.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[‚úÖ Tasks To Do]]></title><description><![CDATA[ 
 <br>Practical Tasks to perform<br>‚úÖ = finished<br>
‚òëÔ∏è = done but not satisfied<br><br><br><br><br>]]></description><link>https://notes.sarangwandile.xyz/aws/‚úÖ-tasks-to-do.html</link><guid isPermaLink="false">AWS/‚úÖ Tasks To Do.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:47:03 GMT</pubDate></item><item><title><![CDATA[üóíÔ∏è Tasks to Read]]></title><description><![CDATA[ 
 <br>üìÑ Tasks to read<br>
üè° Read the topics in home.<br><br>Youtube Channel Recommendations <br>
<br>nehra classes
<br>gaurav sharma
<br>abhishek veermalla
]]></description><link>https://notes.sarangwandile.xyz/aws/üóíÔ∏è-tasks-to-read.html</link><guid isPermaLink="false">AWS/üóíÔ∏è Tasks to Read.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:45:40 GMT</pubDate></item><item><title><![CDATA[ü§ø Tasks To Do]]></title><description><![CDATA[ 
 <br>Practical Tasks to perform<br>‚úÖ = finished<br>
‚òëÔ∏è = done but not satisfied<br><br>Week's Assignment<br><br>Ongoing Task<br>]]></description><link>https://notes.sarangwandile.xyz/aws/ü§ø-tasks-to-do.html</link><guid isPermaLink="false">AWS/ü§ø Tasks To Do.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sat, 28 Dec 2024 13:27:34 GMT</pubDate></item><item><title><![CDATA[2024-11-18]]></title><description><![CDATA[ 
 <br>EC2  <br>
<br>instances
<br>security groups
<br>key
<br>ebs, snapshot backup, ami
<br>lifecycle policy
<br>load balancer 
<br>auto scaling groups
<br>VPC<br>
<br>vpc 
<br>subnet 
<br>route table
<br>network
<br>igw
<br>nat gat
<br>publi priv
<br>peering
<br>IAM<br>
<br>Identity Access Management
<br>Idenity ---&gt; user<br>
Permissions --&gt; policy<br>group ---&gt; developer<br>
devops<br>
<br>
Two types of policies<br>
- aws managed policy<br>
- customer managed policy<br>iam components<br>
user<br>
group<br>
policy<br>
role<br>tasks:<br>
<br>direct policy
<br>inline policy
<br>To read

<br>difference between inline policy and customer managed policy
<br>Types of policies in IAM
<br>Types of users in aws in IAM

<br>Types of policy<br>
<br>AWS Managed Policy
<br>Customer Managed Policy
<br>Direct Inline Policy
<br>Types of User<br>
<br>IAM
<br>Root User
<br>IAM Identity Center User
<br>Federeted Identity
<br>AWS Builder ID User
<br>Policy Types<br>
<br>Identity Based
<br>Resource Based
<br>Service control policies
<br>Permission Boundary Policies
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-18.html</link><guid isPermaLink="false">Daily Notes/2024-11-18.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[2024-11-18 EC2 VPC and stuffs]]></title><description><![CDATA[ 
 <br>EC2  <br>
<br>instances
<br>security groups
<br>key
<br>ebs, snapshot backup, ami
<br>lifecycle policy
<br>load balancer 
<br>auto scaling groups
<br>VPC<br>
<br>vpc 
<br>subnet 
<br>route table
<br>network
<br>igw
<br>nat gat
<br>publi priv
<br>peering
<br>IAM<br>
<br>Identity Access Management
<br>Idenity ---&gt; user<br>
Permissions --&gt; policy<br>group ---&gt; developer<br>
devops<br>
<br>
Two types of policies<br>
- aws managed policy<br>
- customer managed policy<br>iam components<br>
user<br>
group<br>
policy<br>
role<br>tasks:<br>
<br>direct policy
<br>inline policy
<br>To read

<br>difference between inline policy and customer managed policy
<br>Types of policies in IAM
<br>Types of users in aws in IAM

<br>Types of policy<br>
<br>AWS Managed Policy
<br>Customer Managed Policy
<br>Direct Inline Policy
<br>Types of User<br>
<br>IAM
<br>Root User
<br>IAM Identity Center User
<br>Federeted Identity
<br>AWS Builder ID User
<br>Policy Types<br>
<br>Identity Based
<br>Resource Based
<br>Service control policies
<br>Permission Boundary Policies
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-18-ec2-vpc-and-stuffs.html</link><guid isPermaLink="false">Daily Notes/2024-11-18 EC2 VPC and stuffs.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3]]></title><description><![CDATA[ 
 <br><br>
S3 is global service<br>
but bucket is region specific
<br>in bucket things stored in the form of object<br>storage classes<br><a rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/s3/storage-classes/" target="_blank">https://aws.amazon.com/s3/storage-classes/</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-19.html</link><guid isPermaLink="false">Daily Notes/2024-11-19.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3 Task]]></title><description><![CDATA[ 
 <br><br>
<br>Create an IAM Policy:

<br>First, create an IAM policy that defines the permissions for the specific S3 bucket(s).
<br>This policy should follow the principle of least privilege, granting only the necessary permissions.


<br>Attach the Policy to the IAM User:

<br>Go to the IAM console and select the user.
<br>Choose "Add permissions" and then "Attach existing policies directly".
<br>Find and select the policy you created for S3 bucket access.


<br>Example Policy (customize as needed):
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}


<br>    
4. Security Considerations:
    - Always use specific bucket ARNs instead of wildcards.
    - Grant only the necessary permissions (e.g., read-only if that's all that's required).
    - Consider using IAM groups for easier management if multiple users need the same access.
	
5. Best Practices:
    - Regularly review and audit permissions.
    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.
    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.
	
6. Testing:
    - After applying the policy, test the access in a safe, non-production environment.
    - Verify that the user can only access the intended buckets and perform only the allowed actions.

Remember, it's crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.


# Static web hosting
Create s3 bucket
properties --&gt; enable static web hosting
permission --&gt; turn off block public access
					--&gt; object ownership --&gt; enable ACL
					--&gt; ACL --&gt; enable everyone list and read
					--&gt; upload files

]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-22.html</link><guid isPermaLink="false">Daily Notes/2024-11-22.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[S3 Task]]></title><description><![CDATA[ 
 <br><br>
<br>Create an IAM Policy:

<br>First, create an IAM policy that defines the permissions for the specific S3 bucket(s).
<br>This policy should follow the principle of least privilege, granting only the necessary permissions.


<br>Attach the Policy to the IAM User:

<br>Go to the IAM console and select the user.
<br>Choose "Add permissions" and then "Attach existing policies directly".
<br>Find and select the policy you created for S3 bucket access.


<br>Example Policy (customize as needed):
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}


<br>    
4. Security Considerations:
    - Always use specific bucket ARNs instead of wildcards.
    - Grant only the necessary permissions (e.g., read-only if that's all that's required).
    - Consider using IAM groups for easier management if multiple users need the same access.
	
5. Best Practices:
    - Regularly review and audit permissions.
    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.
    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.
	
6. Testing:
    - After applying the policy, test the access in a safe, non-production environment.
    - Verify that the user can only access the intended buckets and perform only the allowed actions.

Remember, it's crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.


# Static web hosting
Create s3 bucket
properties --&gt; enable static web hosting
permission --&gt; turn off block public access
					--&gt; object ownership --&gt; enable ACL
					--&gt; ACL --&gt; enable everyone list and read
					--&gt; upload files

]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-22-s3-task.html</link><guid isPermaLink="false">Daily Notes/2024-11-22 s3 task.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Containerizing-wordpress-app]]></title><description><![CDATA[ 
 <br><br>and migrating from database<br><br>Dockerfile for wordpress<br>
<img alt="hello-ss-wordpress-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"><br>docker-compose file<br>
<img alt="docker-compose-file-for-wordpress.png" src="https://notes.sarangwandile.xyz/lib/media/docker-compose-file-for-wordpress.png"><br>mysql docker file<br>
<img alt="hello-ss-mysql-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-mysql-container.png"><br>myPassw0rd<br>'wpuser123'@'localhost' IDENTIFIED BY 'wpuserPassw0rd';<br>|Site Title|wp-testsite|<br>
|Username|wpwebuser |<br>
|password|)5S@PsG1J$VACnJ@%Q|<br>
|email|<a data-tooltip-position="top" aria-label="mailto:myemail@example.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:myemail@example.com" target="_blank">myemail@example.com</a>|<br>
|||<br>Docker mariadb password<br>
wpuser123Passw0rd<br><a rel="noopener nofollow" class="external-link" href="https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/" target="_blank">https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/</a><br><a rel="noopener nofollow" class="external-link" href="https://devhints.io/mysql" target="_blank">https://devhints.io/mysql</a><br><a rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">https://docs.docker.com/engine/install/ubuntu/</a><br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a><br>#exporting/backuping old mysql data
mysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql

#current mysql configuration
mysql user: wpuser123
mysql passwd: wpuserPassw0rd
db name: new_db

#login to ec2 and run the backup command
mysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql

<br><br>
<br>create new folder in wordpress<br>
cd into it
<br>mkdir wordpress &amp;&amp; cd wordpress
<br>
<br>
pull wordpress and mariadb containers<br>
sudo docker pull wordpress<br>
sudo docker pull mariadb

<br>
create custom docker network<br>
sudo docker network wp-app<br>
sudo docker network ls

<br>
Creation of mariadb database container

<br>
<br>hash the password of db first
<br>echo `openssl passwd` | sudo tee $PWD/.db-pass
<br>  now hashed password file must be in .db-pass file<br>
here i have set myPassw0rd password<br>cat .db-pass
$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0
<br>
<br>Create mariaDB data directory to mount container database with /var/lib/mysql<br>
sudo mkdir -p data
<br>~/wordpress 
	- data
<br>
<br>Create and run Mariadb Docker container
<br>sudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v '/home/ubuntu/wordpress/data:/var/lib/mysql' --name wp-mariadb mariadb
<br>
<br>check health and logs<br>
docker ps
<br>sudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log
<br>example:<br>sudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log
<br>
<br>check mounted volume for generated data<br>
ls -1 data
<br>regarding migration of old database<br>
copy the backup file into container db folder<br>
cp mywpold_db.sql wordpress/data/<br>
<br>login to mariadb container
<br>docker exec -it wp-mariadb bash
<br>restore old db<br>cd /var/lib/mysql
mariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811" target="_blank">https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/backup-and-restore-overview/" target="_blank">https://mariadb.com/kb/en/backup-and-restore-overview/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217" target="_blank">https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-23.html</link><guid isPermaLink="false">Daily Notes/2024-11-23.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Containerizing-wordpress-app]]></title><description><![CDATA[ 
 <br><br>and migrating from database<br><br>Dockerfile for wordpress<br>
<img alt="hello-ss-wordpress-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"><br>docker-compose file<br>
<img alt="docker-compose-file-for-wordpress.png" src="https://notes.sarangwandile.xyz/lib/media/docker-compose-file-for-wordpress.png"><br>mysql docker file<br>
<img alt="hello-ss-mysql-container.png" src="https://notes.sarangwandile.xyz/lib/media/hello-ss-mysql-container.png"><br>myPassw0rd<br>'wpuser123'@'localhost' IDENTIFIED BY 'wpuserPassw0rd';<br>|Site Title|wp-testsite|<br>
|Username|wpwebuser |<br>
|password|)5S@PsG1J$VACnJ@%Q|<br>
|email|<a data-tooltip-position="top" aria-label="mailto:myemail@example.com" rel="noopener nofollow" class="external-link" href="https://notes.sarangwandile.xyz/mailto:myemail@example.com" target="_blank">myemail@example.com</a>|<br>
|||<br>Docker mariadb password<br>
wpuser123Passw0rd<br><a rel="noopener nofollow" class="external-link" href="https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/" target="_blank">https://linux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/</a><br><a rel="noopener nofollow" class="external-link" href="https://devhints.io/mysql" target="_blank">https://devhints.io/mysql</a><br><a rel="noopener nofollow" class="external-link" href="https://docs.docker.com/engine/install/ubuntu/" target="_blank">https://docs.docker.com/engine/install/ubuntu/</a><br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a><br>#exporting/backuping old mysql data
mysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql

#current mysql configuration
mysql user: wpuser123
mysql passwd: wpuserPassw0rd
db name: new_db

#login to ec2 and run the backup command
mysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql

<br><br>
<br>create new folder in wordpress<br>
cd into it
<br>mkdir wordpress &amp;&amp; cd wordpress
<br>
<br>
pull wordpress and mariadb containers<br>
sudo docker pull wordpress<br>
sudo docker pull mariadb

<br>
create custom docker network<br>
sudo docker network wp-app<br>
sudo docker network ls

<br>
Creation of mariadb database container

<br>
<br>hash the password of db first
<br>echo `openssl passwd` | sudo tee $PWD/.db-pass
<br>  now hashed password file must be in .db-pass file<br>
here i have set myPassw0rd password<br>cat .db-pass
$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0
<br>
<br>Create mariaDB data directory to mount container database with /var/lib/mysql<br>
sudo mkdir -p data
<br>~/wordpress 
	- data
<br>
<br>Create and run Mariadb Docker container
<br>sudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v '/home/ubuntu/wordpress/data:/var/lib/mysql' --name wp-mariadb mariadb
<br>
<br>check health and logs<br>
docker ps
<br>sudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log
<br>example:<br>sudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log
<br>
<br>check mounted volume for generated data<br>
ls -1 data
<br>regarding migration of old database<br>
copy the backup file into container db folder<br>
cp mywpold_db.sql wordpress/data/<br>
<br>login to mariadb container
<br>docker exec -it wp-mariadb bash
<br>restore old db<br>cd /var/lib/mysql
mariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/" target="_blank">https://kifarunix.com/how-to-deploy-wordpress-as-a-docker-container/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811" target="_blank">https://mariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mariadb.com/kb/en/backup-and-restore-overview/" target="_blank">https://mariadb.com/kb/en/backup-and-restore-overview/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217" target="_blank">https://www.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/?rdt=49217</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-23-does-not-belong-here.html</link><guid isPermaLink="false">Daily Notes/2024-11-23 does not belong here.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/hello-ss-wordpress-container.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-25]]></title><description><![CDATA[ 
 <br>aws configure<br>
aws cli<br>to create new bucket<br>
aws s3 mb s3://spiderman-cdec-devops123<br>to list all buckets<br>
aws s3 ls]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-25.html</link><guid isPermaLink="false">Daily Notes/2024-11-25.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[2024-11-26]]></title><description><![CDATA[ 
 <br>cloudwatch<br>after installing nginx logs must be present<br>sudo ls /var/log/nginx/
access.log  error.log
<br>to send these logs to cloudwatch<br>create role<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br>attach role to instance<br>install agent<br>
<a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a><br>enable agent service<br>
systemctl <br>cd /opt/aws<br>create configuration file<br>
sudo /opt/aws/amazon-cloudwatch-agent/b<br>
in/amazon-cloudwatch-agent-config-wizard<br>Log file path:<br>
/var/log/nginx/access.log<br>config.json generated in bin folder<br>autorized vs authenticated read<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>recap<br>
<br>Created ec2-instance
<br>install nginx
<br>ec2-describe permission<br>collectd file create<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png">]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-26.html</link><guid isPermaLink="false">Daily Notes/2024-11-26.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-26 Cloudwatch class]]></title><description><![CDATA[ 
 <br>cloudwatch<br>after installing nginx logs must be present<br>sudo ls /var/log/nginx/
access.log  error.log
<br>to send these logs to cloudwatch<br>create role<br><img alt="creating-role-for-ec2.png" src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"><br>attach role to instance<br>install agent<br>
<a rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html" target="_blank">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html</a><br>enable agent service<br>
systemctl <br>cd /opt/aws<br>create configuration file<br>
sudo /opt/aws/amazon-cloudwatch-agent/b<br>
in/amazon-cloudwatch-agent-config-wizard<br>Log file path:<br>
/var/log/nginx/access.log<br>config.json generated in bin folder<br>autorized vs authenticated read<br><img alt="amazon-cloudwatch-agent-configuration-file-validation-succeded.png" src="https://notes.sarangwandile.xyz/lib/media/amazon-cloudwatch-agent-configuration-file-validation-succeded.png"><br>./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json
<br>recap<br>
<br>Created ec2-instance
<br>install nginx
<br>ec2-describe permission<br>collectd file create<br><img alt="cloudwatch-log-events.png" src="https://notes.sarangwandile.xyz/lib/media/cloudwatch-log-events.png">]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-26-cloudwatch-class.html</link><guid isPermaLink="false">Daily Notes/2024-11-26 Cloudwatch class.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/creating-role-for-ec2.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-27]]></title><description><![CDATA[ 
 <br>sns - notification service<br>
attach filter before creating alarm<br>
create alarm<br>get graph<br>
get notification]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-27.html</link><guid isPermaLink="false">Daily Notes/2024-11-27.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate></item><item><title><![CDATA[Creating RDS database]]></title><description><![CDATA[ 
 <br><br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br>Scratchpad
HA - high availability<br>
n-1<br>
3306 maridb sql<br>
sudo yum list<br>
installing mariadb client<br>
yum install mariadb105<br>
diagram<br>
default vpc<br>
3 public subnet<br>
1 subnet --&gt; ec2<br>
2 subnet --&gt; rds<br>
endpoint access<br>
mysql -h  -u  -p&gt; 
<br><img alt="rds-diagram-1.png" src="https://notes.sarangwandile.xyz/lib/media/rds-diagram-1.png"><br>create database demo;

show database;

use demo;

drop database demo;

CREATE TABLE Persons (  
    PersonID int,  
    LastName varchar(255),  
    FirstName varchar(255),  
    Address varchar(255),  
    City varchar(255)  
);

desc Persons;

Insert Into Persons (PersonID, LastName, FirstName) values (1, "DemoName", "Amir");

select * from Persons;

https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql

1. CREATE USER ' ==sammy=='@'localhost' IDENTIFIED BY '==password==';

grant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;

grant select on demo.* to "sammy"@"%";           

show grants for 

<br>install tomcat
build application


create user
CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';

show user
SELECT User, Host FROM mysql.user;



]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-28.html</link><guid isPermaLink="false">Daily Notes/2024-11-28.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Creating RDS database]]></title><description><![CDATA[ 
 <br><br><br><img alt="rds-logos-database-selections.png" src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"><br>
<br>Goto RDS dashboard and click on Create Database 
<br>Select Standard create for database creation method
<br>I chose MariaDB engine
<br>Engine Version selected n-1 that is one step behind the latest one
<br>Select Free Tier Template
<br>I leave database-1 as DB instance identifier name
<br>Self Managed Credential Management
<br>and inputed desired Master Password i.e Passw0rd123
<br>In instance configuration I selected Burstable classes for DB instance class
<br>and selected db.t3.micro
<br>for storage I selected General Purpose SSD gp2 and allocated 20GB of storage
<br>I left default setting as it is and clicked on Create Database
<br><img alt="database-created-successfully.png" src="https://notes.sarangwandile.xyz/lib/media/database-created-successfully.png"><br><br><img alt="launching-ec2-instance-button.png" src="https://notes.sarangwandile.xyz/lib/media/launching-ec2-instance-button.png"><br>
<br>Launch ec2 instance
<br>I gave name of ec2 instance db-test
<br>Selected Amazon linux free tier 
<br>and added security group for 3306 port<br>
[make sure to have same security group for both ec2 and rds]
<br>Clicked on create instance
<br>Log into instance
<br><br>
<br>After login to ec2 install mariadb client
<br>yum install mariadb105
<br>
<br>Login to endpoint of RDS we just created
<br>mysql -h &lt;hostname&gt; -u &lt;user&gt; -p
mysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p
<br>Input the password and congrats you just logged into your fresh sql database.<br><img alt="mariadb-installed-success.png" src="https://notes.sarangwandile.xyz/lib/media/mariadb-installed-success.png"><br>Scratchpad
HA - high availability<br>
n-1<br>
3306 maridb sql<br>
sudo yum list<br>
installing mariadb client<br>
yum install mariadb105<br>
diagram<br>
default vpc<br>
3 public subnet<br>
1 subnet --&gt; ec2<br>
2 subnet --&gt; rds<br>
endpoint access<br>
mysql -h  -u  -p&gt; 
<br><img alt="rds-diagram-1.png" src="https://notes.sarangwandile.xyz/lib/media/rds-diagram-1.png"><br>create database demo;

show database;

use demo;

drop database demo;

CREATE TABLE Persons (  
    PersonID int,  
    LastName varchar(255),  
    FirstName varchar(255),  
    Address varchar(255),  
    City varchar(255)  
);

desc Persons;

Insert Into Persons (PersonID, LastName, FirstName) values (1, "DemoName", "Amir");

select * from Persons;

https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql

1. CREATE USER ' ==sammy=='@'localhost' IDENTIFIED BY '==password==';

grant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;

grant select on demo.* to "sammy"@"%";           

show grants for 

<br>install tomcat
build application


create user
CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'password';

show user
SELECT User, Host FROM mysql.user;



]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-28-creating-rds.html</link><guid isPermaLink="false">Daily Notes/2024-11-28 Creating RDS.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/rds-logos-database-selections.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-11-30]]></title><description><![CDATA[ 
 <br><br>download zip file<br>
unzip it<br>sudo yum install java-17<br>Info
in ubuntu java packages are called<br>
jdk and jre<br>
java development kit and java runtime
<br>cd into extracted folder<br>cd bin<br>bash catalina.sh start<br>git clone <a rel="noopener nofollow" class="external-link" href="https://github.com/cholekulche/BE-application-student.git" target="_blank">https://github.com/cholekulche/BE-application-student.git</a><br>cd into it<br>
install maven<br>
sudo yum install maven<br>build appliaction<br>
mvn clean package<br>Info
first install java before maven because<br>
maven will install java latest version which might conflict with required versions
<br>cd target<br>code<br>
artifact application<br> mv studentapp-2.2-SNAPSHOT.war studentapp.war<br> mv studentapp.war ../../apache-tomcat-9.0.97/webapps/.<br>cd ../../apache-tomcat-9.0.97/webapps/<br>.war file will get extracted automatically into student app<br>check ip:8080/studentapp<br><img alt="student-app-final-page.png" src="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png"><br>continue to <a data-href="2024-12-02" href="https://notes.sarangwandile.xyz/daily-notes/2024-12-02.html" class="internal-link" target="_self" rel="noopener nofollow">2024-12-02</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-11-30.html</link><guid isPermaLink="false">Daily Notes/2024-11-30.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/student-app-final-page.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-02]]></title><description><![CDATA[ 
 <br>continued from <a data-href="2024-11-30" href="https://notes.sarangwandile.xyz/daily-notes/2024-11-30.html" class="internal-link" target="_self" rel="noopener nofollow">2024-11-30</a><br>create database<br>
copy whole command<br>show database;<br>use studentapp;<br>show tables;<br><br>cd apache-tomcat/conf<br>
vi context.xml<br>edit jdbc connector <br>&lt;Resource Name ... &gt;
		  edit stuffs
		  adding rds hosting etc
<br>url/mysql-connector.jar<br>
put it into tomcat/lib/<br>curl -O url<br>bash catalina stop<br>
bash catalina start<br>restart catalina <br>try filling form<br>if get error check catalina*.log<br><br>pritam sir repo student-ui app<br><br>route53]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-02.html</link><guid isPermaLink="false">Daily Notes/2024-12-02.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-03]]></title><description><![CDATA[ 
 <br><br>ip to name<br>
name to ip <br>dns record max limit 10k <br>route53 &gt;&gt; hosted zone &gt;&gt; create<br>
update nameserver in godaddy<br>
<img alt="{78B1E5B8-5F9F-4C45-A995-997C82837901} 1.png" src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png"><br>
<br>before updating nameserver<br>
<img alt="{E7EF4A45-A02D-40A7-BEA5-6D5466BDCC60}.png" src="https://notes.sarangwandile.xyz/lib/media/{e7ef4a45-a02d-40a7-bea5-6d5466bdcc60}.png">
<br>create s3 bucket<br>
enable static web hosting add files <br>route53 &gt;&gt; hosted zone &gt;&gt; domain name &gt;&gt; create records<br>skip subdomain select A records<br>
alias on<br>
alias to s3 bucket endpoint<br>
region select<br>skip s3 bucket part<br>
create ec2 instance<br>
install nginx<br>
make site available<br>add ip in values in create record option<br>learndevops.live<br> search acm service in aws<br>
certificate manager<br>create<br>
create records in r53<br>routing policy<br>
simple routing]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-03.html</link><guid isPermaLink="false">Daily Notes/2024-12-03.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-04]]></title><description><![CDATA[ 
 <br>
<br>deploy tomcat instance --&gt; no database attach
<br>make ami of it
<br>autoscaling with load balancer
<br>mount efs which will have tomcat
<br>two instances
<br><img alt="{78B1E5B8-5F9F-4C45-A995-997C82837901}.png" src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png"><br>
new task implement failover policy on nginx instance<br>To read --&gt; blue green deployment strategy <br>manually implement it on instance <br>Toread -- &gt;canary deployment]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-04.html</link><guid isPermaLink="false">Daily Notes/2024-12-04.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{78b1e5b8-5f9f-4c45-a995-997c82837901}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-07]]></title><description><![CDATA[ 
 <br><br>cache<br>CDN - content delivery network<br>x86 and arm<br><br>name to ip 
ip to name
<br>.in .com -- &gt; registrar<br>doc.aws.amazon.com  --&gt; it reads from right to left<br>Task: read how url works<br>latency - time between request and response<br>edge location (cdn location)]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-07.html</link><guid isPermaLink="false">Daily Notes/2024-12-07.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-10]]></title><description><![CDATA[ 
 <br>cloudfront vs cloudwatch<br>cloudfront<br>
cdn<br>
cache<br>cloudwatch<br>
logs monitoring tool<br>s3<br>
loadbalancer<br>autoscaling<br><br>lambda<br>serverless --&gt; managed by aws<br>lambda function to start and stop ec2 instances<br>boto3 library]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-10.html</link><guid isPermaLink="false">Daily Notes/2024-12-10.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-11]]></title><description><![CDATA[ 
 <br><br><br>
<br>Make new user
<br>attach policy according to task
<br>granular level permission
<br><br>user will create s3 bucket, instance, db, 
instance will have:
	tomcat link to rds
	its logs goes to s3 bucket
	tomcat will be accessible via domain and not via instance ip
routing policy simple
user cant have admin access 
user cant access iam
<br><br><br>  Created custom Policy policy4tom and added permission for ec2 full access<br>{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "VisualEditor0",
			"Effect": "Allow",
			"Action": [
				"rds:*",
				"s3:*",
				"ec2:*"
			],
			"Resource": "*"
		}
	]
}
<br>and set permission boundary for Administrator access<br>
<img alt="Pasted image 20241213154148.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"><br>Created EC2 Instance with security group rule for port 8080 enabled<br>
and added this user data<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 
<br>after running instance check if tomcat page shows in port 8080<br><img alt="{51C73FFF-C61A-4231-BA6B-783F6BD22F4D}.png" src="https://notes.sarangwandile.xyz/lib/media/{51c73fff-c61a-4231-ba6b-783f6bd22f4d}.png"><br>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-11.html</link><guid isPermaLink="false">Daily Notes/2024-12-11.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-11 AWS Task]]></title><description><![CDATA[ 
 <br><br><br>
<br>Make new user
<br>attach policy according to task
<br>granular level permission
<br><br>user will create s3 bucket, instance, db, 
instance will have:
	tomcat link to rds
	its logs goes to s3 bucket
	tomcat will be accessible via domain and not via instance ip
routing policy simple
user cant have admin access 
user cant access iam
<br><br><br>  Created custom Policy policy4tom and added permission for ec2 full access<br>{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "VisualEditor0",
			"Effect": "Allow",
			"Action": [
				"rds:*",
				"s3:*",
				"ec2:*"
			],
			"Resource": "*"
		}
	]
}
<br>and set permission boundary for Administrator access<br>
<img alt="Pasted image 20241213154148.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"><br>Created EC2 Instance with security group rule for port 8080 enabled<br>
and added this user data<br>curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip
unzip apache-tomcat-9.0.97.zip
yum install java-17 -y 
cd apache-tomcat-9.0.97/bin/
bash ./catalina.sh start 
<br>after running instance check if tomcat page shows in port 8080<br><img alt="{51C73FFF-C61A-4231-BA6B-783F6BD22F4D}.png" src="https://notes.sarangwandile.xyz/lib/media/{51c73fff-c61a-4231-ba6b-783f6bd22f4d}.png"><br>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-11-aws-task.html</link><guid isPermaLink="false">Daily Notes/2024-12-11 AWS Task.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241213154148.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-14]]></title><description><![CDATA[ 
 <br>aws exam]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-14.html</link><guid isPermaLink="false">Daily Notes/2024-12-14.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-17]]></title><description><![CDATA[ 
 <br>SDLC<br>
Agile<br>
waterfall]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-17.html</link><guid isPermaLink="false">Daily Notes/2024-12-17.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-18]]></title><description><![CDATA[ 
 <br>Git Theory Explained]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-18.html</link><guid isPermaLink="false">Daily Notes/2024-12-18.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:28 GMT</pubDate></item><item><title><![CDATA[2024-12-23 docker-day]]></title><description><![CDATA[ 
 <br><img alt="{494AEA1D-4739-4A7B-B8A5-C8BB6655CD12}.png" src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"><br>
Before Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture<br><br><br>
<br>Single unified application
<br><br>
<br>one app divided in chunks
<br><br><img style="max-width:400px; " class="excalidraw-svg excalidraw-embedded-img excalidraw-canvas-immersive" src="https://notes.sarangwandile.xyz/blob://d3148054-ed4c-4484-973a-ae5f0e34b845" filesource="Drawings/Docker-Architecture-diagram_1.excalidraw.md" w="400" draggable="false" oncanvas="false"><br><br>
<br>its like small virtual machines
<br>containers are free to use all the rams they needs
<br>whereas in vms softwares has limited prealloted ram to use
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/" target="_blank">https://www.geeksforgeeks.org/monolithic-vs-microservices-architecture/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://adventofdocker.com" target="_blank">https://adventofdocker.com</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-23-docker-day.html</link><guid isPermaLink="false">Daily Notes/2024-12-23 docker-day.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 25 Dec 2024 13:33:38 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{494aea1d-4739-4a7b-b8a5-c8bb6655cd12}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-24]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20241224092817.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"><br>
docker client ---&gt; docker commands<br>
docker machine ---&gt; docker host<br>ubuntu package name docker.i<br>in amazon-linux<br>
yum install docker -y<br>enable daemon<br>
systemctl enable --now docker<br>docker pull nginx<br>Tip
Images can also be pulled from SHA Id.
<br>docker run nginx<br>-d detached mode<br>
ps list running containers<br>
-a list all containers stopped and running<br>
rm remove stopped containers<br>
kill kill the containers<br>
-P expose container on random host port ]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-24.html</link><guid isPermaLink="false">Daily Notes/2024-12-24.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 24 Dec 2024 04:29:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-24 docker basics]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20241224092817.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"><br>
docker client ---&gt; docker commands<br>
docker machine ---&gt; docker host<br>ubuntu package name docker.i<br>in amazon-linux<br>
yum install docker -y<br>enable daemon<br>
systemctl enable --now docker<br>docker pull nginx<br>Tip
Images can also be pulled from SHA Id.
<br>docker run nginx<br>-d detached mode<br>
ps list running containers<br>
-a list all containers stopped and running<br>
rm remove stopped containers<br>
kill kill the containers<br>
-P expose container on random host port ]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-24-docker-basics.html</link><guid isPermaLink="false">Daily Notes/2024-12-24 docker basics.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 24 Dec 2024 04:29:27 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241224092817.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-12-30]]></title><description><![CDATA[ 
 <br>Task 1: <a data-href="Task 4 Create daemon service of tomcat" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html" class="internal-link" target="_self" rel="noopener nofollow">Task 4 Create daemon service of tomcat</a><br><a data-href="read about docker.sock file" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html" class="internal-link" target="_self" rel="noopener nofollow">read about docker.sock file</a><br>persistence storage<br>
docker volume<br>
docker volume ls<br>creating docker volume<br>
docker volume create dev<br>Root directory of docker<br>
cd /var/lib/docker/<br>all systemd service files lies in<br>
/etc/systemd/system/<br>configuration files<br>
/etc/docker<br>socket file<br>
/var/run/docker.sock<br>docker run -d -P -v dev:/usr/share/nginx/html nginx<br>Tommorow<br>
dockerfile<br>
network]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-30.html</link><guid isPermaLink="false">Daily Notes/2024-12-30.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate></item><item><title><![CDATA[Dockerfile Explained]]></title><description><![CDATA[ 
 <br><br><br>
<br>Step by step instructions to build Image
<br>Blueprint of a container
<br>Written in go
<br>Has Instructions
<br>Some Basic Instructions are:<br><br>There are two ways to run command in container.<br>
<br>Using CMD instruction
<br>Using Entrypoint
<br> CMD<br>
<br>Purpose: Defines the default command to execute when the container starts.
<br>Flexibility: Easily overridden by using the docker run command with a different command.
<br>Example: CMD ["python", "app.py"]
<br>ENTRYPOINT<br>
<br>Purpose: Configures the container to run as an executable. It sets the main command that always executes.
<br>Less Flexible: While arguments can be appended, the core command defined by ENTRYPOINT remains.
<br>Example: ENTRYPOINT ["/usr/bin/nginx"]
<br>Key Differences and Use Cases<br><br>Entrypoint file can be single command or shell script.<br>
Simple example of entrypoint script is this:<br>
Entrypoint.sh<br>#!/bin/bash

# Check if a configuration file exists
if [ ! -f /app/config.yml ]; then
  echo "Error: config.yml not found!"
  exit 1
fi

# Start the application
exec python /app/app.py
<br>In your Dockerfile, you would use:<br>Dockerfile<br>COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
<br>Example of Dockerfile using command for entrypoint<br>
Dockerfile<br>FROM ubuntu:latest

# Install Apache
RUN apt-get update &amp;&amp; apt-get install -y apache2

# Set the entrypoint to the Apache server
ENTRYPOINT ["apache2ctl"]

# Default command to start Apache in the foreground
CMD ["-D", "FOREGROUND"] 
<br>In this example, ENTRYPOINT ["apache2ctl"] configures the container to run the Apache web server as its main process. When you start a container from this image, it will always execute apache2ctl.<br><br><br>
<br>Follow on <a data-href="Task 5 Create dockerfiles" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-5-create-dockerfiles.html" class="internal-link" target="_self" rel="noopener nofollow">Task 5 Create dockerfiles</a>
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2024-12-31-dockerfile-explained.html</link><guid isPermaLink="false">Daily Notes/2024-12-31 Dockerfile Explained.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 31 Dec 2024 18:09:30 GMT</pubDate></item><item><title><![CDATA[2025-01-02 Multistage Dockerfile]]></title><description><![CDATA[ 
 <br><br><br>FROM amazonlinux:latest
RUN yum update &amp;&amp; yum install java-17 unzip -y
copy tomcat /tomcat/.
RUN chmod +x /opt/apache-tomcat-9.0.98/bin/catalina.sh 
EXPOSE 8080
COPY studentui studentui/.
WORKDIR /studentui
RUN mvn clean package
RUN mv target/*.war /tomcat/webapps/student.war
CMD ["/tomcat/bin/catalina.sh", "run"]
<br><br>FROM maven:3.9.9-amazoncorretto-8-debian-bookworm AS builder
COPY student-ui /.
RUN mvn clean package     # build in target/*.war

FROM tomcat:jre8:alpine
COPY --from=builder /target/*.war webapps/student.war
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-02-multistage-dockerfile.html</link><guid isPermaLink="false">Daily Notes/2025-01-02 Multistage Dockerfile.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 02 Jan 2025 04:25:33 GMT</pubDate></item><item><title><![CDATA[2025-01-06]]></title><description><![CDATA[ 
 <br><br><img alt="Pasted image 20250107191227.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20250107191227.png"><br>
cluster -- group of identical instance<br>
resources -- pods<br>node -- instance<br>EKS <br>control plane / master<br>etcd = cluster database   name of the master, cluster etc
api server = communication among cluster components, recieves and forwards information
scheduler = scheduling pods
 pod = smallest unit of kubernetes, jacket like structure to container
 kubeproxy = for networking
 kubelet = sends information of node to api server also sends heartbeat
 controller = many controllers, manages pods and containers.
<br>bare minimum criteria for<br>
0.25m 6.25core<br>GB     Gi<br>
1024   1000<br>containerd<br>kubeadm<br><a data-href="Read about each and every component of kubernetes" href="https://notes.sarangwandile.xyz/devops/for-reading/read-about-each-and-every-component-of-kubernetes.html" class="internal-link" target="_self" rel="noopener nofollow">Read about each and every component of kubernetes</a>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-06.html</link><guid isPermaLink="false">Daily Notes/2025-01-06.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 07 Jan 2025 13:43:13 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/pasted-image-20250107191227.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20250107191227.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2025-01-08]]></title><description><![CDATA[ 
 <br>üî¶Lights out]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-08.html</link><guid isPermaLink="false">Daily Notes/2025-01-08.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Wed, 08 Jan 2025 16:58:49 GMT</pubDate></item><item><title><![CDATA[2025-01-09 k8s manifest files]]></title><description><![CDATA[ 
 <br><br>The ReplicaSet configuration defines a number of identical pods required, and if a pod is evicted or fails, creates more pods to compensate for the loss.<br>
ReplicaSets are usually not used directly‚Äîthey are typically created as part of a deployment.<br>Here is a simple example using&nbsp;matchLabels&nbsp;as the selector.<br>
rs.yaml<br>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontendn
    spec:
      containers:
      - name: nginx
        image: nginx
<br>Apply Manifest kubectl apply -f rs.yaml<br>Note: use either one file naming yml or yaml<br><br>Deployments are one level up from ReplicaSets. They allow you to use a declarative method to deploy ReplicaSets and pods. You use YAML configuration to define what your group of pods should look like, and the deployment manipulates Kubernetes objects to create pods&nbsp;exactly according to the YAML specification.<br>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontendn
    spec:
      containers:
      - name: nginx
        image: nginx
<br>deployment ----&gt; ReplicaSet -------&gt; Pods<br><img alt="Manifest_structure.svg" src="https://notes.sarangwandile.xyz/lib/media/manifest_structure.svg"><br><br>In a Kubernetes cluster, each Pod has an internal IP address. But the Pods in a Deployment come and go, and their IP addresses change. So it doesn't make sense to use Pod IP addresses directly. With a Service, you get a stable IP address that lasts for the life of the Service, even as the IP addresses of the member Pods change.<br>A key aim of Services in Kubernetes is that you don't need to modify your existing application to use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you've containerized. You use a Service to make that set of Pods available on the network so that clients can interact with it.<br><br><br>Hot Tip
Its not possible to remember all shortcodes for k8's Resources<br>
Thats why we have this command to Lists all the resources in cluster
kubectl api-resources

<br><br>]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-09-k8s-manifest-files.html</link><guid isPermaLink="false">Daily Notes/2025-01-09 k8s manifest files.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Fri, 10 Jan 2025 03:43:34 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/manifest_structure.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/manifest_structure.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2025-01-10 Service Types]]></title><description><![CDATA[ 
 <br>max surge / max unavailable<br>
rolling updates<br>aws s3 ls --profile dev<br>aws configure --profile<br>Service<br>
<br>ClusterIP 

<br>default service


<br>Service.yaml<br>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app.kubernetes.io/name: proxy
spec:
  containers:
  - name: nginx
    image: nginx:stable
    ports:
      - containerPort: 80
        name: http-web-svc

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app.kubernetes.io/name: proxy
  ports:
  - name: name-of-service-port
    protocol: TCP
    port: 80
    targetPort: http-web-svc
<br>spec:<br>
type: LoadBalancer<br>annotations to change service type<br>
can service lb me classic use kr skte ho kya?<br>spec:<br>
type: NodePort<br>kubectl get -o wide<br>get more information<br>‚úÖ Tasks To read<br>
<br>minimum 5 points in services types
<br>secrets and ConfigMap
]]></description><link>https://notes.sarangwandile.xyz/daily-notes/2025-01-10-service-types.html</link><guid isPermaLink="false">Daily Notes/2025-01-10 Service Types.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Fri, 10 Jan 2025 09:12:51 GMT</pubDate></item><item><title><![CDATA[Task 1 - Creating Tomcat student-ui container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Get the docker image of Amazon linux 
<br>Add tomcat package  
<br>Add student ui 
<br>Then commit the image 
<br>Store on ecr as well as on docker hub
<br><br>Step 1: Getting the the Amazon Linux Image<br>
Upon searching the for amazonlinux image in <a data-tooltip-position="top" aria-label="https://hub.docker.com" rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">dockerhub</a> I found its official image<br>
I pulled it into the system with docker pull amazonlinux<br>sudo docker run -it -d -p 32768:8080 --name tomcat-student-ui amazonlinux`
<br>and I run the image interactively and in detached mode so i can execute shell command later on<br><img alt="{1CC503E6-0A58-42E5-93CF-64946C7AA1EF}.png" src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"><br>Lets dive into the container <br>sudo docker exec -it &lt;containerid&gt; &lt;shell-command&gt;`
<br><img alt="{0119C4A3-2DE3-4CB0-9692-E63934BF4F5C}.png" src="https://notes.sarangwandile.xyz/lib/media/{0119c4a3-2de3-4cb0-9692-e63934bf4f5c}.png"><br><br>Our App student-ui required specific version of tomcat thats why we gonna install it from source<br>Make sure to install dependancies and unzip package before hand<br><br>Install this specific version from source<br>cd /opt # we can use this directory for temporary space
curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip
unzip apache-tomcat-9.0.98.zip # unzip the archive
yum install java-17 -y # tomcat 9 requires java-17 to work properly
cd apache-tomcat-9.0.98/bin/
bash ./catalina.sh start 
<br><img alt="{9BD47135-6F08-466A-A3C7-A11FC17A759D}.png" src="https://notes.sarangwandile.xyz/lib/media/{9bd47135-6f08-466a-a3c7-a11fc17a759d}.png"><br><img alt="{40C024BF-0490-4CFC-9507-9363489D25BD}.png" src="https://notes.sarangwandile.xyz/lib/media/{40c024bf-0490-4cfc-9507-9363489d25bd}.png"><br>It looks like our tomcat server is up and running on desired port<br><br>but first we need these package building tools git and maven for building our student-ui app<br><br>sudo yum install git maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br><br>cd student-ui
mvn clean package
<br><img alt="{8E723B0C-8E54-4F56-94BC-F7D231F3287A}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e723b0c-8e54-4f56-94bc-f7d231f3287a}.png"><br>
this creates .war file in ./target folder<br>
rename it to suitable short name and move to /opt/apache-tomcat-9.0.98/webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war /opt/apache-tomcat-9.0.97/webapps/
<br>and the app should be accessible on tomcat server on http://instance-ip:32768/studentapp<br><img alt="{0DFF6CA3-CC34-4310-933F-F66D3256551D}.png" src="https://notes.sarangwandile.xyz/lib/media/{0dff6ca3-cc34-4310-933f-f66d3256551d}.png"><br><br>Before creating the image its better to remove the unnecessary packages that we no longer need to make the size of the image minimal as possible.<br><br>yum remove maven git unzip -y
<br>Lets exit from container shell and build the image <br>sudo docker commit &lt;container-id&gt;
<br><img alt="{A2D16CE6-81BA-4DAF-A151-58EA34744A1B}.png" src="https://notes.sarangwandile.xyz/lib/media/{a2d16ce6-81ba-4daf-a151-58ea34744a1b}.png"><br>You see the created image doesnt have any name so lets give it a tag<br>sudo docker tag &lt;image-id&gt; &lt;newtag&gt;
<br><img alt="{A6DFB555-CE8C-404D-AE0E-0E5BE4AECD43}.png" src="https://notes.sarangwandile.xyz/lib/media/{a6dfb555-ce8c-404d-ae0e-0e5be4aecd43}.png"><br>
now push it to docker hub and ECR<br><br><br>
<br>Login to docker hub
<br>Click on repositories
<br>Create new repository
<br>Give it proper name and click create
<br>Here is my repo looks llike<br><img alt="{8E536B96-794C-49C6-B49F-257C6FEE2E9E}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e536b96-794c-49c6-b49f-257c6fee2e9e}.png"><br>Lets push our image into this repo<br>
First rename add new tag to image appropriate according to docker hub repo name<br>sudo docker tag &lt;old-tag-name&gt; &lt;newtag-name&gt;
<br><img alt="{62B7DAF9-7C95-4E53-BEB1-28F215445939}.png" src="https://notes.sarangwandile.xyz/lib/media/{62b7daf9-7c95-4e53-beb1-28f215445939}.png"><br>and now push it to docker hub<br>sudo docker push archsarangx/tomcat-student-ui:latest
<br><img alt="{3386541C-1516-4A09-9493-93A52B3FF2E0}.png" src="https://notes.sarangwandile.xyz/lib/media/{3386541c-1516-4a09-9493-93a52b3ff2e0}.png"><br>and its successfully uploaded on docker hub at<br>
<img alt="{034DA07E-A936-4284-9683-BF0300BC5EB4}.png" src="https://notes.sarangwandile.xyz/lib/media/{034da07e-a936-4284-9683-bf0300bc5eb4}.png"><br>
and anyone can pull it with<br>docker pull archsarangx/tomcat-student-ui:latest
<br><br><br>
<br>goto amazon ECR service and create repositoy
<br><img alt="Pasted image 20241228180114.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228180114.png"><br>then click on blue repo name and click on view push commands<br><br>aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 970547378605.dkr.ecr.us-west-2.amazonaws.com
<br><br>``<br>docker tag archsarangx/tomcat-student-ui:latest 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br>docker push 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br><img alt="{8230C2A4-4835-4E3A-9500-E4C8B9187FB3}.png" src="https://notes.sarangwandile.xyz/lib/media/{8230c2a4-4835-4e3a-9500-e4c8b9187fb3}.png"><br><img alt="{E9698F18-9E9B-4973-9E1A-3E629727DA11}.png" src="https://notes.sarangwandile.xyz/lib/media/{e9698f18-9e9b-4973-9e1a-3e629727da11}.png"><br>
üéâ And our Image is successfully uploaded on both ECR and docker hub.<br><br>Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/dockertasks/task-1-creating-tomcat-student-ui-container.html</link><guid isPermaLink="false">DevOps/DockerTasks/Task 1 Creating Tomcat student-ui container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 13:47:56 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task 2: Create the Free-css template container]]></title><description><![CDATA[ 
 <br><br><br>
<br>get the docker image of Amazon linux 
<br>install nginx 
<br>add free-css template
<br><br><br>
<br>
Goto docker hub --&gt; <a rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">https://hub.docker.com</a>

<br>
Click on Search and search for ---&gt; amazonlinux

<br>
Here I found this official image<br>
<img alt="{BACB7514-FD34-4BC8-9CB6-0314BF45113F}.png" src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png">

<br>
I simply pulled the image into my ec2 instance<br>
docker pull amazonlinux
here is successful pull looks like
<img alt="{BD7598CF-AF45-41FB-82E8-A7E26163D176}.png" src="https://notes.sarangwandile.xyz/lib/media/{bd7598cf-af45-41fb-82e8-a7e26163d176}.png">

<br>
We can further confirm if the image is really present in the system with docker images
<img alt="Pasted image 20241227205640.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227205640.png"><br>
Looks like amazonlinux is successfully pulled in our system<br>
lets start the container using this image

<br>
Running the container from image<br>
sudo docker run -d -p 32768:80 --name free-css-template amazonlinux
We used -d to run the container in detached mode meaning its output wont occupy the terminal screen and quietly run in the background after running this command it will simply give us container-id and gives us prompt.<br>
--name flag will assign the name for our container instead of their random funny names and with -p we are forwarding the network traffic of port 80 of the container to 32768 port of our host machine in this case ec2-instance.<br>
<img alt="{5005E6A5-9A5C-44EE-B8B4-37364CD7D1E2}.png" src="https://notes.sarangwandile.xyz/lib/media/{5005e6a5-9a5c-44ee-b8b4-37364cd7d1e2}.png">
We can check for the further detail of the container with<br>
docker ps<br>
Upon inspection we can see there is no container running<br>
lets check again with docker ps -a and for our surprise container is exited just after it run. Thats because its an Operating system container which doesnt really do anything itself unless we assign it a task or a process that will run in background constantly so we can enter into the container to perform our tasks.
We can achieve this by running the container interactively
but first clean up the exited container with docker rm &lt;container-id&gt;

<br><br>
<br>Run a container interactively.
<br>docker run -it -d -p 32768:80 --name free-css-template amazonlinux
<br>   I run the container with -it and -d so it opened the interactive stdin shell session for me to interact with it and the container itself is in detached mode allowing us to enter into it anytime with exec command<br>docker exec -it &lt;container-id&gt; &lt;shell-command&gt;
<br><img alt="{8DD627B8-39AA-4F0D-8320-CF6DA2D7BB33}.png" src="https://notes.sarangwandile.xyz/lib/media/{8dd627b8-39aa-4f0d-8320-cf6da2d7bb33}.png"><br>we got bash shell session inside container<br>
Now we need to update the packages inside the container<br><br>yum update
<br><img alt="{FEF2B4B8-3A70-49CA-B07C-475B398724EC}.png" src="https://notes.sarangwandile.xyz/lib/media/{fef2b4b8-3a70-49ca-b07c-475b398724ec}.png"><br>
<br>Install nginx package start it
<br>yum install nginx -y
<br> since this amazonlinux container image doesn't come with systemd preinstalled we cannot start nginx daemon so we need to run this command manually in background<br>nginx &amp;
<br>and our nginx server is successfully started at port 32768 on our host ec2-instance<br><img alt="{E0754AEE-5490-497B-AA74-428CD4B239A9}.png" src="https://notes.sarangwandile.xyz/lib/media/{e0754aee-5490-497b-aa74-428cd4b239a9}.png"><br><br>Lets add free css template in it<br>
Head on to <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/" target="_blank">https://www.free-css.com/</a> and download any free css template you want by right clicking on the download button and copying link address<br>
<img alt="{B54DF9F8-AC7B-420B-9EAA-54AE5D0B261F}.png" src="https://notes.sarangwandile.xyz/lib/media/{b54df9f8-ac7b-420b-9eaa-54ae5d0b261f}.png"><br>
and go to your containers terminal session and download this file in nginx server directory with curl or wget<br>cd /usr/share/nginx/html
curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/carvilla.zip

# Extract the zip file
unzip carvilla.zip

# If unzip is not found install unzip package
sudo yum install unzip
<br>extracting the zip has extracted all contents into folder named carvilla-v1.0<br>
I'll rename it for my convenient. <br>mv carvilla-v1.0 mywebsite
<br>and the website is running at port 32768/mywebsite<br>now we can get out of this container with exit command.<br><img alt="{ADBAE0E2-C52F-4FE1-9DBE-85A5BABFF759}.png" src="https://notes.sarangwandile.xyz/lib/media/{adbae0e2-c52f-4fe1-9dbe-85a5babff759}.png"><br>We have our container successfully hosting our free-css template on port http://instance-ip:32768/mywebsite<br>
Lets create the image out of this container so we can spin many containers as we want and save the hustle to do all these steps all over again.<br><br>
<br>Creating the image<br>
<img alt="{FE045571-00F2-44B6-B1CB-E7F26EE9AE7E}.png" src="https://notes.sarangwandile.xyz/lib/media/{fe045571-00f2-44b6-b1cb-e7f26ee9ae7e}.png"><br>
Now that we have our container running and our site is visible I want to make image out of it and push it to docker hub 
<br>for that we use docker commit &lt;container-id&gt;<br>it returns with the sha-id<br>
<img alt="{83AC20A6-5137-45BC-8761-6AD28799FF56}.png" src="https://notes.sarangwandile.xyz/lib/media/{83ac20a6-5137-45bc-8761-6ad28799ff56}.png"><br>with docker images we can confirm the image creation<br>
<img alt="{9EE50687-0A6E-4930-966B-D6D50B9E78B7}.png" src="https://notes.sarangwandile.xyz/lib/media/{9ee50687-0a6e-4930-966b-d6d50b9e78b7}.png"><br>our image shown in first row doesnt have name so lets give it a tag and then push it to repo<br>
docker tag &lt;image-id&gt; &lt;tagname&gt;<br>
docker tag local-image:tagname new-repo:tagname<br><img alt="Pasted image 20241227225859.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227225859.png"><br>Here I have already created repository in docker hub so I gave its name here<br>and run docker hub login command before running push command<br>
docker login<br>and push it to our docker hub<br>
docker push new-repo:tagname<br><img alt="Pasted image 20241227231042.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227231042.png"><br>And our project is successful pushed over docker hub and can be access from here<br><a rel="noopener nofollow" class="external-link" href="https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags" target="_blank">https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags</a><br><br><img alt="{393988AE-E9F0-47A9-A960-5EBE39901FED}.png" src="https://notes.sarangwandile.xyz/lib/media/{393988ae-e9f0-47a9-a960-5ebe39901fed}.png"><br>To store the image into Amazon's Elastic Container Registry which is similar to docker hub but their own we need to create a repository here too.<br>I gave the repo same name<br>
<img alt="{F7CDF0B9-3C17-4D22-9547-7D65863E4BDE}.png" src="https://notes.sarangwandile.xyz/lib/media/{f7cdf0b9-3c17-4d22-9547-7d65863e4bde}.png"><br>and Upon clicking create button my repo is created<br>
<img alt="{7B336AAA-A8CB-4F61-823E-072A4025B6A6}.png" src="https://notes.sarangwandile.xyz/lib/media/{7b336aaa-a8cb-4f61-823e-072a4025b6a6}.png"><br>
Click on the Repo name in blue and click on view push commands to get instruction for pushing our image into this repo<br><img alt="{7CB87CED-E1FD-4698-BCA5-DFC2D6C53B23}.png" src="https://notes.sarangwandile.xyz/lib/media/{7cb87ced-e1fd-4698-bca5-dfc2d6c53b23}.png"><br><br>To push our image we need to first Authenticate with registry for that carefully use first command also we need to have aws-cli installed and configured for that luckily for me I am using amazonlinux ec2 instance so its already installed i can just run aws configure to configure aws-cli<br>
For those havent can check <a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" target="_blank">this documentation</a> to configure their aws-cli within instance.<br><img alt="Pasted image 20241228154309.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228154309.png"><br>and then run this command provided in the instructions but remember to add sudo as we are using standard user<br>
<img alt="{AC6817B4-D44D-4A43-AEFF-16292F4D44D0}.png" src="https://notes.sarangwandile.xyz/lib/media/{ac6817b4-d44d-4a43-aeff-16292f4d44d0}.png"><br>Now that We have successfully login to ECR lets tag the image accordingly and push it to ECR<br>sudo docker tag &lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt; &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;newname&gt;:&lt;newtag&gt;<br><img alt="{19760D25-4AB9-4A59-9757-62575607EF5A}.png" src="https://notes.sarangwandile.xyz/lib/media/{19760d25-4ab9-4a59-9757-62575607ef5a}.png"><br>
and push to ECR<br>sudo docker push &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt;<br><img alt="{D18E449C-3E7D-4B49-8AC1-75893D2A92F7}.png" src="https://notes.sarangwandile.xyz/lib/media/{d18e449c-3e7d-4b49-8ac1-75893d2a92f7}.png"><br>and refresh the amazon EKS web site to see the latest image listed in there<br>
<img alt="{48017E2C-FBB3-4AB6-B125-BFEFA58746BE}.png" src="https://notes.sarangwandile.xyz/lib/media/{48017e2c-fbb3-4ab6-b125-bfefa58746be}.png"><br>Thank you for Reading<br>
Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/dockertasks/task-2-create-the-free-css-template-container.html</link><guid isPermaLink="false">DevOps/DockerTasks/Task 2 Create the Free-css template container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 13:47:16 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Create the MYSQL Container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Host mysql container 
<br>Use -e ## environment variables
<br>use MYSQL_USER, MYSQL_PASSWORD
<br><br><br><br>sudo docker pull mysql
<br><br>sudo docker run -e MYSQL_USER=sarang -e MYSQL_PASSWORD=mysupersecretpassword -e MYSQL_RANDOM_ROOT_PASSWORD=yes -p 3306:3306 --name mysql_container -v mysql_data:/var/lib/mysql mysql:latest
<br>Here we used -e flag to specify each environment variables for mysql<br>MYSQL_USER=sarang  This sets the username for our mysql database<br>
MYSQL_PASSWORD=mysupersecretpassword This sets the password for our database<br>MYSQL_RANDOM_ROOT_PASSWORD=yes    # This tells the mysql to generate a random root password (it can be seen in docker logs)<br>
-v mysql_data:/var/lib/mysql mysql:latest This flag sets the persistence volume for our container and it mounts /var/lib/mysql directory of the container to the mysql_data directory of our host.]]></description><link>https://notes.sarangwandile.xyz/devops/dockertasks/task-3-create-mysql-container.html</link><guid isPermaLink="false">DevOps/DockerTasks/Task 3 Create Mysql Container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 13:49:31 GMT</pubDate></item><item><title><![CDATA[Agile Development]]></title><description><![CDATA[ 
 <br><br>Agile Manifesto
The agile manifesto outlines 4 values and 12 principles for teams, but‚Äîdecades later‚Äîis it still relevant? Find out
<br>Scrum
In scrum, a product is built in a series of fixed-length iterations called sprints, giving agile teams a framework for shipping software on a regular cadence. Learn how the scrum methodology impacts traditional project management.
<br>Kanban
Kanban is a popular agile framework that requires real-time communication of team's capacity and full transparency of work. Learn how the kanban methodology for agile software development can benefit for your team.
<br><br>The Agile methodology is a project management approach that involves breaking the project into phases and emphasizes continuous collaboration and improvement. Teams follow a cycle of planning, executing, and evaluating.<br>Whereas the traditional "waterfall" approach has one discipline contribute to the project, then "throw it over the wall" to the next contributor, agile calls for collaborative cross-functional teams. Open communication, collaboration, adaptation, and trust amongst team members are at the heart of agile. Although the project lead or product owner typically prioritizes the work to be delivered, the team takes the lead on deciding how the work will get done, self-organizing around granular tasks and assignments.<br>Agile isn't defined by a set of ceremonies or specific development techniques. Rather, agile is a group of methodologies that demonstrate a commitment to tight feedback cycles and continuous improvement.<br><img alt="Agile development example | Atlassian Agile Coach" src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The original&nbsp;<a data-tooltip-position="top" aria-label="http://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="http://agilemanifesto.org/" target="_blank">Agile Manifesto</a>&nbsp;didn't prescribe two-week iterations or an ideal team size. It simply laid out a set of core values that put people first. The way you and your team live those values today ‚Äì whether you do scrum by the book, or blend elements of kanban and XP ‚Äì is entirely up to you.<br><br>Teams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year's worth of plans. "Just enough" planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.<br>But it's not just a numbers game‚Äîfirst and foremost, it's about people. As described by the Agile Manifesto, authentic human interactions are more important than rigid processes. Collaborating with customers and teammates is more important than predefined arrangements. And delivering a working solution to the customer's problem is more important than hyper-detailed documentation.<br>An agile team unites under a shared vision, then brings it to life the way they know is best. Each team sets their own standards for quality, usability, and completeness. Their "definition of done" then informs how fast they'll churn the work out. Although it can be scary at first, company leaders find that when they put their trust in an agile team, that team feels a greater sense of ownership and rises to meet (or exceed) management's expectations.<br><br>The publication of the Agile Manifesto in 2001 marks the birth of agile as a methodology. Since then, many agile frameworks have emerged such as scrum,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-at-scale/lean-portfolio-management" target="_blank">lean</a>, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way.&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/scrum" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/scrum" target="_blank">Scrum</a>&nbsp;and XP are favored by software development teams, while&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/kanban" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/kanban" target="_blank">kanban</a>&nbsp;is a darling among service-oriented teams like IT or human resources.<br>Today, many agile teams combine practices from a few different frameworks, spiced up with practices unique to the team. Some teams adopt some agile rituals&nbsp;(like regular stand-ups, retros, backlogs, etc.), while others created a&nbsp;new agile practice (<a data-tooltip-position="top" aria-label="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/agile/agile-marketing/what-is-agile-marketing" target="_blank">agile marketing teams</a>&nbsp;who adhere to the&nbsp;Agile Marketing Manifesto).<br><img alt="Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:8631b273-6575-4586-9a9a-f5eb84d4b46f/illustrations-spot-Code%20Release%202.svg?cdnVersion=2472" referrerpolicy="no-referrer"><br>The agile teams of tomorrow will value their own effectiveness over adherence to doctrine. Openness, trust, and autonomy are emerging as the cultural currency for companies who want to attract the best people and get the most out of them. Such companies are already proving that practices can vary across teams, as long as they're guided by the right principles.<br><br>The way each team practices agile should be unique to their needs and culture. Indeed, no two teams inside Atlassian have identical agile practices.<br>Although many of our teams organize their work in sprints, estimate in story points, and prioritize their backlogs, we're not die-hard practitioners of scrum. Or kanban. Or any other trademarked methodology. Instead, we give each team the autonomy to cherry-pick the practices that will make them most effective. And we encourage you to take a similar approach.<br>For example, if you're on a queue-oriented team like IT, kanban provides a solid foundation for your agile practice. But nothing should stop you from sprinkling in a few scrum practices like demo sessions with stakeholders or regular retrospectives.<br>The key to doing agile right is&nbsp;<a data-tooltip-position="top" aria-label="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" rel="noopener nofollow" class="external-link" href="https://www.atlassian.com/blog/agile/how-to-stay-agile-and-keep-improving" target="_blank">embracing a mindset of continuous improvement</a>. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don't.<br><img alt="Atlassian on agile | Atlassian agile coach" src="https://wac-cdn.atlassian.com/dam/jcr:358e6b8e-5eec-428f-8d2f-c9aedb962263/illustrations-spot-hero-Status%20Page%20Light-1115x898@2x.png?cdnVersion=2472" referrerpolicy="no-referrer">]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/agile-development.html</link><guid isPermaLink="false">DevOps/For Reading/Agile Development.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:27 GMT</pubDate><enclosure url="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://wac-cdn.atlassian.com/dam/jcr:5e0e5b6f-9329-4c20-a711-6eef96956d88/nursery-teams%20(1).svg?cdnVersion=2472"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Agile Vs DevOps]]></title><description><![CDATA[ 
 <br>Agile and DevOps have shaped the way software is developed today. They‚Äôve become so widely adapted and revered as to permeate beyond the world of software development into shaping project management and org charts in businesses of all stripes.<br>DevOps and agile can be tricky to define, and the lines between the two often seem to blur.<br>
<br>At a basic level, DevOps is the combination of two teams (software development and IT operations) to create a more powerful, efficient software development process.
<br>Agile is a series of methodologies around iterative development designed to make tasks smaller and more manageable and increase collaboration.
<br>But what are the deeper differences between DevOps and agile? Is DevOps&nbsp;better&nbsp;than agile? Or can DevOps and agile be implemented together? In this post, we‚Äôll dive into some common agile and DevOps FAQs.<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#1" target="_blank">What is the difference between DevOps and agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#2" target="_blank">What is agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#3" target="_blank">What are the benefits of agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#4" target="_blank">How can a company be agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#5" target="_blank">What is DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#6" target="_blank">Why combine software development and IT operations?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#7" target="_blank">What are some DevOps concepts and key terms?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#8" target="_blank">What are the benefits of DevOps?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#9" target="_blank">Is DevOps better than agile? Is DevOps a replacement for agile?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">What is CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#11" target="_blank">What are the benefits of CI/CD?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#12" target="_blank">What are some common DevOps tools?</a>
<br><a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#13" target="_blank">How can I learn more about DevOps?</a>
<br><br>Agile and DevOps are both used in the development of software. While they‚Äôre both designed to improve the software development process, they seek to do so by taking different approaches. But they‚Äôre not mutually exclusive. (More on that below.)<br>Agile is essentially about giving software developers a shared set of principles to drive decision-making and allow for more responsiveness to change.<br>DevOps is about a culture change intended to drive collaboration between software developers and IT operations.<br>It's often said that DevOps is agile applied beyond the software development team.<br><br><img alt="Post-COVID DevOps" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank"><strong></strong>
</a>Post-COVID DevOps: Accelerating the Future<br>How has COVID affected ‚Äî or even accelerated ‚Äî DevOps best practices for engineering teams?&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/post-covid-devops-accelerating-future-webinarhttps://get.acloudguru.com/post-covid-devops-accelerating-future-webinar" target="_blank">Watch this free, on-demand webinar</a>&nbsp;panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.<br><br><br>Agile is an iterative software development approach that&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/riding-the-next-cloud-why-edge-computing-is-the-next-wave-to-catch" target="_blank">focuses on collaboration and quick, rapid releases</a>. It's a set of values and principles that can be used to help drive decisions in software development.&nbsp;<br>When it comes to agile, it‚Äôs tough to define it more concisely than the original micro&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/" target="_blank">manifesto</a>&nbsp;itself, which was written back in 2001. (No TL;DR version needed. It's only 68 words.) It states:<br>We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:<br>
<br>Individuals and interactions&nbsp;over processes and tools
<br>Working software&nbsp;over comprehensive documentation
<br>Customer collaboration&nbsp;over contract negotiation
<br>Responding to change&nbsp;over following a plan
<br>That is, while there is value in the items on the right, we value the items on the left more<br>The manifest is paired with&nbsp;<a data-tooltip-position="top" aria-label="https://agilemanifesto.org/principles.html" rel="noopener nofollow" class="external-link" href="https://agilemanifesto.org/principles.html" target="_blank">12 agile principles</a>&nbsp;to help make better decisions.&nbsp;<br>Copying agile approaches and investing in agile tools doesn‚Äôt make your team agile. It‚Äôs not just about doing two-week iterations or building out smaller teams. In agile, the&nbsp;what&nbsp;a team does is less important than the&nbsp;why&nbsp;they do it.<br>Agile (as the name implies) is about the flexibility and adaptability to build software with ever-changing needs and toss set-in-stone plans out the window.<br>This includes better connecting the dev team with end-users. (You could sort of think of that a bit like the&nbsp;<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=m4OvQIGDg4I" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=m4OvQIGDg4I" target="_blank">‚ÄúWhat would you say you do here?‚Äù scene</a>&nbsp;from&nbsp;Office Space.)<br><br>
<br>Agility&nbsp;-&nbsp; More quickly respond to market changes or customer feedback.
<br>Quality&nbsp;- A focus on testing and sustained customer involvement means the chances of a product‚Äôs overall quality being high are greater.
<br>Collaboration&nbsp;- Agile is about people. By placing value on human interactions over processes and ‚Äúthat‚Äôs just the way it‚Äôs done,‚Äù organizations are able to let employees act guided by their experience and a shared set of values rather than being micro-managed or shackled to detailed documentation.
<br>Ownership&nbsp;- The trust required from leadership to have agile teams can create an increased feeling of ownership.
<br>Customer satisfaction&nbsp;- With a focus on finding and fixing problems quickly and a direct line between customers and developers, customers are more likely to be satisfied and come back for more.
<br>Predictability&nbsp;- By doing away with big plans that are set in stone and often detached from reality, organizations can get a better picture of what‚Äôs going well and what‚Äôs not working right away rather than months down the road when it‚Äôs too late to do anything to correct it.
<br>Increase productivity&nbsp;- Regularly used planning tools like Kanban and Scrum help teams keep tabs of work and progress toward goals.
<br><br>Many organizations are used to working in a waterfall model. A waterfall model is a linear approach is a sequence of events that starts with a heavy upfront investment of time and resources in scoping out requirements and budgeting before moving into development, testing, and production.<br>Moving this paradigm to agile and&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/overview/agile-at-scale" rel="noopener nofollow" class="external-link" href="https://acloud.guru/overview/agile-at-scale" target="_blank">running agile at scale</a>&nbsp;can be difficult and complex. This isn‚Äôt helped by the fact that ‚Äúagile‚Äù is less of a thing you do and more of a basis for decision-making. For organizations guided by process above all else, this can require a major shift and definitely takes some backing from management.<br><br>With a name that sounds like a covert military team and the kind of goofy capitalization you‚Äôve come to expect from tech terminology, DevOps combines software development and IT operations. Take the "dev" from software development and the "ops" from IT operations and you get this portmanteau, which is the fun-to-say term that describes mashing together of multiple words, like spork, phablet, brunch, jorts, and bromance.<br>DevOps isn't just a process. It‚Äôs a shift in workplace culture. It‚Äôs a collaboration between teams. Doubling down on automation and installing all of the right software won‚Äôt get you there alone. Like agile, people are the key component.<br>Interested in upscaling or beginning your journey with DevOps? A Cloud Guru‚Äôs&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/learning-paths/devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/learning-paths/devops" target="_blank">DevOps Learning Paths</a>&nbsp;offers custom courses fit for beginners and advanced gurus!<br>AWS offers the following definition of the DevOps model:<br>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization‚Äôs ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.<br><br><img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/06/cost-optimization-blog-header.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank"><strong></strong></a>Automating AWS Cost Optimization<br>
AWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this&nbsp;<a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">free, on-demand webinar</a>, you'll get an overview of AWS cost-optimization tools and strategies.<br><a data-tooltip-position="top" aria-label="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" rel="noopener nofollow" class="external-link" href="https://go.acloudguru.com/AWS-Cost-Optimization-Webinar" target="_blank">Watch Now</a><br><br><br>How did these two groups get combined into one? And why would you want to combine development and IT operations?&nbsp;<br>Way back when (around 2007 or so), IT operations teams were separate from development teams. Then, people in the development and operations communities realized there was some issues in the way these teams were structured and how they interacted (or didn‚Äôt interact).<br>
<br>Development and IT operations teams had different objectives
<br>Development and operations teams had different performance indicators
<br>Development and operations teams were siloed physically in different rooms, buildings, or offices
<br>So they started talking. This gave birth to DevOps and the&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-professional-cloud-devops-engineer-certification-path-introduction-gcp-devops-engineer-track-part-1" target="_blank">DevOps engineer</a>.&nbsp;<br>Patrick Debois, often called the Godfather of DevOps, brought these groups together at&nbsp;<a data-tooltip-position="top" aria-label="https://devopsdays.org/" rel="noopener nofollow" class="external-link" href="https://devopsdays.org/" target="_blank">devopsdays</a>&nbsp;in 2009 where they discussed ways to bridge the gap between the two fields. Now, thousands of enterprises have adapted or are working toward adapting these practices.<br>These new approaches basically make ops everyone‚Äôs job to a degree, which makes&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-future-of-ops-jobs" target="_blank">the future of ops jobs</a>&nbsp;very interesting.&nbsp;<br><br>
<br>Container&nbsp;- A lightweight, standalone, executable piece of software. It includes everything needed to run that software.
<br>Continuous delivery (CD)&nbsp;- The ongoing and manual or automatic release of software to production. It's aimed around new cycles. Some orgs release a new version with any changes.
<br>Continuous integration (CI)&nbsp;- The ongoing programming, building, and testing of code. This is done before turning it over to the continuous delivery (CD) system for deployment. With CD, it makes up&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/devops-vs-agile-whats-the-difference#CICD" target="_blank">CI/CD</a>.
<br>Infrastructure as Code (IaC)&nbsp;- Defining infrastructure you want to use with programming code that can be understood by cloud services. These cloud services then create the infrastructure for you based on this code. This allows you to define standards and reuse code, which saves time.
<br>Microservices&nbsp;- Application architecture that is broken into multiple small pieces. Containers are often used to deploy microservices.
<br>Open source&nbsp;- Computer software code released under a license for free, like&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/linux-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/linux-this-month" target="_blank">Linux</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloud.guru/series/kubernetes-this-month" rel="noopener nofollow" class="external-link" href="https://acloud.guru/series/kubernetes-this-month" target="_blank">Kubernetes</a>.
<br>Pipeline&nbsp;- A set of connected processes where the output of one process is the input for the next.
<br>Serverless&nbsp;- Running a service or microservice on cloud-based infrastructure without worrying about the servers running your code. You simply provide the code, and the cloud provider runs the code and gives you the results. See more on the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions" target="_blank">Function as a Service&nbsp;(FaaS) services of AWS, Azure, and GCP</a>.
<br>Source code repository&nbsp;- A place to upload and track the history of your code, like&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">GitHub</a>&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/aws-developer-tools-overview-and-codecommit-cheat-sheet" target="_blank">AWS CodeCommit</a>.
<br>Unit testing&nbsp;- Breaking your application down into small parts to test that each features works.
<br><br>DevOps is all about producing higher-quality software faster and saving a lot of time and money. Here‚Äôs a more detailed breakdown of the benefits.<br>
<br>Speed&nbsp;- Release updates and new features faster, adapt to the changing market, and become more efficient.
<br>Rapid delivery&nbsp;- Increase deployment frequency and the pace of releases. Respond to customers' needs faster and build a competitive advantage.&nbsp;
<br>Reliability&nbsp;- Automatic testing is built-in. Ensures rollouts are of the highest quality and that you have less downtime because you build for stability and test before deploying.
<br>Scale&nbsp;- Implements automation. With the use of cloud and container technology, you can scale usage up and down and save money while you do so.
<br>Collaboration&nbsp;- Allow teams who used to be apart to work together. Workflows can be combined, inefficiency is reduced, and time is saved.
<br>Security&nbsp;- Allows infrastructure to be created by code, just like software. By doing this (instead of doing it manually), you can define policies to stay compliant no matter how many servers will be deployed.
<br>Want an overview of the benefits with a heavy dash of sarcasm? Check out our post&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/5-reasons-to-not-move-to-devops" target="_blank">5 reasons to NOT move to DevOps</a>.<br><br>DevOps and agile can speed up the delivery of and increase the quality of software.&nbsp;Agile replaced the old-school waterfall model, but DevOps isn‚Äôt a replacement for agile.<br>DevOps exists because of agile, and the two can coexist and be used together. You don‚Äôt have to choose between DevOps and agile. Ideally, your organization will practice both.&nbsp;<br>See the&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/the-top-devops-skills-people-are-learning-at-a-cloud-guru-right-now" target="_blank">top DevOps skills people are learning at ACG right now</a><br><br>Continuous integration and continuous development (or CI/CD) is a DevOps tactic ‚Äî a way to deliver apps to customers with the introduction of automation into the agile development process.&nbsp;<br>The&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/implementing-a-full-ci-cd-pipeline" target="_blank">CI/CD pipeline</a>&nbsp;(as it's called when these practices are combined) has become an integral part of DevOps culture. A variety of tools and techniques are used for implementing such a pipeline. (More on those tools below.)<br>The CI/CD pipeline is supported by teams working in an agile way with either DevOps or a&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank">site reliability engineering (SRE)</a>&nbsp;approach.&nbsp;<br>Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/blog/engineering/you-need-sre-skills-to-thrive-in-a-serverless-world-kelsey-hightower" target="_blank"><em></em></a>why you need SRE skills to thrive in a serverless world&nbsp;and check out our&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/reliability-engineering-concepts" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/reliability-engineering-concepts" target="_blank"><em></em></a>Site Reliability Engineering Concepts&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/google-cloud-devops-and-sres-gcp-devops-engineer-track-part-2" target="_blank"><em></em></a>Google Cloud DevOps and SREs&nbsp;courses.<br><br>CI/CD pipelines smooth and speed up the flow of code from development through operations and QA into production by automating manual processes and increasing confidence in your releases.&nbsp;<br>This is the exact opposite of the waterfall release approach still practiced by many large organizations, where developers throw code ‚Äúover the wall‚Äù to ops, devs don‚Äôt get access to production, and ops doesn‚Äôt have much inside knowledge of how the code works.<br>CI/CD allows organizations to:<br>
<br>Build faster
<br>Decrease code review time
<br>Automatic
<br>Faster fault isolation
<br>Additional deployment features
<br>As mentioned before, you can be doing the right things and using the right tools and&nbsp;still&nbsp;not be agile or properly implementing DevOps. A broken and messy team can result in broken and messy CI/CD pipelines. You can almost&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/cd-pipeline" target="_blank">predict how your CI/CD pipeline looks</a>&nbsp;based on how your dev and ops teams work together.<br><br><img alt="Cloud Dictionary" src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2020/12/Cloud-Dictionary-Resource-Image.jpg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank"><strong></strong></a>Get the Cloud Dictionary of Pain<br>
Speaking cloud doesn‚Äôt have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this&nbsp;<a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">cloud guide</a>&nbsp;for succinct definitions of some of the most painful cloud terms.<br><a data-tooltip-position="top" aria-label="https://get.acloudguru.com/cloud-dictionary-of-pain" rel="noopener nofollow" class="external-link" href="https://get.acloudguru.com/cloud-dictionary-of-pain" target="_blank">Get the Goods</a><br><br><br>Here are some of the most popular DevOps CI/CD tools you might use if working in an AWS cloud environment.<br>
<br>
Git&nbsp;- Free open-source version control system. It stores the entire history of your code that your developers will continue to push new code to It has a tiny footprint and fast performance. It also supports branching, allowing others to work on features separately without having developers trip over code.

<br>
GitHub&nbsp;-&nbsp;Online service to host Git repositories. GitHub is owned by Microsoft, which offers a similar DevOps tool called Azure DevOps. Whether you are a developer or not, working alone or in a team,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-azure-devops" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-azure-devops" target="_blank">Azure DevOps training</a>&nbsp;can help you organize the way you plan, create and deliver software. Get the lowdown on&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/azure-devops-vs-github-comparing-microsofts-devops-twins" target="_blank">Azure DevOps vs GitHub in this comparison of Microsoft DevOps tools</a>.

<br>
AWS CodeCommit&nbsp;-&nbsp;Fully managed server for hosting Git repositories. It‚Äôs secure and encrypted, highly available, and easily integrated with other AWS services.

<br>
AWS CodeBuild&nbsp;-&nbsp;Fully managed continuous integration code that complies code, runs tests, and produces software packages. When developers write code and push it into Git, CodeBuild compiles that code and produces a software package. It scales automatically and can process multiple builds concurrently.

<br>
AWS CodeDeploy&nbsp;-&nbsp;Fully managed deployment service that takes the software package (or files you updated and pushed into your Git repository) and deploys it to AWS or on-premises servers. It integrates well with AWS services and your own servers. It completely automates software deployment, eliminating error-prone manual operations.

<br>
AWS CodePipeline&nbsp;-&nbsp;Fully management continuous delivery service that helps you completely automate your release pipeline. It can automate the build, test, and deploy phases of software development and it can integrate with CodeCommit and GitHub, giving you the flexibility to use the source-control system of your choice. Read more about&nbsp;<a data-tooltip-position="top" aria-label="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" rel="noopener nofollow" class="external-link" href="https://www.pluralsight.com/resources/blog/cloud/automating-ci-cd-with-aws-codepipeline" target="_blank">automating CI/CD with AWS CodePipeline</a>.

<br>Other common DevOps tools include&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/docker-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/docker-deep-dive" target="_blank">Docker</a>, Jira,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/jenkins-fundamentals" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/jenkins-fundamentals" target="_blank">Jenkins</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/puppet-quick-start" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/puppet-quick-start" target="_blank">Puppet</a>, Chef,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/splunk-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/splunk-deep-dive" target="_blank">Splunk</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/introduction-to-ansible" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/introduction-to-ansible" target="_blank">Ansible</a>,&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/kubernetes-deep-dive" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/kubernetes-deep-dive" target="_blank">Kubernetes</a>, Bamboo, and&nbsp;<a data-tooltip-position="top" aria-label="https://acloudguru.com/course/nagios-certified-professional-prep-course" rel="noopener nofollow" class="external-link" href="https://acloudguru.com/course/nagios-certified-professional-prep-course" target="_blank">Nagios</a>.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/agile-vs-devops.html</link><guid isPermaLink="false">DevOps/For Reading/Agile Vs DevOps.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:52:57 GMT</pubDate><enclosure url="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.pluralsight.com/content/dam/ps/blog/migration/app/uploads/2021/03/ResourcePage-1.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Git Fetch vs Git pull]]></title><description><![CDATA[ 
 <br><br>Git fetch and git pull are both Git commands used to retrieve update information from a remote repository. So, how do they differ? Git fetch downloads the changes from the remote repository to the local repository but does not make any changes to the current working directory. Since the changes are not merged into the local branch, you can check the changes from the remote repository without interrupting your current work. On the other hand, git pull retrieves the latest changes from the remote repository like git fetch, but it also automatically merges those changes into the current branch. In contrast to git fetch, git pull directly applies the changes from the remote repository to the local working directory.<br><br>The git fetch command retrieves the latest commit history from the remote repository, but it does not affect the local working directory. Even after fetching remote changes, they are not reflected in the local branch. It is primarily used when you want to retrieve the latest status from the remote repository and review the changes before they are reflected in the local repository. To apply the retrieved changes to the local branch, you need to manually run git merge or&nbsp;<a data-tooltip-position="top" aria-label="https://docs.gitlab.com/ee/topics/git/git_rebase.html" rel="noopener nofollow" class="external-link" href="https://docs.gitlab.com/ee/topics/git/git_rebase.html" target="_blank">git rebase</a>.<br><br>The git pull command combines&nbsp;git fetch&nbsp;and&nbsp;git merge&nbsp;(or&nbsp;git rebase) into a single command. This allows you to fetch changes from the remote repository and automatically integrate them into the current local branch.<br>While git fetch retrieves changes from the remote repository without applying them to the local branch, running git pull automatically integrates the changes from the remote repository into the local branch.<br>Git pull is suitable for quickly reflecting remote changes in the local branch, but it can lead to conflicts, so caution is needed, especially when working with multiple people.<br><br>Git fetch is a command used to retrieve the latest information from a remote repository. The retrieved information is not directly reflected in the local branch. Using git pull will reflect all remote branches, including incorrect or problematic ones, in the local branch.<br>When changes are made simultaneously on both remote and local branches, or when there are new users on the team, it is safer to use git fetch to retrieve the remote branch contents first and then perform merge or rebase.<br><br>Git pull is a command that performs more processes compared to git fetch. Git pull can perform both git fetch and additionally execute git merge or git rebase. For this reason, git pull is recommended when you want to quickly reflect changes from the remote repository in the local branch.<br><br><br>Git pull is a command that performs git fetch followed by git merge or git rebase. While git fetch does not affect the local repository, git pull automatically synchronizes changes from the remote repository with the local repository.<br><br>When executing git pull, there may be conflicts between remote and local changes. Merge conflicts are particularly likely to occur, so if conflicts arise, they need to be resolved manually. Additionally, using git pull --rebase allows you to incorporate the latest changes while performing a rebase.<br><br>Git fetch is useful for checking and retrieving the latest status of the remote repository. However, the changes retrieved are not automatically reflected in the local branch; git fetch is used to synchronize the local and remote repositories.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/git-fetch-vs-git-pull.html</link><guid isPermaLink="false">DevOps/For Reading/Git Fetch vs Git pull.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:19:02 GMT</pubDate></item><item><title><![CDATA[Git Theory]]></title><description><![CDATA[ 
 <br><br>A centralized version control system (VCS) uses a single, central repository to store all file versions and their change history. Team members have their own working copies, but all modifications are ultimately committed to this central server. This facilitates collaboration by providing a single source of truth, but it also creates a single point of failure. Examples include <a data-tooltip-position="top" aria-label="https://subversion.apache.org/" rel="noopener nofollow" class="external-link" href="https://subversion.apache.org/" target="_blank">Subversion (SVN)</a> and <a data-tooltip-position="top" aria-label="https://www.nongnu.org/cvs/" rel="noopener nofollow" class="external-link" href="https://www.nongnu.org/cvs/" target="_blank">CVS</a>.<br><img alt="CVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"><br><br>In a Decentralized Version Control System (DVCS), every user has a complete copy of the repository, including its entire history. This eliminates the reliance on a central server, allowing for offline work and greater flexibility. Changes are shared between repositories as needed. Popular examples include <a data-tooltip-position="top" aria-label="https://git-scm.com" rel="noopener nofollow" class="external-link" href="https://git-scm.com" target="_blank">Git</a> and <a data-tooltip-position="top" aria-label="https://www.mercurial-scm.org/" rel="noopener nofollow" class="external-link" href="https://www.mercurial-scm.org/" target="_blank">Mercurial</a>.<br><img alt="DVCS-Diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/dvcs-diagram.svg"><br><br>Git is like a super-powered tracking system for your files and the changes you make to them over time. Imagine it as a special folder that remembers every version of your work, allowing you to go back to any previous stage if needed. &nbsp;<br>Here's what makes Git special:<br>
<br>Keeps a detailed history: Git meticulously records every change you make to your files, who made it, and when. This history helps you understand how your project evolved. &nbsp;
<br>Branching and merging: Git allows you to create separate branches, like alternate timelines, to experiment with new features or try different ideas without affecting the main project. You can then merge these branches back into the main project when you're ready. &nbsp;
<br>Collaboration made easy: Git is designed for teamwork. Multiple people can work on the same project simultaneously, and Git helps manage and integrate everyone's contributions smoothly. &nbsp;
<br>Offline access: You have the entire project history on your computer, so you can work even without an internet connection. &nbsp;
<br>Popular and widely used: Git is the most popular version control system in the world, used by countless developers and companies. &nbsp;
<br><br>Git was created by Linus Torvalds, the famous creator of the Linux operating system, in 2005. He needed a better tool to manage the Linux kernel development, as the existing version control systems were not efficient enough for such a large and complex project.<br>Here's a glimpse of Git's history:<br>
<br>Early Days (2002-2005): Linux kernel development relied on a proprietary DVCS called <a data-tooltip-position="top" aria-label="https://www.bitkeeper.org/" rel="noopener nofollow" class="external-link" href="https://www.bitkeeper.org/" target="_blank">BitKeeper</a>. When its free-of-charge use was revoked, Torvalds decided to create his own version control system, with the goal of being faster, simpler, and more robust.
<br>Birth of Git (April 2005): Torvalds began work on Git and within a remarkably short period, had a functional system ready to manage the Linux kernel.
<br>Community Takes Over (July 2005): Junio Hamano took over the maintenance of Git, guiding its development and shaping it into the mature system it is today.
<br>Widespread Adoption: Git's speed, flexibility, and powerful features quickly gained popularity among developers. It became the preferred choice for open-source projects and eventually spread to commercial software development.
<br>Today, Git is the most widely used version control system worldwide, powering the development of countless software projects, from small personal projects to massive corporate endeavors.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/git-theory.html</link><guid isPermaLink="false">DevOps/For Reading/Git Theory.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 13:28:03 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/cvcs-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[K8s Service Types]]></title><description><![CDATA[ 
 <br><br>The idea of a Service is to group a set of Pod endpoints into a single resource. You can configure various ways to access the grouping. By default, you get a stable cluster IP address that clients inside the cluster can use to contact Pods in the Service. A client sends a request to the stable IP address, and the request is routed to one of the Pods in the Service.<br>A Service identifies its member Pods with a selector. For a Pod to be a member of the Service, the Pod must have all of the labels specified in the selector. A&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" target="_blank">label</a>&nbsp;is an arbitrary key/value pair that is attached to an object.<br>The following Service manifest has a selector that specifies two labels. The&nbsp;selector&nbsp;field says any Pod that has both the&nbsp;app: metrics&nbsp;label and the&nbsp;department:engineering&nbsp;label is a member of this Service.<br>apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: metrics
    department: engineering
  ports:
  ...
<br><br>In a Kubernetes cluster, each Pod has an internal IP address. But the Pods in a Deployment come and go, and their IP addresses change. So it doesn't make sense to use Pod IP addresses directly. With a Service, you get a stable IP address that lasts for the life of the Service, even as the IP addresses of the member Pods change.<br>A Service also provides load balancing. Clients call a single, stable IP address, and their requests are balanced across the Pods that are members of the Service.<br><br>There are five types of Services:<br>
<br>ClusterIP (default):&nbsp;Internal clients send requests to a stable internal IP address.<br>

<br>NodePort:&nbsp;Clients send requests to the IP address of a node on one or more&nbsp;nodePort&nbsp;values that are specified by the Service.<br>

<br>LoadBalancer:&nbsp;Clients send requests to the IP address of a network load balancer.<br>

<br>ExternalName:&nbsp;Internal clients use the DNS name of a Service as an alias for an external DNS name.<br>

<br>Headless:&nbsp;You can use a&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" target="_blank">headless service</a>&nbsp;when you want a Pod grouping, but don't need a stable IP address.<br>

<br>Note
The&nbsp;NodePort&nbsp;type is an extension of the&nbsp;ClusterIP&nbsp;type. So a Service of type&nbsp;NodePort&nbsp;has a cluster IP address.
The&nbsp;LoadBalancer&nbsp;type is an extension of the&nbsp;NodePort&nbsp;type. So a Service of type&nbsp;LoadBalancer&nbsp;has a cluster IP address and one or more&nbsp;nodePort&nbsp;values.
<br><br>ClusterIP&nbsp;is the default service type in Kubernetes, and it provides internal connectivity between different components of our application.&nbsp;<br>Kubernetes assigns a virtual IP address to a&nbsp;ClusterIP&nbsp;service that can solely be accessed from within the cluster during its creation.&nbsp;This IP address is stable and doesn‚Äôt change even if the pods behind the service are rescheduled or replaced.<br>ClusterIP&nbsp;services are an excellent choice for internal communication between different components of our application that don‚Äôt need to be exposed to the outside world. For example, if we have a microservice that processes data and sends it to another microservice for further processing, we can use a&nbsp;ClusterIP&nbsp;service to connect them.<br>When you create a Service of type&nbsp;ClusterIP, Kubernetes creates a stable IP address that is accessible from nodes in the cluster.<br>Here is a manifest for a Service of type ClusterIP:<br>apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: backend
  ports:
  - name: http
    port: 80
    targetPort: 8080
<br>In this example, we define a service named&nbsp;backend&nbsp;with a selector that targets pods labeled with&nbsp;app: backend.&nbsp;The service exposes port&nbsp;80, which is the port used by clients to access the service, and forwards the traffic to the pods‚Äô port&nbsp;8080, which is where the backend application is running.<br>You can&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps" target="_blank">create the Service</a>&nbsp;by using&nbsp;kubectl apply -f [MANIFEST_FILE]. After you create the Service, you can use&nbsp;kubectl get service&nbsp;to see the stable IP address:<br>NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)
my-cip-service   ClusterIP   10.11.247.213   none          80/TCP
<br>Clients in the cluster call the Service by using the cluster IP address and the TCP port specified in the&nbsp;port&nbsp;field of the Service manifest. The request is forwarded to one of the member Pods on the TCP port specified in the&nbsp;targetPort&nbsp;field. For the preceding example, a client calls the Service at&nbsp;10.11.247.213&nbsp;on TCP port 80. The request is forwarded to one of the member Pods on TCP port 8080. The member Pod must have a container that is listening on TCP port 8080. If there is no container listening on port 8080, clients will see a message like "Failed to connect" or "This site can't be reached".<br><br>NodePort&nbsp;services extend the functionality of&nbsp;ClusterIP&nbsp;services by enabling external connectivity to our application. When we create a&nbsp;NodePort&nbsp;service on any node within the cluster that meets the defined criteria,&nbsp;Kubernetes opens up a designated port that forwards traffic to the corresponding&nbsp;ClusterIP&nbsp;service running on the node.<br>These services are ideal for applications that need to be accessible from outside the cluster, such as web applications or APIs. With&nbsp;NodePort&nbsp;services, we can access our application using the node‚Äôs IP address and the port number assigned to the service.<br>When you create a Service of type&nbsp;NodePort, Kubernetes gives you a&nbsp;nodePort&nbsp;value. Then the Service is accessible by using the IP address of any node along with the&nbsp;nodePort&nbsp;value.<br>Here is a manifest for a Service of type&nbsp;NodePort:<br>apiVersion: v1 
kind: Service 
metadata: 
  name: frontend 
spec: 
  selector: 
    app: frontend 
  type: NodePort 
  ports: 
    - name: http 
      port: 80 
      targetPort: 8080
<br>We define a service named&nbsp;frontend&nbsp;that targets pods labeled with&nbsp;app: frontend&nbsp;by setting a selector. The service exposes port&nbsp;80&nbsp;and forwards the traffic to the pods‚Äô port&nbsp;8080. We set the service type to&nbsp;NodePort, and Kubernetes exposes the service on a specific port on a qualifying node within the cluster.<br>When we create a&nbsp;NodePort&nbsp;service, Kubernetes assigns a port number from a predefined range of&nbsp;30000-32767. Additionally, we can specify a custom port number by adding the&nbsp;nodePort&nbsp;field to the service definition:<br>apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  selector:
    app: frontend
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 8080
    nodePort: 30080
<br>The&nbsp;nodePort&nbsp;field is specified as&nbsp;30080, which tells Kubernetes to expose the service on port&nbsp;30080&nbsp;on every node in the cluster.<br>After you create the Service, you can use&nbsp;kubectl get service -o yaml&nbsp;to view its specification and see the&nbsp;nodePort&nbsp;value.<br>spec:
  clusterIP: 10.11.254.114
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32675
    port: 80
    protocol: TCP
    targetPort: 8080
<br>External clients call the Service by using the external IP address of a node along with the TCP port specified by&nbsp;nodePort. The request is forwarded to one of the member Pods on the TCP port specified by the&nbsp;targetPort&nbsp;field.<br>For example, suppose the external IP address of one of the cluster nodes is&nbsp;203.0.113.2. Then for the preceding example, the external client calls the Service at&nbsp;203.0.113.2&nbsp;on TCP port 32675. The request is forwarded to one of the member Pods on TCP port 8080. The member Pod must have a container listening on TCP port 8080.<br>The&nbsp;NodePort&nbsp;Service type is an extension of the&nbsp;ClusterIP&nbsp;Service type. So internal clients have two ways to call the Service:<br>
<br>Use&nbsp;clusterIP&nbsp;and&nbsp;port.
<br>Use a node's IP address and&nbsp;nodePort.
<br>For some cluster configurations, the&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/load-balancing/docs/https" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/load-balancing/docs/https" target="_blank">external Application Load Balancer</a>&nbsp;uses a Service of type&nbsp;NodePort.<br>An external Application Load Balancer is a proxy server, and is fundamentally different from the&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/load-balancing/docs/network" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/load-balancing/docs/network" target="_blank">external passthrough Network Load Balancer</a>&nbsp;described in this topic under&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_loadbalancer" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_loadbalancer" target="_blank">Service of type LoadBalancer</a>.<br>Note
You can specify your own&nbsp;nodePort&nbsp;value in the 30000--32767 range. However, it's best to omit the field and let Kubernetes allocate a&nbsp;nodePort&nbsp;for you. This avoids collisions between Services.
<br><br><a data-tooltip-position="top" aria-label="https://www.baeldung.com/ops/kubernetes-ingress-vs-load-balancer" rel="noopener nofollow" class="external-link" href="https://www.baeldung.com/ops/kubernetes-ingress-vs-load-balancer" target="_blank"><em></em>&nbsp;services</a>LoadBalancer&nbsp;connect our applications externally, and production environments use them where high availability and scalability are critical. When we create a&nbsp;LoadBalancer&nbsp;service,&nbsp;Kubernetes provisions a load balancer in our cloud environment and forwards the traffic to the nodes running the service.<br>LoadBalancer&nbsp;services are ideal for applications that need to handle high traffic volumes, such as web applications or APIs. With&nbsp;LoadBalancer&nbsp;services, we can access our application using a single IP address assigned to the load balancer.<br>Here‚Äôs an example of a simple&nbsp;LoadBalancer&nbsp;service definition:<br>apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  type: LoadBalancer
  ports:
    - name: http
      port: 80
      targetPort: 8080
<br>We set the service type to&nbsp;LoadBalancer&nbsp;to instruct Kubernetes to provision a load balancer. Here, we define a service named&nbsp;web&nbsp;and specify a selector that targets pods labeled with&nbsp;app: web. Additionally, we expose port&nbsp;80&nbsp;and forward traffic to the pods‚Äô port&nbsp;8080.<br>After creating the&nbsp;LoadBalancer&nbsp;service, Kubernetes provisions a load balancer in the cloud environment with a public IP address. We can use this IP address to access our application from outside the cluster.<br><br>A Service of type&nbsp;ExternalName&nbsp;provides an internal alias for an external DNS name. Internal clients make requests using the internal DNS name, and the requests are redirected to the external name.<br>Here is a manifest for a Service of type&nbsp;ExternalName:<br>apiVersion: v1
kind: Service
metadata:
  name: my-xn-service
spec:
  type: ExternalName
  externalName: example.com
<br>When you create a Service, Kubernetes creates a DNS name that internal clients can use to call the Service. For the preceding example, the DNS name is my-xn-service.default.svc.cluster.local. When an internal client makes a request to my-xn-service.default.svc.cluster.local, the request gets redirected to example.com.<br>The&nbsp;ExternalName&nbsp;Service type is fundamentally different from the other Service types. In fact, a Service of type&nbsp;ExternalName&nbsp;does not fit the definition of Service given at the beginning of this topic. A Service of type&nbsp;ExternalName&nbsp;is not associated with a set of Pods, and it does not have a stable IP address. Instead, a Service of type&nbsp;ExternalName&nbsp;is a mapping from an internal DNS name to an external DNS name.<br><br>A headless Service is a type of Kubernetes Service that does not allocate a cluster IP address. Instead, a headless Service uses DNS to expose the IP addresses of the Pods that are associated with the Service. This allows you to connect directly to the Pods, instead of going through a proxy.<br>Headless Services are useful for a variety of scenarios, including:<br>
<br>Load balancing across pods: You can use headless Services to load balance across Pods. To implement this, create a Service with a selector that matches the Pods that you want to load balance. The Service will then distribute traffic evenly across all of the Pods that match the selector.<br>

<br>Service discovery: You can use a headless Service to implement Service discovery. To implement this, create a Service with a name and a selector. DNS record for the headless service contains all the IPs of the Pods behind the Service that match the selector. Clients can use these DNS records to find the IP addresses of the Pods that are associated with the Service.<br>

<br>Direct Pod access: Clients can connect directly to the Pods that are associated with a headless Service, which can be useful for Services that require direct access to the underlying Pods, such as load balancers and DNS servers.<br>

<br>Flexibility: Headless services can be used to create a variety of different topologies, such as load balancers, DNS servers, and distributed databases.<br>

<br>If you have special network requirements for your workloads that can not be solved using headless Services with selectors, there is also the possibility of using headless Services without selectors. Headless Services are a useful tool for accessing Services that are not located within the Kubernetes cluster itself, as the control plane does not create EndpointSlice objects, you can read more about it in&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/services-networking/service/#without-selectors" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/services-networking/service/#without-selectors" target="_blank">Service without selectors</a><br>The following example is a manifest for a Headless Service:<br>apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - name: http
    port: 80
    targetPort: 80
<br>Once you have created a headless Service, you can find the IP addresses of the Pods that are associated with the Service by querying the DNS. For example, the following command lists the IP addresses of the Pods that are associated with the nginx Service:<br>Note:&nbsp;This example assumes that the Pods created are tagged with the&nbsp;nginx&nbsp;label.<br>dig +short nginx.default.svc.cluster.local
<br>Another example which uses Kubernetes query expansion::<br>dig +short +search nginx
<br>You can create a headless Service with a single command, and headless Services are easy to update and scale.<br>kubectl create service clusterip my-svc --clusterip="None" --dry-run=client -o yaml &gt; [file.yaml]
<br><br>A Service is an abstraction in the sense that it is not a process that listens on some network interface. Part of the abstraction is implemented in the&nbsp;<a data-tooltip-position="top" aria-label="https://linux.die.net/man/8/iptables" rel="noopener nofollow" class="external-link" href="https://linux.die.net/man/8/iptables" target="_blank">iptables</a>&nbsp;rules of the cluster nodes. Depending on the type of the Service, other parts of the abstraction are implemented by either an&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/load-balancing/docs/network" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/load-balancing/docs/network" target="_blank">external passthrough Network Load Balancer</a>&nbsp;or an&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/load-balancing/docs/https" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/load-balancing/docs/https" target="_blank">external Application Load Balancer</a>.<br><br>The value of the&nbsp;port&nbsp;field in a Service manifest is arbitrary. However, the value of&nbsp;targetPort&nbsp;is not arbitrary. Each member Pod must have a container listening on&nbsp;targetPort.<br>Here's a Service, of type&nbsp;LoadBalancer, that has a&nbsp;port&nbsp;value of 50000:<br>apiVersion: v1
kind: Service
metadata:
  name: my-ap-service
spec:
  clusterIP: 10.11.241.93
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30641
    port: 50000
    protocol: TCP
    targetPort: 8080
  selector:
    app: parts
    department: engineering
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 203.0.113.200
<br>A client calls the Service at&nbsp;203.0.113.200&nbsp;on TCP port 50000. The request is forwarded to one of the member Pods on TCP port 8080.<br><br>The&nbsp;ports&nbsp;field of a Service is an array of&nbsp;<a data-tooltip-position="top" aria-label="https://v1-25.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceport-v1-core" rel="noopener nofollow" class="external-link" href="https://v1-25.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceport-v1-core" target="_blank">ServicePort</a>&nbsp;objects. The ServicePort object has these fields:<br>
<br>name
<br>protocol
<br>port
<br>targetPort
<br>nodePort
<br>If you have more than one ServicePort, each ServicePort must have a unique name.<br>Here is a Service, of type&nbsp;LoadBalancer, that has two&nbsp;ServicePort&nbsp;objects:<br>apiVersion: v1
kind: Service
metadata:
  name: my-tp-service
spec:
  clusterIP: 10.11.242.196
  externalTrafficPolicy: Cluster
  ports:
  - name: my-first-service-port
    nodePort: 31233
    port: 60000
    protocol: TCP
    targetPort: 50000
  - name: my-second-service-port
    nodePort: 31081
    port: 60001
    protocol: TCP
    targetPort: 8080
  selector:
    app: tests
    department: engineering
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 203.0.113.201
<br>Note:&nbsp;You can specify a maximum of five ports for a LoadBalancer service.<br>In the preceding example, if a client calls the Service at&nbsp;203.0.113.201&nbsp;on TCP port 60000, the request is forwarded to a member Pod on TCP port 50000. But if a client calls the Service at&nbsp;203.0.113.201&nbsp;on TCP port 60001, the request is forwarded to a member Pod on TCP port 8080.<br>Each member Pod must have a container listening on TCP port 50000 and a container listening on TCP port 8080. This could be a single container with two threads, or two containers running in the same Pod.<br><br>When you create a Service, Kubernetes creates an&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#endpoints-v1-core" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#endpoints-v1-core" target="_blank">Endpoints</a>&nbsp;object that has the same name as your Service. Kubernetes uses the Endpoints object to keep track of which Pods are members of the Service.<br><br>You can create an IPv6 Service of type&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_clusterip" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_clusterip" target="_blank"><code></code></a>ClusterIP&nbsp;or&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/concepts/service#service_of_type_nodeport" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/service#service_of_type_nodeport" target="_blank"><code></code></a>NodePort. GKE supports dual-stack Services of type&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer" target="_blank"><code></code></a>LoadBalancer&nbsp;during&nbsp;<a data-tooltip-position="top" aria-label="https://cloud.google.com/products#product-launch-stages" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/products#product-launch-stages" target="_blank">Preview</a>&nbsp;which carries no SLA or technical support.<br>For each of these Service types, you can define&nbsp;ipFamilies&nbsp;and&nbsp;ipFamilyPolicy&nbsp;fields as either IPv4, IPv6, or a&nbsp;<a data-tooltip-position="top" aria-label="https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services" rel="noopener nofollow" class="external-link" href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services" target="_blank">dual-stack</a>&nbsp;Service.<br><br><br><br>
<br><a data-tooltip-position="top" aria-label="https://cloud.google.com/kubernetes-engine/docs/concepts/service" rel="noopener nofollow" class="external-link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/service" target="_blank">GKE Documentation</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.baeldung.com/ops/kubernetes-service-types" target="_blank">https://www.baeldung.com/ops/kubernetes-service-types</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/k8s-service-types.html</link><guid isPermaLink="false">DevOps/For Reading/K8s Service Types.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Fri, 10 Jan 2025 10:37:44 GMT</pubDate></item><item><title><![CDATA[Monolithic vs Microservice Architecture]]></title><description><![CDATA[ 
 <br>Monolithic applications are built as a single, unified unit. Think of it like a large container holding all the components of your application‚Äîuser interface, business logic, database access‚Äîall bundled together. &nbsp;<br>Microservices, on the other hand, break down the application into a collection of small, independent services. Each service focuses on a specific business function and communicates with other services through APIs. Imagine it as a set of interconnected Lego blocks, each serving a distinct purpose. &nbsp;<br>Here's a table highlighting the key differences:<br><br>In a nutshell:<br>
<br>Monolithic apps are simpler to start with but can become complex to manage as they grow. &nbsp;
<br>Microservices offer greater flexibility, scalability, and fault isolation, but they come with increased architectural complexity. &nbsp;
<br>The choice between monolithic and microservices depends on the specific needs and complexity of your application.<br>References:<br>
<br><a rel="noopener nofollow" class="external-link" href="https://rtslabs.com/data-integration-in-a-microservices-architecture" target="_blank">https://rtslabs.com/data-integration-in-a-microservices-architecture</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/monolithic-vs-microservice-architecture.html</link><guid isPermaLink="false">DevOps/For Reading/Monolithic vs Microservice Architecture.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sun, 29 Dec 2024 14:51:11 GMT</pubDate></item><item><title><![CDATA[read about docker.sock file]]></title><description><![CDATA[ 
 <br><br>Basically this file act as a bridge between docker daemon and docker client.<br>
Its responsible for api communications between docker client and dockerd service that runs in the background.<br>So when you run docker run nginx<br>
docker client send request to this file over at /var/run/docker.sock and then docker daemon which is constantly listening for this socket it receives the request and create container.<br><img alt="docker-sock-file.svg" src="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg"><br><br>The docker.sock file is a Unix socket that allows communication between the Docker client and the Docker daemon (the background process that manages containers). It acts like a bridge, enabling you to manage containers, images, networks, and more.<br>Here's a breakdown:<br>
<br>What is a socket? A socket is a way for programs to communicate with each other, either on the same machine or over a network. Think of it like a phone line specifically for applications.
<br>Why a socket instead of a network connection? Using a Unix socket for communication between the client and daemon on the same host is more efficient and secure than using network-based protocols like HTTP or TCP.
<br>How it works: When you run a Docker command (e.g., docker run), the client sends the command to the docker.sock file. The Docker daemon, listening on the other end of the socket, receives the command and executes it.
<br>Location: The docker.sock file is typically located at /var/run/docker.sock on Linux systems.
<br>Security: The docker.sock file is usually owned by the root user and the docker group, which restricts access and enhances security.
<br>Key points to remember:<br>
<br>It's the primary way to interact with the Docker daemon.
<br>It enables efficient and secure communication.
<br>It's a critical component for managing Docker containers and images.
<br>Misusing or exposing the docker.sock file can lead to security risks.
<br>If you're interested in diving deeper, I recommend checking out these resources:<br>
<br>Understanding /var/run/docker.sock: <a rel="noopener nofollow" class="external-link" href="https://dev.to/piyushbagani15/understanding-varrundockersock-the-key-to-dockers-inner-workings-nm7" target="_blank">https://dev.to/piyushbagani15/understanding-varrundockersock-the-key-to-dockers-inner-workings-nm7</a>
<br>What is Docker socket binding? <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/what-is-docker-socket-binding/" target="_blank">https://www.geeksforgeeks.org/what-is-docker-socket-binding/</a>
<br>Docker Tips: about /var/run/docker.sock: <a rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/docker-tips-about-varrundockersock/" target="_blank">https://www.geeksforgeeks.org/docker-tips-about-varrundockersock/</a>
<br><br>When you install docker in a machine. Two diffrent programs come in:<br>
<br>Docker Client
<br>Docker Server
<br>Docker Server receives commands over a socket (either over a network or through a "file")<br>Docker Client communicates over a network and sends message to the Docker server to say make a container, start a container, stop a container etc.<br>When the client and server are running on the same computer, they can connect through a special file called a socket. And since they can communicate through a file and Docker can efficiently share files between hosts and containers, it means you can run the client inside Docker itself.<br>Here is a sample:<br>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker sh
<br>This command creates a container that docker client installed within. And check the volume part:&nbsp;-v /var/run/docker.sock:/var/run/docker.sock<br>With&nbsp;-v&nbsp;flag it shares host&nbsp;docker.sock&nbsp;file so you can manipulate the containers within the host via a container.<br>docker run --rm -it ubuntu bash # Creates a new container via container 
<br>Run&nbsp;docker ps&nbsp;on host terminal.<br>CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES
0f9e333b59fe   ubuntu    "bash"                   5 seconds ago    Up 4 seconds              zealous_wilson
b4a8af31416b   docker    "docker-entrypoint.s‚Ä¶"   16 minutes ago   Up 16 minutes             epic_elion
<br><br>Docker.sock file<br>The Docker.sock file, also known as the Docker socket, is a Unix domain socket (UDS) file used by Docker to communicate between the Docker daemon (dockerd) and the Docker client. It allows the client to send commands to the daemon and receive responses.<br><br>By default, the Docker.sock file is located at&nbsp;/var/run/docker.sock&nbsp;on Linux systems and&nbsp;C:\ProgramData\docker\socket&nbsp;on Windows systems.<br><br>The Docker.sock file enables the following:<br>
<br>Command execution: The Docker client sends commands (e.g.,&nbsp;docker run,&nbsp;docker ps) to the daemon through the socket, which executes the requested action.
<br>Response retrieval: The daemon sends responses (e.g., container output, error messages) back to the client through the socket.
<br><br>
<br>Access control: The Docker.sock file is owned by the&nbsp;docker&nbsp;group and has restrictive permissions by default, ensuring that only authorized users can access it.
<br>Encryption: The communication between the client and daemon is not encrypted by default. To secure the socket, you can enable TLS encryption using the&nbsp;--tls&nbsp;and&nbsp;--tlscert&nbsp;flags when starting the daemon.
<br><br>You can mount the Docker.sock file inside a container using the&nbsp;-v&nbsp;flag when running the container. For example:<br>docker run -v /var/run/docker.sock:/var/run/docker.sock my-image
<br>This allows the container to access the Docker socket and interact with the Docker daemon on the host system.<br><br>If you need to access the Docker daemon remotely or securely, consider using a TCP socket (e.g.,&nbsp;dockerd -H tcp://0.0.0.0:2375) or a proxy like Docker Compose or Kubernetes.<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://lobster1234.github.io/2019/04/05/docker-socket-file-for-ipc/" target="_blank">https://lobster1234.github.io/2019/04/05/docker-socket-file-for-ipc/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://stackoverflow.com/questions/35110146/what-is-the-purpose-of-the-file-docker-sock" target="_blank">https://stackoverflow.com/questions/35110146/what-is-the-purpose-of-the-file-docker-sock</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.educative.io/answers/var-run-dockersock" target="_blank">https://www.educative.io/answers/var-run-dockersock</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/read-about-docker.sock-file.html</link><guid isPermaLink="false">DevOps/For Reading/read about docker.sock file.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/docker-sock-file.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Reading Docker Inspect output]]></title><description><![CDATA[ 
 <br>Output of Mysql container from <a data-href="Task 3 Create Mysql Container" href="https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html" class="internal-link" target="_self" rel="noopener nofollow">Task 3 Create Mysql Container</a> <br><br>docker inspect &lt;container-id&gt;<br>[
  {
    "Id": "3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3",
    "Created": "2024-12-29T12:46:23.52003502Z",
    "Path": "docker-entrypoint.sh",
    "Args": [
      "mysqld"
    ],
    "State": {
      "Status": "running",
      "Running": true,
      "Paused": false,
      "Restarting": false,
      "OOMKilled": false,
      "Dead": false,
      "Pid": 13864,
      "ExitCode": 0,
      "Error": "",
      "StartedAt": "2024-12-29T12:46:23.928523535Z",
      "FinishedAt": "0001-01-01T00:00:00Z"
    },
    "Image": "sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb",
    "ResolvConfPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/resolv.conf",
    "HostnamePath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hostname",
    "HostsPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hosts",
    "LogPath": "/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3-json.log",
    "Name": "/mysql_container",
    "RestartCount": 0,
    "Driver": "overlay2",
    "Platform": "linux",
    "MountLabel": "",
    "ProcessLabel": "",
    "AppArmorProfile": "",
    "ExecIDs": null,
    "HostConfig": {
      "Binds": [
        "mysql_data:/var/lib/mysql"
      ],
      "ContainerIDFile": "",
      "LogConfig": {
        "Type": "json-file",
        "Config": {}
      },
      "NetworkMode": "default",
      "PortBindings": {
        "3306/tcp": [
          {
            "HostIp": "",
            "HostPort": "3306"
          }
        ]
      },
      "RestartPolicy": {
        "Name": "no",
        "MaximumRetryCount": 0
      },
      "AutoRemove": false,
      "VolumeDriver": "",
      "VolumesFrom": null,
      "ConsoleSize": [
        30,
        120
      ],
      "CapAdd": null,
      "CapDrop": null,
      "CgroupnsMode": "private",
      "Dns": [],
      "DnsOptions": [],
      "DnsSearch": [],
      "ExtraHosts": null,
      "GroupAdd": null,
      "IpcMode": "private",
      "Cgroup": "",
      "Links": null,
      "OomScoreAdj": 0,
      "PidMode": "",
      "Privileged": false,
      "PublishAllPorts": false,
      "ReadonlyRootfs": false,
      "SecurityOpt": null,
      "UTSMode": "",
      "UsernsMode": "",
      "ShmSize": 67108864,
      "Runtime": "runc",
      "Isolation": "",
      "CpuShares": 0,
      "Memory": 0,
      "NanoCpus": 0,
      "CgroupParent": "",
      "BlkioWeight": 0,
      "BlkioWeightDevice": [],
      "BlkioDeviceReadBps": [],
      "BlkioDeviceWriteBps": [],
      "BlkioDeviceReadIOps": [],
      "BlkioDeviceWriteIOps": [],
      "CpuPeriod": 0,
      "CpuQuota": 0,
      "CpuRealtimePeriod": 0,
      "CpuRealtimeRuntime": 0,
      "CpusetCpus": "",
      "CpusetMems": "",
      "Devices": [],
      "DeviceCgroupRules": null,
      "DeviceRequests": null,
      "MemoryReservation": 0,
      "MemorySwap": 0,
      "MemorySwappiness": null,
      "OomKillDisable": null,
      "PidsLimit": null,
      "Ulimits": [
        {
          "Name": "nofile",
          "Hard": 65536,
          "Soft": 32768
        }
      ],
      "CpuCount": 0,
      "CpuPercent": 0,
      "IOMaximumIOps": 0,
      "IOMaximumBandwidth": 0,
      "MaskedPaths": [
        "/proc/asound",
        "/proc/acpi",
        "/proc/kcore",
        "/proc/keys",
        "/proc/latency_stats",
        "/proc/timer_list",
        "/proc/timer_stats",
        "/proc/sched_debug",
        "/proc/scsi",
        "/sys/firmware",
        "/sys/devices/virtual/powercap"
      ],
      "ReadonlyPaths": [
        "/proc/bus",
        "/proc/fs",
        "/proc/irq",
        "/proc/sys",
        "/proc/sysrq-trigger"
      ]
    },
    "GraphDriver": {
      "Data": {
        "LowerDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f-init/diff:/var/lib/docker/overlay2/96c958faea5ea8ce734cd4db5008bb2188c285dce8190a8c7d35528a9e2ad055/diff:/var/lib/docker/overlay2/643ab6a924a7f65786e4033a2c82e8815cbca24318f429d9f62600d0e7e955a1/diff:/var/lib/docker/overlay2/a9e3b51cd314aa5cad0258b1330d9ca358ff9e4f063ed45f34948ff834b39c0a/diff:/var/lib/docker/overlay2/37efbaa9117f660d35ed59ca9683c1c22a545a8be5621032781bd59467451a29/diff:/var/lib/docker/overlay2/ea28ca7d3354a82731cf3e4947821e814414353c8b431189aec28a2e758946cd/diff:/var/lib/docker/overlay2/729ec68eb4d4f5273a13a068bb24d70d34dcb160fceb795f3058054552b28174/diff:/var/lib/docker/overlay2/6876ecc19a1ae8a61be41a8f298e9e0a012c0ab952944a4923ac46559c32ed3c/diff:/var/lib/docker/overlay2/8b4ab94e45bbe1ab2068b4488f60434de5788dcd4626f2eda8ccd367e2c513ac/diff:/var/lib/docker/overlay2/06533c7033b5c8d5069139b433e42842ebadf121567da334c956001cd942a139/diff:/var/lib/docker/overlay2/d01a0a333a0ac1376c37feec0c1cb771b28631b213fb399ed12ff685085c4b71/diff",
        "MergedDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/merged",
        "UpperDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/diff",
        "WorkDir": "/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/work"
      },
      "Name": "overlay2"
    },
    "Mounts": [
      {
        "Type": "volume",
        "Name": "mysql_data",
        "Source": "/var/lib/docker/volumes/mysql_data/_data",
        "Destination": "/var/lib/mysql",
        "Driver": "local",
        "Mode": "z",
        "RW": true,
        "Propagation": ""
      }
    ],
    "Config": {
      "Hostname": "3ce6ffd3357d",
      "Domainname": "",
      "User": "",
      "AttachStdin": false,
      "AttachStdout": true,
      "AttachStderr": true,
      "ExposedPorts": {
        "3306/tcp": {},
        "33060/tcp": {}
      },
      "Tty": false,
      "OpenStdin": false,
      "StdinOnce": false,
      "Env": [
        "MYSQL_USER=name",
        "MYSQL_PASSWORD=passwd",
        "MYSQL_RANDOM_ROOT_PASSWORD=yes",
        "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
        "GOSU_VERSION=1.17",
        "MYSQL_MAJOR=innovation",
        "MYSQL_VERSION=9.1.0-1.el9",
        "MYSQL_SHELL_VERSION=9.1.0-1.el9"
      ],
      "Cmd": [
        "mysqld"
      ],
      "Image": "mysql:latest",
      "Volumes": {
        "/var/lib/mysql": {}
      },
      "WorkingDir": "/",
      "Entrypoint": [
        "docker-entrypoint.sh"
      ],
      "OnBuild": null,
      "Labels": {}
    },
    "NetworkSettings": {
      "Bridge": "",
      "SandboxID": "86cf8ccab46e8354b1949fbc9c3c96452864b775418ee65163a356bb7de1eaac",
      "SandboxKey": "/var/run/docker/netns/86cf8ccab46e",
      "Ports": {
        "3306/tcp": [
          {
            "HostIp": "0.0.0.0",
            "HostPort": "3306"
          },
          {
            "HostIp": "::",
            "HostPort": "3306"
          }
        ],
        "33060/tcp": null
      },
      "HairpinMode": false,
      "LinkLocalIPv6Address": "",
      "LinkLocalIPv6PrefixLen": 0,
      "SecondaryIPAddresses": null,
      "SecondaryIPv6Addresses": null,
      "EndpointID": "b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2",
      "Gateway": "172.17.0.1",
      "GlobalIPv6Address": "",
      "GlobalIPv6PrefixLen": 0,
      "IPAddress": "172.17.0.2",
      "IPPrefixLen": 16,
      "IPv6Gateway": "",
      "MacAddress": "02:42:ac:11:00:02",
      "Networks": {
        "bridge": {
          "IPAMConfig": null,
          "Links": null,
          "Aliases": null,
          "MacAddress": "02:42:ac:11:00:02",
          "NetworkID": "1306a6d500d7b8f9a7da615852fab3d6ee12750f0d8550e6292d8ed5b5850930",
          "EndpointID": "b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2",
          "Gateway": "172.17.0.1",
          "IPAddress": "172.17.0.2",
          "IPPrefixLen": 16,
          "IPv6Gateway": "",
          "GlobalIPv6Address": "",
          "GlobalIPv6PrefixLen": 0,
          "DriverOpts": null,
          "DNSNames": null
        }
      }
    }
  }
]
<br><br>The docker inspect command provides a detailed view of a Docker object's configuration. This could be a container, image, network, volume, etc. You've provided the output for a container. Let's break down the key components:<br>1. Id: 3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3<br>
<br>This is the unique identifier for your container.
<br>2. Created: 2024-12-29T12:46:23.52003502Z<br>
<br>Timestamp indicating when the container was created (in UTC).
<br>3. Path: docker-entrypoint.sh<br>
<br>This is the default command that will be executed within the container when it starts. Entrypoint scripts are useful for setting up the container environment, running services, or executing other tasks.
<br>4. Args: ["mysqld"]<br>
<br>These are the arguments passed to the Path (entrypoint). In this case, "mysqld" is likely the command to start the MySQL server.
<br>5. State:<br>
<br>Status: running - Current state of the container.
<br>Running: true - Indicates if the container is currently running.
<br>Paused: false - Indicates if the container is paused.
<br>Restarting: false - Indicates if the container is in the process of restarting.
<br>OOMKilled: false - Indicates if the container was killed due to running out of memory (Out Of Memory).
<br>Dead: false - Indicates if the container has exited.
<br>Pid: 13864 - Process ID of the main process running inside the container.
<br>ExitCode: 0 - Exit code of the container (0 usually means success).
<br>Error: "" - Any error message associated with the container's state.
<br>StartedAt: 2024-12-29T12:46:23.928523535Z - Timestamp when the container was started.
<br>FinishedAt: 0001-01-01T00:00:00Z - Timestamp when the container finished (if applicable).
<br>6. Image: sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb<br>
<br>The ID (SHA256 hash) of the image that was used to create this container.
<br>7. ResolvConfPath, HostnamePath, HostsPath, LogPath:<br>
<br>Paths to files within the container's filesystem that handle DNS resolution, hostname, host mappings, and container logs.
<br>8. Name: /mysql_container<br>
<br>The name you assigned to the container when you created it.
<br>9. RestartCount: 0<br>
<br>Number of times the container has been restarted.
<br>10. Driver: overlay2<br>
<br>The storage driver used for the container's filesystem layers.
<br>11. Platform: linux<br>
<br>The platform (OS) for which the container was built.
<br>12. MountLabel, ProcessLabel, AppArmorProfile: - Security-related labels and profiles.<br>13. ExecIDs: null - List of IDs of exec instances that have been run in this container (if any).<br>14. HostConfig: This section contains container settings that are specific to the host machine where the container is running:<br>- **Binds:** `["mysql_data:/var/lib/mysql"]` -  Shows volumes mounted inside the container. Here, a volume named "mysql_data" is mounted to `/var/lib/mysql` inside the container. This is often used for persistent storage.
- **LogConfig:** Configuration for container logging.
- **NetworkMode:** `default` - Networking mode for the container.
- **PortBindings:** `{"3306/tcp": [{"HostIp": "", "HostPort": "3306"}]}` -  Port mappings between the host and the container. This exposes the container's port 3306 on the host's port 3306.
- **RestartPolicy:** `{"Name": "no", "MaximumRetryCount": 0}` - Container restart policy (in this case, it won't restart automatically).
- **AutoRemove:** `false` - Whether the container should be automatically removed when it exits.
- **[Many other settings]:**  Resource limits (memory, CPU), security options, and other low-level configurations.
<br>15. GraphDriver: - Data: Information about the graph driver (storage driver) used by the container.<br>16. Mounts: - Detailed information about the mounts configured for the container, including the source, destination, and options.<br>17. Config: This section describes the container's configuration, which is defined by the image and any modifications you made:<br>- **Hostname:** `3ce6ffd3357d` - Hostname of the container.
- **Domainname:**  Domain name of the container.
- **User:** User that the container runs as.
- **AttachStdin, AttachStdout, AttachStderr:**  Whether to attach to standard input/output/error streams.
- **ExposedPorts:**  Ports that the container exposes (may or may not be published to the host).
- **Tty:**  Whether the container has a pseudo-TTY allocated.
- **OpenStdin:** Whether to keep STDIN open even if not attached.
- **Env:** `["MYSQL_USER=name", "MYSQL_PASSWORD=passwd", ...]` - Environment variables set within the container.
- **Cmd:** `["mysqld"]` -  The command to be executed when the container starts.
- **Image:** `mysql:latest` - The image the container was created from.
- **Volumes:** Volumes defined in the container configuration.
- **WorkingDir:** `/` - The working directory inside the container.
- **Entrypoint:** `["docker-entrypoint.sh"]` - The entrypoint script for the container.
- **OnBuild:**  Triggers that execute in the context of the image build.
- **Labels:**  Metadata labels associated with the container.
<br>18. NetworkSettings: This section provides details about the container's network configuration:<br>- **Bridge:**  Name of the bridge network the container is attached to.
- **SandboxID, SandboxKey:** Information related to the container's network namespace.
- **Ports:**  Details about the ports that are published to the host.
- **[Other network settings]:** IP address, MAC address, gateway, and other network-related information.
<br>If you have a specific question about any of these elements, feel free to ask! I'm here to help you understand your Docker container's configuration.]]></description><link>https://notes.sarangwandile.xyz/devops/for-reading/reading-docker-inspect-output.html</link><guid isPermaLink="false">DevOps/For Reading/Reading Docker Inspect output.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate></item><item><title><![CDATA[Kubernetes Architecture]]></title><description><![CDATA[ 
 <br>Kubernetes Cluster Architecture<br>
<img alt="cluster-diagram.svg" src="https://notes.sarangwandile.xyz/lib/media/cluster-diagram.svg"><br>
<br>Control Plane:

<br>API Server: Central management point
<br>etcd: Cluster state database
<br>Controller Manager: Maintains desired state
<br>Scheduler: Places pods on nodes


<br>Worker Nodes:

<br>Kubelet: Node agent
<br>Container Runtime: Runs containers
<br>Kube Proxy: Networking rules
<br>Pods: Groups of containers


<br>External Access:

<br>kubectl CLI: Command-line interface
<br>Load Balancer: External traffic distribution
<br>Ingress: HTTP/HTTPS routing


<br>Storage:

<br>Persistent Volumes: Durable storage


<br>Networking:

<br>CNI: Pod networking
<br>Network Policies: Traffic rules


<br>The arrows show the communication flow between components. For example:<br>
<br>All components communicate through the API Server
<br>Kubelets on worker nodes report to API Server
<br>Pods can access Persistent Volumes
<br>Network policies control pod communication
]]></description><link>https://notes.sarangwandile.xyz/devops/notes/kubernetes-architecture.html</link><guid isPermaLink="false">DevOps/Notes/Kubernetes Architecture.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 07 Jan 2025 16:01:14 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/cluster-diagram.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/cluster-diagram.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task 1 - Creating Tomcat student-ui container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Get the docker image of Amazon linux 
<br>Add tomcat package  
<br>Add student ui 
<br>Then commit the image 
<br>Store on ecr as well as on docker hub
<br><br>Step 1: Getting the the Amazon Linux Image<br>
Upon searching the for amazonlinux image in <a data-tooltip-position="top" aria-label="https://hub.docker.com" rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">dockerhub</a> I found its official image<br>
I pulled it into the system with docker pull amazonlinux<br>sudo docker run -it -d -p 32768:8080 --name tomcat-student-ui amazonlinux`
<br>and I run the image interactively and in detached mode so i can execute shell command later on<br><img alt="{1CC503E6-0A58-42E5-93CF-64946C7AA1EF}.png" src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"><br>Lets dive into the container <br>sudo docker exec -it &lt;containerid&gt; &lt;shell-command&gt;`
<br><img alt="{0119C4A3-2DE3-4CB0-9692-E63934BF4F5C}.png" src="https://notes.sarangwandile.xyz/lib/media/{0119c4a3-2de3-4cb0-9692-e63934bf4f5c}.png"><br><br>Our App student-ui required specific version of tomcat thats why we gonna install it from source<br>Make sure to install dependancies and unzip package before hand<br><br>Install this specific version from source<br>cd /opt # we can use this directory for temporary space
curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip
unzip apache-tomcat-9.0.98.zip # unzip the archive
yum install java-17 -y # tomcat 9 requires java-17 to work properly
cd apache-tomcat-9.0.98/bin/
bash ./catalina.sh start 
<br><img alt="{9BD47135-6F08-466A-A3C7-A11FC17A759D}.png" src="https://notes.sarangwandile.xyz/lib/media/{9bd47135-6f08-466a-a3c7-a11fc17a759d}.png"><br><img alt="{40C024BF-0490-4CFC-9507-9363489D25BD}.png" src="https://notes.sarangwandile.xyz/lib/media/{40c024bf-0490-4cfc-9507-9363489d25bd}.png"><br>It looks like our tomcat server is up and running on desired port<br><br>but first we need these package building tools git and maven for building our student-ui app<br><br>sudo yum install git maven -y
<br><br>git clone https://github.com/Pritam-Khergade/student-ui
<br><br>cd student-ui
mvn clean package
<br><img alt="{8E723B0C-8E54-4F56-94BC-F7D231F3287A}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e723b0c-8e54-4f56-94bc-f7d231f3287a}.png"><br>
this creates .war file in ./target folder<br>
rename it to suitable short name and move to /opt/apache-tomcat-9.0.98/webapps directory<br>mv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war
mv target/studentapp.war /opt/apache-tomcat-9.0.97/webapps/
<br>and the app should be accessible on tomcat server on http://instance-ip:32768/studentapp<br><img alt="{0DFF6CA3-CC34-4310-933F-F66D3256551D}.png" src="https://notes.sarangwandile.xyz/lib/media/{0dff6ca3-cc34-4310-933f-f66d3256551d}.png"><br><br>Before creating the image its better to remove the unnecessary packages that we no longer need to make the size of the image minimal as possible.<br><br>yum remove maven git unzip -y
<br>Lets exit from container shell and build the image <br>sudo docker commit &lt;container-id&gt;
<br><img alt="{A2D16CE6-81BA-4DAF-A151-58EA34744A1B}.png" src="https://notes.sarangwandile.xyz/lib/media/{a2d16ce6-81ba-4daf-a151-58ea34744a1b}.png"><br>You see the created image doesnt have any name so lets give it a tag<br>sudo docker tag &lt;image-id&gt; &lt;newtag&gt;
<br><img alt="{A6DFB555-CE8C-404D-AE0E-0E5BE4AECD43}.png" src="https://notes.sarangwandile.xyz/lib/media/{a6dfb555-ce8c-404d-ae0e-0e5be4aecd43}.png"><br>
now push it to docker hub and ECR<br><br><br>
<br>Login to docker hub
<br>Click on repositories
<br>Create new repository
<br>Give it proper name and click create
<br>Here is my repo looks llike<br><img alt="{8E536B96-794C-49C6-B49F-257C6FEE2E9E}.png" src="https://notes.sarangwandile.xyz/lib/media/{8e536b96-794c-49c6-b49f-257c6fee2e9e}.png"><br>Lets push our image into this repo<br>
First rename add new tag to image appropriate according to docker hub repo name<br>sudo docker tag &lt;old-tag-name&gt; &lt;newtag-name&gt;
<br><img alt="{62B7DAF9-7C95-4E53-BEB1-28F215445939}.png" src="https://notes.sarangwandile.xyz/lib/media/{62b7daf9-7c95-4e53-beb1-28f215445939}.png"><br>and now push it to docker hub<br>sudo docker push archsarangx/tomcat-student-ui:latest
<br><img alt="{3386541C-1516-4A09-9493-93A52B3FF2E0}.png" src="https://notes.sarangwandile.xyz/lib/media/{3386541c-1516-4a09-9493-93a52b3ff2e0}.png"><br>and its successfully uploaded on docker hub at<br>
<img alt="{034DA07E-A936-4284-9683-BF0300BC5EB4}.png" src="https://notes.sarangwandile.xyz/lib/media/{034da07e-a936-4284-9683-bf0300bc5eb4}.png"><br>
and anyone can pull it with<br>docker pull archsarangx/tomcat-student-ui:latest
<br><br><br>
<br>goto amazon ECR service and create repositoy
<br><img alt="Pasted image 20241228180114.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228180114.png"><br>then click on blue repo name and click on view push commands<br><br>aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 970547378605.dkr.ecr.us-west-2.amazonaws.com
<br><br>``<br>docker tag archsarangx/tomcat-student-ui:latest 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br>docker push 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest
<br><img alt="{8230C2A4-4835-4E3A-9500-E4C8B9187FB3}.png" src="https://notes.sarangwandile.xyz/lib/media/{8230c2a4-4835-4e3a-9500-e4c8b9187fb3}.png"><br><img alt="{E9698F18-9E9B-4973-9E1A-3E629727DA11}.png" src="https://notes.sarangwandile.xyz/lib/media/{e9698f18-9e9b-4973-9e1a-3e629727da11}.png"><br>
üéâ And our Image is successfully uploaded on both ECR and docker hub.<br><br>Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-1-creating-tomcat-student-ui-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 1 Creating Tomcat student-ui container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{1cc503e6-0a58-42e5-93cf-64946c7aa1ef}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task 2: Create the Free-css template container]]></title><description><![CDATA[ 
 <br><br><br>
<br>get the docker image of Amazon linux 
<br>install nginx 
<br>add free-css template
<br><br><br>
<br>
Goto docker hub --&gt; <a rel="noopener nofollow" class="external-link" href="https://hub.docker.com" target="_blank">https://hub.docker.com</a>

<br>
Click on Search and search for ---&gt; amazonlinux

<br>
Here I found this official image<br>
<img alt="{BACB7514-FD34-4BC8-9CB6-0314BF45113F}.png" src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png">

<br>
I simply pulled the image into my ec2 instance<br>
docker pull amazonlinux
here is successful pull looks like
<img alt="{BD7598CF-AF45-41FB-82E8-A7E26163D176}.png" src="https://notes.sarangwandile.xyz/lib/media/{bd7598cf-af45-41fb-82e8-a7e26163d176}.png">

<br>
We can further confirm if the image is really present in the system with docker images
<img alt="Pasted image 20241227205640.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227205640.png"><br>
Looks like amazonlinux is successfully pulled in our system<br>
lets start the container using this image

<br>
Running the container from image<br>
sudo docker run -d -p 32768:80 --name free-css-template amazonlinux
We used -d to run the container in detached mode meaning its output wont occupy the terminal screen and quietly run in the background after running this command it will simply give us container-id and gives us prompt.<br>
--name flag will assign the name for our container instead of their random funny names and with -p we are forwarding the network traffic of port 80 of the container to 32768 port of our host machine in this case ec2-instance.<br>
<img alt="{5005E6A5-9A5C-44EE-B8B4-37364CD7D1E2}.png" src="https://notes.sarangwandile.xyz/lib/media/{5005e6a5-9a5c-44ee-b8b4-37364cd7d1e2}.png">
We can check for the further detail of the container with<br>
docker ps<br>
Upon inspection we can see there is no container running<br>
lets check again with docker ps -a and for our surprise container is exited just after it run. Thats because its an Operating system container which doesnt really do anything itself unless we assign it a task or a process that will run in background constantly so we can enter into the container to perform our tasks.
We can achieve this by running the container interactively
but first clean up the exited container with docker rm &lt;container-id&gt;

<br><br>
<br>Run a container interactively.
<br>docker run -it -d -p 32768:80 --name free-css-template amazonlinux
<br>   I run the container with -it and -d so it opened the interactive stdin shell session for me to interact with it and the container itself is in detached mode allowing us to enter into it anytime with exec command<br>docker exec -it &lt;container-id&gt; &lt;shell-command&gt;
<br><img alt="{8DD627B8-39AA-4F0D-8320-CF6DA2D7BB33}.png" src="https://notes.sarangwandile.xyz/lib/media/{8dd627b8-39aa-4f0d-8320-cf6da2d7bb33}.png"><br>we got bash shell session inside container<br>
Now we need to update the packages inside the container<br><br>yum update
<br><img alt="{FEF2B4B8-3A70-49CA-B07C-475B398724EC}.png" src="https://notes.sarangwandile.xyz/lib/media/{fef2b4b8-3a70-49ca-b07c-475b398724ec}.png"><br>
<br>Install nginx package start it
<br>yum install nginx -y
<br> since this amazonlinux container image doesn't come with systemd preinstalled we cannot start nginx daemon so we need to run this command manually in background<br>nginx &amp;
<br>and our nginx server is successfully started at port 32768 on our host ec2-instance<br><img alt="{E0754AEE-5490-497B-AA74-428CD4B239A9}.png" src="https://notes.sarangwandile.xyz/lib/media/{e0754aee-5490-497b-aa74-428cd4b239a9}.png"><br><br>Lets add free css template in it<br>
Head on to <a rel="noopener nofollow" class="external-link" href="https://www.free-css.com/" target="_blank">https://www.free-css.com/</a> and download any free css template you want by right clicking on the download button and copying link address<br>
<img alt="{B54DF9F8-AC7B-420B-9EAA-54AE5D0B261F}.png" src="https://notes.sarangwandile.xyz/lib/media/{b54df9f8-ac7b-420b-9eaa-54ae5d0b261f}.png"><br>
and go to your containers terminal session and download this file in nginx server directory with curl or wget<br>cd /usr/share/nginx/html
curl -O https://www.free-css.com/assets/files/free-css-templates/download/page296/carvilla.zip

# Extract the zip file
unzip carvilla.zip

# If unzip is not found install unzip package
sudo yum install unzip
<br>extracting the zip has extracted all contents into folder named carvilla-v1.0<br>
I'll rename it for my convenient. <br>mv carvilla-v1.0 mywebsite
<br>and the website is running at port 32768/mywebsite<br>now we can get out of this container with exit command.<br><img alt="{ADBAE0E2-C52F-4FE1-9DBE-85A5BABFF759}.png" src="https://notes.sarangwandile.xyz/lib/media/{adbae0e2-c52f-4fe1-9dbe-85a5babff759}.png"><br>We have our container successfully hosting our free-css template on port http://instance-ip:32768/mywebsite<br>
Lets create the image out of this container so we can spin many containers as we want and save the hustle to do all these steps all over again.<br><br>
<br>Creating the image<br>
<img alt="{FE045571-00F2-44B6-B1CB-E7F26EE9AE7E}.png" src="https://notes.sarangwandile.xyz/lib/media/{fe045571-00f2-44b6-b1cb-e7f26ee9ae7e}.png"><br>
Now that we have our container running and our site is visible I want to make image out of it and push it to docker hub 
<br>for that we use docker commit &lt;container-id&gt;<br>it returns with the sha-id<br>
<img alt="{83AC20A6-5137-45BC-8761-6AD28799FF56}.png" src="https://notes.sarangwandile.xyz/lib/media/{83ac20a6-5137-45bc-8761-6ad28799ff56}.png"><br>with docker images we can confirm the image creation<br>
<img alt="{9EE50687-0A6E-4930-966B-D6D50B9E78B7}.png" src="https://notes.sarangwandile.xyz/lib/media/{9ee50687-0a6e-4930-966b-d6d50b9e78b7}.png"><br>our image shown in first row doesnt have name so lets give it a tag and then push it to repo<br>
docker tag &lt;image-id&gt; &lt;tagname&gt;<br>
docker tag local-image:tagname new-repo:tagname<br><img alt="Pasted image 20241227225859.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227225859.png"><br>Here I have already created repository in docker hub so I gave its name here<br>and run docker hub login command before running push command<br>
docker login<br>and push it to our docker hub<br>
docker push new-repo:tagname<br><img alt="Pasted image 20241227231042.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241227231042.png"><br>And our project is successful pushed over docker hub and can be access from here<br><a rel="noopener nofollow" class="external-link" href="https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags" target="_blank">https://hub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags</a><br><br><img alt="{393988AE-E9F0-47A9-A960-5EBE39901FED}.png" src="https://notes.sarangwandile.xyz/lib/media/{393988ae-e9f0-47a9-a960-5ebe39901fed}.png"><br>To store the image into Amazon's Elastic Container Registry which is similar to docker hub but their own we need to create a repository here too.<br>I gave the repo same name<br>
<img alt="{F7CDF0B9-3C17-4D22-9547-7D65863E4BDE}.png" src="https://notes.sarangwandile.xyz/lib/media/{f7cdf0b9-3c17-4d22-9547-7d65863e4bde}.png"><br>and Upon clicking create button my repo is created<br>
<img alt="{7B336AAA-A8CB-4F61-823E-072A4025B6A6}.png" src="https://notes.sarangwandile.xyz/lib/media/{7b336aaa-a8cb-4f61-823e-072a4025b6a6}.png"><br>
Click on the Repo name in blue and click on view push commands to get instruction for pushing our image into this repo<br><img alt="{7CB87CED-E1FD-4698-BCA5-DFC2D6C53B23}.png" src="https://notes.sarangwandile.xyz/lib/media/{7cb87ced-e1fd-4698-bca5-dfc2d6c53b23}.png"><br><br>To push our image we need to first Authenticate with registry for that carefully use first command also we need to have aws-cli installed and configured for that luckily for me I am using amazonlinux ec2 instance so its already installed i can just run aws configure to configure aws-cli<br>
For those havent can check <a data-tooltip-position="top" aria-label="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" rel="noopener nofollow" class="external-link" href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html" target="_blank">this documentation</a> to configure their aws-cli within instance.<br><img alt="Pasted image 20241228154309.png" src="https://notes.sarangwandile.xyz/lib/media/pasted-image-20241228154309.png"><br>and then run this command provided in the instructions but remember to add sudo as we are using standard user<br>
<img alt="{AC6817B4-D44D-4A43-AEFF-16292F4D44D0}.png" src="https://notes.sarangwandile.xyz/lib/media/{ac6817b4-d44d-4a43-aeff-16292f4d44d0}.png"><br>Now that We have successfully login to ECR lets tag the image accordingly and push it to ECR<br>sudo docker tag &lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt; &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;newname&gt;:&lt;newtag&gt;<br><img alt="{19760D25-4AB9-4A59-9757-62575607EF5A}.png" src="https://notes.sarangwandile.xyz/lib/media/{19760d25-4ab9-4a59-9757-62575607ef5a}.png"><br>
and push to ECR<br>sudo docker push &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt;<br><img alt="{D18E449C-3E7D-4B49-8AC1-75893D2A92F7}.png" src="https://notes.sarangwandile.xyz/lib/media/{d18e449c-3e7d-4b49-8ac1-75893d2a92f7}.png"><br>and refresh the amazon EKS web site to see the latest image listed in there<br>
<img alt="{48017E2C-FBB3-4AB6-B125-BFEFA58746BE}.png" src="https://notes.sarangwandile.xyz/lib/media/{48017e2c-fbb3-4ab6-b125-bfefa58746be}.png"><br>Thank you for Reading<br>
Have a good day!]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-2-create-the-free-css-template-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 2 Create the Free-css template container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate><enclosure url="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://notes.sarangwandile.xyz/lib/media/{bacb7514-fd34-4bc8-9cb6-0314bf45113f}.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Create the MYSQL Container]]></title><description><![CDATA[ 
 <br><br><br>
<br>Host mysql container 
<br>Use -e ## environment variables
<br>use MYSQL_USER, MYSQL_PASSWORD
<br><br><br><br>sudo docker pull mysql
<br><br>sudo docker run -e MYSQL_USER=sarang -e MYSQL_PASSWORD=mysupersecretpassword -e MYSQL_RANDOM_ROOT_PASSWORD=yes -p 3306:3306 --name mysql_container -v mysql_data:/var/lib/mysql mysql:latest
<br>Here we used -e flag to specify each environment variables for mysql<br>MYSQL_USER=sarang  This sets the username for our mysql database<br>
MYSQL_PASSWORD=mysupersecretpassword This sets the password for our database<br>MYSQL_RANDOM_ROOT_PASSWORD=yes    # This tells the mysql to generate a random root password (it can be seen in docker logs)<br>
-v mysql_data:/var/lib/mysql mysql:latest This flag sets the persistence volume for our container and it mounts /var/lib/mysql directory of the container to the mysql_data directory of our host.]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-3-create-mysql-container.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 3 Create Mysql Container.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate></item><item><title><![CDATA[Task 4 Create daemon service of tomcat]]></title><description><![CDATA[ 
 <br>The simplest possible script I could make for tomcat is this one<br># make catalina.sh executable
chmod +x ~/apache-tomcat-9.0.98/bin/catalina.sh 

# navigate to systemd services directory
cd /etc/systemd/system

# Create new tomcat service file
vi tomcat.service
<br>Add this script <br>[Unit]
Description=Tomcat Service Daemon

[Service]
Type=forking
ExecStart=home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start
ExecReload=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start
ExecStop=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh stop

[Install]
WantedBy=multi-user.target
<br>Run these commands afterwards<br># Reloading the systemd daemon
systemctl daemon-reload

# Enabling and starting tomcat.service
systemctl enable --now tomcat.service

# Checking Status of our service
systemctl status tomcat.service
<br>However this isnt sufficient for tomcats full usecase<br>
for that you need to add Environment Variables<br># Set environment variables (optional but recommended) 

# Update with your Java installation path
Environment=JAVA_HOME=/usr/lib/jvm/jre 

# Update with your Tomcat PID file path 
Environment=CATALINA_PID=/home/ec2-user/apache-tomcat-9.0.98/temp/tomcat.pid 

# Update with your Tomcat installation path 
Environment=CATALINA_HOME=/home/ec2-user/apache-tomcat-9.0.98 

# Update with your Tomcat base directory 
Environment=CATALINA_BASE=/home/ec2-user/apache-tomcat-9.0.98 

# Customize JVM options 
Environment='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'
Environment='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'

<br>also for security purposes its recommended to use tomcat service as a dedicated tomcat user<br>after making changes in the file its important to reload systemd daemon with systemctl daemon-reload and restarting the tomat service and check the status.<br>If everything is working and your tomcat page is displaying on port 8080 it means your tomcat service is successfully working.]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-4-create-daemon-service-of-tomcat.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 4 Create daemon service of tomcat.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Mon, 30 Dec 2024 16:13:04 GMT</pubDate></item><item><title><![CDATA[Task 5 Create dockerfiles]]></title><description><![CDATA[ 
 <br><br>
<br>Create a Seperate folder and move to it<br>
mkdir my_nginx_image &amp;&amp; cd my_nginx_image
<br>Create a docker file with vi Dockerfile
<br>FROM nginx:latest
COPY oxer-html /usr/share/nginx/html/.
<br>
<br>Build the Image docker build -t . my_nginx_image
<br>Run the container docker run -dP my_nginx_image
<br><br>FROM amazonlinux:latest
RUN yun update &amp;&amp; yum install nginx
EXPOSE 80
CMD ["/usr/sbin/nginx", "-g", "daemon off;"]
<br><br>FROM amazonlinux:latest
WORKDIR /opt
RUN yum update &amp;&amp; yum install java-17 unzip -y
RUN curl -O https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip
RUN unzip apache-tomcat-9.0.98.zip
RUN chmod +x /opt/apache-tomcat-9.0.98/bin/catalina.sh 
EXPOSE 8080
CMD ["/opt/apache-tomcat-9.0.98/bin/catalina.sh", "run"]
<br><br>
<br><a data-href="2024-12-31 Dockerfile Explained" href="https://notes.sarangwandile.xyz/daily-notes/2024-12-31-dockerfile-explained.html" class="internal-link" target="_self" rel="noopener nofollow">2024-12-31 Dockerfile Explained</a>
]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-5-create-dockerfiles.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 5 Create dockerfiles.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 31 Dec 2024 17:35:52 GMT</pubDate></item><item><title><![CDATA[Task 6 How to set permanent alias]]></title><description><![CDATA[ 
 <br>aliases are shortnames or nick names for big forgottable commands user sets by themselves the use for their own convenience and comfort over big and tedius commands.<br>Example. If you set alias for ls -la --color=auto command to be ll it will automagically trigger that command everytime you use ll command.<br>There are two ways to set Alias in linux terminal:<br><br>alias k="kubectl"
<br>Here you only set alias for your current shell meaning as long you have that shell session on you can use it if you kill it or spawn in new shell or restart terminal it wont be available again. To use permanent Alias use Method 2.<br><br>Adding this line in ~/.bashrc or /.bash-profile file<br>You can add this line into your shell configuration whether its bash or zsh. it will work.<br>alias k="kubectl"
<br>Limitation: For simple purposes this is fine but it wont work in your shell scripts cuz its limited to your shell and not its child processes.<br>Using export will add aditional functionality to your alias and expands its availability to the child processess of shell also which makes them usable in your custom shell scripts too.<br>export alias k="kubectl"
<br>You can add your alias in this way either in ~/.bashrc or ~/.profile or ~/.bash-profile file<br>Immediate result may not be seen you need to restart shell or source the file again <br>source ~/.bashrc
or 
. ~/.bashrc
]]></description><link>https://notes.sarangwandile.xyz/devops/tasks-done/task-6-how-to-set-permanent-alias.html</link><guid isPermaLink="false">DevOps/Tasks Done/Task 6 How to set permanent alias.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Tue, 07 Jan 2025 08:57:04 GMT</pubDate></item><item><title><![CDATA[Top 40 Linux Commands]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>]]></description><link>https://notes.sarangwandile.xyz/linux/top-40-linux-commands.html</link><guid isPermaLink="false">Linux/Top 40 Linux Commands.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Sat, 04 Jan 2025 05:21:49 GMT</pubDate></item><item><title><![CDATA[Understanding Vim]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br>Vim Modes<br>]]></description><link>https://notes.sarangwandile.xyz/linux/understanding-vim.html</link><guid isPermaLink="false">Linux/Understanding Vim.md</guid><dc:creator><![CDATA[Sarang Wandile]]></dc:creator><pubDate>Thu, 19 Dec 2024 16:34:29 GMT</pubDate></item></channel></rss>