{"AWS/For-Reading/AWS-Instance-Types":{"title":"AWS Instance Types","links":[],"tags":[],"content":"AWS EC2 Instance Types Grouped by Family\nGeneral-Purpose\n\nM1 (e.g., m1.small, m1.medium, m1.large)\n\nBalanced CPU, memory, and network resources\nSuitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications\n\n\nM3 (e.g., m3.large, m3.xlarge, m3.2xlarge)\n\nHigher CPU performance and larger instance sizes\nRecommended for general-purpose instances with demanding CPU requirements\n\n\nM6g (e.g., m6g.medium, m6g.large, m6g.xlarge, m6g.2xlarge)\n\nLatest generation of general-purpose instances\nProvides a balance of CPU, memory, and network resources\nSuitable for small to medium-sized databases, data processing, caching, and backend servers for SAP, Microsoft SharePoint, and other enterprise applications\n\n\n\nCompute-Optimized\n\nC1 (e.g., c1.xlarge, c1.2xlarge, c1.4xlarge)\n\nHigh CPU-to-memory ratio\nIdeal for CPU-bound scale-out applications, such as front-end fleets for high-traffic web sites, on-demand batch processing, distributed analytics, web servers, video encoding, and high-performance science and engineering applications\n\n\nCC2 (e.g., cc2.8xlarge)\n\nLatest generation of compute-optimized instances\nProvides the lowest cost for CPU performance among all EC2 instance types\nSupports cluster networking and high core count (32 vCPUs)\n\n\n\nMemory-Optimized\n\nM2 (e.g., m2.xlarge, m2.2xlarge, m2.4xlarge)\n\nHigh memory-to-CPU ratio\nSuitable for memory-bound applications, such as high-performance databases, distributed cache, genome assembly, and larger deployments of SAP, Microsoft SharePoint, and other enterprise applications\n\n\nCR1 (e.g., cr1.16xlarge)\n\nLatest generation of memory-optimized instances\nProvides more memory (244 GiB), faster CPU (Intel Xeon E5-2670), and supports cluster networking\n\n\n\nStorage-Optimized\n\nHI1 (e.g., hi1.4xlarge)\n\nOptimized for very high random I/O performance and low cost per IOPS\nIdeal for transactional applications, such as large deployments of NoSQL databases like Cassandra and MongoDB\n\n\nHS1 (e.g., hs1.8xlarge)\n\nOptimized for very high storage density, low storage cost, and high sequential I/O performance\nSuitable for large-scale data warehouses, large always-on Hadoop clusters, and cluster file systems\n\n\n\nGPU Instances\n\nCG1 (e.g., cg1.4xlarge)\n\nHigh-performance computing instances with NVIDIA Tesla V100 GPUs\nSuitable for applications that require intense graphics processing, such as video encoding, scientific simulations, and machine learning\n\n\n\nMicro Instances\n\nT1 (e.g., t1.micro)\n\nVery low-cost instance option with a small amount of CPU resources\nMay opportunistically increase CPU capacity in short bursts when additional cycles are available\nSuitable for lower-throughput applications, such as bastion hosts or administrative applications, or for low-traffic websites that require additional compute cycles from time to time\n\n\n\nAWS Instance Types for High-Throughput\nAWS instance types support high-throughput workloads like video transcoding and scientific simulations through various combinations of CPU, memory, storage, and networking capacity. Here are some key features and instance types that cater to these workloads:\n\nCompute-Optimized Instances: M5zn, C6g, C6gn, and Hpc6id instances are designed for applications that require extremely high single-thread performance and high throughput, such as:\n\nGaming\nHigh Performance Computing (HPC)\nSimulation modeling for industries like automotive, aerospace, energy, and telecommunications\n\n\nStorage-Optimized Instances: I3en, H1, and I2 instances are ideal for big data processing clusters, MapReduce workloads, and distributed file systems, making them suitable for:\n\nBatch processing workloads\nMedia transcoding\nHigh-performance web servers\nScientific modeling\n\n\nHigh-Performance Computing (HPC) Instances: Hpc6a and Hpc6id instances are designed for tight-coupled, compute-intensive, and high-performance computing jobs, including:\n\nComputational fluid dynamics\nWeather forecasting\nMolecular dynamics\nComplex simulations and deep learning workloads\n\n\nGPU-Powered Instances: Vt1 instances, powered by NVIDIA V100 or T4 GPUs, are optimized for:\n\nLive transcoding video at up to 4K UHD resolutions\nMachine learning inference\nGraphics rendering\nScientific simulations\n\n\nNetwork-Optimized Instances: R instances, with Enhanced Networking and EBS optimization, are suitable for workloads that require high networking throughput, such as:\n\nHigh-performance file systems\nDistributed web scale in-memory caches\nReal-time big data analytics\nTelco applications like 5G User Plane Function (UPF)\n\n\n\nAWS instance types offer a range of configurations to support high-throughput workloads, allowing you to choose the optimal instance for your specific needs. By leveraging these instance types, you can efficiently run demanding workloads like video transcoding and scientific simulations in the cloud.\nAWS Instance Types from AWS DOCS\nWhen you launch an instance, the instance type that you specify determines the hardware of the host computer used for your instance. Each instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities. Select an instance type based on the requirements of the application or software that you plan to run on your instance. For more information about features and use cases, see Amazon EC2 Instance Types Details.\nAmazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such as the network and the disk subsystem, among instances. If each instance on a host computer tries to use as much of one of these shared resources as possible, each receives an equal share of that resource. However, when a resource is underused, an instance can consume a higher share of that resource while it’s available.\nEach instance type provides higher or lower minimum performance from a shared resource. For example, instance types with high I/O performance have a larger allocation of shared resources. Allocating a larger share of shared resources also reduces the variance of I/O performance. For most applications, moderate I/O performance is more than enough. However, for applications that require greater or more consistent I/O performance, consider an instance type with higher I/O performance.\nContents\n\nAvailable instance types\nHardware specifications\nHypervisor type\nAMI virtualization types\nProcessors\nFind an Amazon EC2 instance type\nGet recommendations from EC2 instance type finder\nGet EC2 instance recommendations from Compute Optimizer\nAmazon EC2 instance type changes\nBurstable performance instances\nPerformance acceleration with GPU instances\nAmazon EC2 Mac instances\nAmazon EBS-optimized instance types\nCPU options for Amazon EC2 instances\nAMD SEV-SNP for Amazon EC2 instances\nProcessor state control for Amazon EC2 Linux instances\n\nAvailable instance types\nAmazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.\nInstance type naming conventions\nNames are based on instance family, generation, processor family, capabilities, and size. For more information, see Naming conventions in the Amazon EC2 Instance Types Guide.\nFind an instance type\nTo determine which instance types meet your requirements, such as supported Regions, compute resources, or storage resources, see Find an Amazon EC2 instance type and Amazon EC2 instance type specifications in the Amazon EC2 Instance Types Guide.\nHardware specifications\nFor detailed instance type specifications, see Specifications in the Amazon EC2 Instance Types Guide. For pricing information, see Amazon EC2 On-Demand Pricing.\nTo determine which instance type best meets your needs, we recommend that you launch an instance and use your own benchmark application. Because you pay by the instance second, it’s convenient and inexpensive to test multiple instance types before making a decision. If your needs change, even after you make a decision, you can change the instance type later. For more information, see Amazon EC2 instance type changes.\nHypervisor type\nAmazon EC2 supports the following hypervisors: Xen and Nitro.\nNitro-based instances\n\n\nGeneral purpose: M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8g | T3 | T3a | T4g\n\n\nCompute optimized: C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g\n\n\nMemory optimized: R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6idn | R6in | R6id | R7a | R7g | R7gd | R7i | R7iz | R8g | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | X2gd | X2idn | X2iedn | X2iezn | X8g | z1d\n\n\nStorage optimized: D3 | D3en | I3en | I4g | I4i | Im4gn | Is4gen\n\n\nAccelerated computing: DL1 | DL2q | G4ad | G4dn | G5 | G5g | G6 | G6e | Gr6 | Inf1 | Inf2 | P3dn | P4d | P4de | P5 | P5e | Trn1 | Trn1n | VT1\n\n\nHigh-performance computing: Hpc6a | Hpc6id | Hpc7a | Hpc7g\n\n\nPrevious generation: A1\n\n\nFor more information about the supported versions of Nitro hypervisor, see Network feature support in the Amazon EC2 Instance Types Guide.\nXen-based instances\n\n\nGeneral purpose: M1 | M2 | M3 | M4 | T1 | T2\n\n\nCompute optimized: C1 | C3 | C4\n\n\nMemory optimized: R3 | R4 | X1 | X1e\n\n\nStorage optimized: D2 | H1 | I2 | I3\n\n\nAccelerated computing: F1 | G3 | P2 | P3\n\n\nAMI virtualization types\nThe virtualization type of your instance is determined by the AMI that you use to launch it. Current generation instance types support hardware virtual machine (HVM) only. Some previous generation instance types support paravirtual (PV) and some AWS Regions support PV instances. For more information, see Virtualization types.\nFor best performance, we recommend that you use an HVM AMI. In addition, HVM AMIs are required to take advantage of enhanced networking. HVM virtualization uses hardware-assist technology provided by the AWS platform. With HVM virtualization, the guest VM runs as if it were on a native hardware platform, except that it still uses PV network and storage drivers for improved performance.\nProcessors\nEC2 instances support a variety of processors.\nProcessors\n\nIntel processors\nAMD processors\nAWS Graviton processors\nAWS Trainium\nAWS Inferentia\n\nIntel processors\nAmazon EC2 instances that run on Intel processors might include the following processor features. Not all instances that run on Intel processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types.\n\n\nIntel AES New Instructions (AES-NI) — Intel AES-NI encryption instruction set improves upon the original Advanced Encryption Standard (AES) algorithm to provide faster data protection and greater security. All current generation EC2 instances support this processor feature.\n\n\nIntel Advanced Vector Extensions (Intel AVX, Intel AVX2, and Intel AVX-512) — Intel AVX and Intel AVX2 are 256-bit, and Intel AVX-512 is a 512-bit instruction set extension designed for applications that are Floating Point (FP) intensive. Intel AVX instructions improve performance for applications like image and audio/video processing, scientific simulations, financial analytics, and 3D modeling and analysis. These features are only available on instances launched with HVM AMIs.\n\n\nIntel Turbo Boost Technology — Intel Turbo Boost Technology processors automatically run cores faster than the base operating frequency.\n\n\nIntel Deep Learning Boost (Intel DL Boost) — Accelerates AI deep learning use cases. The 2nd Gen Intel Xeon Scalable processors extend Intel AVX-512 with a new Vector Neural Network Instruction (VNNI/INT8) that significantly increases deep learning inference performance over previous generation Intel Xeon Scalable processors (with FP32) for image recognition/segmentation, object detection, speech recognition, language translation, recommendation systems, reinforcement learning, and more. VNNI may not be compatible with all Linux distributions.\nThe following instances support VNNI: M5n, R5n, M5dn, M5zn, R5b, R5dn, D3, D3en, and C6i. C5 and C5d instances support VNNI for only 12xlarge, 24xlarge, and metal instances.\n\n\nConfusion can result from industry naming conventions for 64-bit CPUs. Chip manufacturer Advanced Micro Devices (AMD) introduced the first commercially successful 64-bit architecture based on the Intel x86 instruction set. Consequently, the architecture is widely referred to as AMD64 regardless of the chip manufacturer. Windows and several Linux distributions follow this practice. This explains why the internal system information on an instance running Ubuntu or Windows displays the CPU architecture as AMD64 even though the instances are running on Intel hardware.\nAMD processors\nAmazon EC2 instances that run on AMD EPYC processors can help you optimize both cost and performance for your workloads. These instances might support the following processor features. Not all instances that run on AMD processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types.\n\n\nAMD Secure Memory Encryption (SME)\n\n\nAMD Transparent Single Key Memory Encryption (TSME)\n\n\nAMD Advanced Vector Extensions (AVX)\n\n\nAMD Secure Encrypted Virtualization-Secure Nested Paging (SEV-SNP)\n\n\nVector Neural Network Instructions (VNNI)\n\n\nBFloat16\n\n\nAWS Graviton processors\nAWS Graviton is a family of processors designed to deliver the best price performance for your workloads running on Amazon EC2 instances.\nFor more information, see Getting started with Graviton.\nAWS Trainium\nInstances powered by AWS Trainium are purpose built for high-performance, cost-effective deep learning training. You can use these instances to train natural language processing, computer vision, and recommender models used across a broad set of applications, such as speech recognition, recommendation, fraud detection, and image and video classification. Use your existing workflows in popular ML frameworks, such as PyTorch and TensorFlow.\nAWS Inferentia\nInstances powered by AWS Inferentia are designed to accelerate machine learning. They provide high performance and low latency machine learning inference. These instances are optimized for deploying deep learning (DL) models for applications, such as natural language processing, object detection and classification, content personalization and filtering, and speech recognition.\nThere are a variety of ways that you can get started:\n\n\nUse SageMaker, a fully-managed service that is the easiest way to get started with machine learning models. For more information, see Get Started with SageMaker in the Amazon SageMaker Developer Guide.\n\n\nLaunch an Inf1 or Inf2 instance using the Deep Learning AMI. For more information, see AWS Inferentia with DLAMI in the AWS Deep Learning AMIs Developer Guide.\n\n\nLaunch an Inf1 or Inf2 instance using your own AMI and install the AWS Neuron SDK, which enables you to compile, run, and profile deep learning models for AWS Inferentia.\n\n\nLaunch a container instance using an Inf1 or Inf2 instance and an Amazon ECS-optimized AMI. For more information, see Amazon Linux 2 (Inferentia) AMIs in the Amazon Elastic Container Service Developer Guide.\n\n\nCreate an Amazon EKS cluster with nodes running Inf1 instances. For more information, see Inferentia support in the Amazon EKS User Guide.\n\n"},"AWS/For-Reading/Agile-Development":{"title":"Agile Development","links":[],"tags":[],"content":"Agile topics\n\n\n                  \n                  Agile Manifesto \n                  \n                \n\nThe agile manifesto outlines 4 values and 12 principles for teams, but—decades later—is it still relevant? Find out\n\n\n\n\n                  \n                  Scrum \n                  \n                \n\nIn scrum, a product is built in a series of fixed-length iterations called sprints, giving agile teams a framework for shipping software on a regular cadence. Learn how the scrum methodology impacts traditional project management.\n\n\n\n\n                  \n                  Kanban \n                  \n                \n\nKanban is a popular agile framework that requires real-time communication of team’s capacity and full transparency of work. Learn how the kanban methodology for agile software development can benefit for your team.\n\n\nWhat is the Agile methodology?\nThe Agile methodology is a project management approach that involves breaking the project into phases and emphasizes continuous collaboration and improvement. Teams follow a cycle of planning, executing, and evaluating.\nWhereas the traditional “waterfall” approach has one discipline contribute to the project, then “throw it over the wall” to the next contributor, agile calls for collaborative cross-functional teams. Open communication, collaboration, adaptation, and trust amongst team members are at the heart of agile. Although the project lead or product owner typically prioritizes the work to be delivered, the team takes the lead on deciding how the work will get done, self-organizing around granular tasks and assignments.\nAgile isn’t defined by a set of ceremonies or specific development techniques. Rather, agile is a group of methodologies that demonstrate a commitment to tight feedback cycles and continuous improvement.\n\nThe original Agile Manifesto didn’t prescribe two-week iterations or an ideal team size. It simply laid out a set of core values that put people first. The way you and your team live those values today – whether you do scrum by the book, or blend elements of kanban and XP – is entirely up to you.\nWhy choose agile?\nTeams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year’s worth of plans. “Just enough” planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.\nBut it’s not just a numbers game—first and foremost, it’s about people. As described by the Agile Manifesto, authentic human interactions are more important than rigid processes. Collaborating with customers and teammates is more important than predefined arrangements. And delivering a working solution to the customer’s problem is more important than hyper-detailed documentation.\nAn agile team unites under a shared vision, then brings it to life the way they know is best. Each team sets their own standards for quality, usability, and completeness. Their “definition of done” then informs how fast they’ll churn the work out. Although it can be scary at first, company leaders find that when they put their trust in an agile team, that team feels a greater sense of ownership and rises to meet (or exceed) management’s expectations.\nAgile yesterday, today, and tomorrow\nThe publication of the Agile Manifesto in 2001 marks the birth of agile as a methodology. Since then, many agile frameworks have emerged such as scrum, kanban, lean, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way. Scrum and XP are favored by software development teams, while kanban is a darling among service-oriented teams like IT or human resources.\nToday, many agile teams combine practices from a few different frameworks, spiced up with practices unique to the team. Some teams adopt some agile rituals (like regular stand-ups, retros, backlogs, etc.), while others created a new agile practice (agile marketing teams who adhere to the Agile Marketing Manifesto).\n\nThe agile teams of tomorrow will value their own effectiveness over adherence to doctrine. Openness, trust, and autonomy are emerging as the cultural currency for companies who want to attract the best people and get the most out of them. Such companies are already proving that practices can vary across teams, as long as they’re guided by the right principles.\nAtlassian on agile\nThe way each team practices agile should be unique to their needs and culture. Indeed, no two teams inside Atlassian have identical agile practices.\nAlthough many of our teams organize their work in sprints, estimate in story points, and prioritize their backlogs, we’re not die-hard practitioners of scrum. Or kanban. Or any other trademarked methodology. Instead, we give each team the autonomy to cherry-pick the practices that will make them most effective. And we encourage you to take a similar approach.\nFor example, if you’re on a queue-oriented team like IT, kanban provides a solid foundation for your agile practice. But nothing should stop you from sprinkling in a few scrum practices like demo sessions with stakeholders or regular retrospectives.\nThe key to doing agile right is embracing a mindset of continuous improvement. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don’t.\n"},"AWS/For-Reading/Agile-Vs-DevOps":{"title":"Agile Vs DevOps","links":[],"tags":[],"content":"Agile and DevOps have shaped the way software is developed today. They’ve become so widely adapted and revered as to permeate beyond the world of software development into shaping project management and org charts in businesses of all stripes.\nDevOps and agile can be tricky to define, and the lines between the two often seem to blur.\n\nAt a basic level, DevOps is the combination of two teams (software development and IT operations) to create a more powerful, efficient software development process.\nAgile is a series of methodologies around iterative development designed to make tasks smaller and more manageable and increase collaboration.\n\nBut what are the deeper differences between DevOps and agile? Is DevOps better than agile? Or can DevOps and agile be implemented together? In this post, we’ll dive into some common agile and DevOps FAQs.\nDevOps vs Agile\n\nWhat is the difference between DevOps and agile?\nWhat is agile?\nWhat are the benefits of agile?\nHow can a company be agile?\nWhat is DevOps?\nWhy combine software development and IT operations?\nWhat are some DevOps concepts and key terms?\nWhat are the benefits of DevOps?\nIs DevOps better than agile? Is DevOps a replacement for agile?\nWhat is CI/CD?\nWhat are the benefits of CI/CD?\nWhat are some common DevOps tools?\nHow can I learn more about DevOps?\n\nWhat is the difference between DevOps and agile?\nAgile and DevOps are both used in the development of software. While they’re both designed to improve the software development process, they seek to do so by taking different approaches. But they’re not mutually exclusive. (More on that below.)\nAgile is essentially about giving software developers a shared set of principles to drive decision-making and allow for more responsiveness to change.\nDevOps is about a culture change intended to drive collaboration between software developers and IT operations.\nIt’s often said that DevOps is agile applied beyond the software development team.\n\n\nPost-COVID DevOps: Accelerating the Future\nHow has COVID affected — or even accelerated — DevOps best practices for engineering teams? Watch this free, on-demand webinar panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.\nWatch Now\n\nWhat is agile?\nAgile is an iterative software development approach that focuses on collaboration and quick, rapid releases. It’s a set of values and principles that can be used to help drive decisions in software development. \nWhen it comes to agile, it’s tough to define it more concisely than the original micro manifesto itself, which was written back in 2001. (No TL;DR version needed. It’s only 68 words.) It states:\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\n\nIndividuals and interactions over processes and tools\nWorking software over comprehensive documentation\nCustomer collaboration over contract negotiation\nResponding to change over following a plan\n\nThat is, while there is value in the items on the right, we value the items on the left more\nThe manifest is paired with 12 agile principles to help make better decisions. \nCopying agile approaches and investing in agile tools doesn’t make your team agile. It’s not just about doing two-week iterations or building out smaller teams. In agile, the what a team does is less important than the why they do it.\nAgile (as the name implies) is about the flexibility and adaptability to build software with ever-changing needs and toss set-in-stone plans out the window.\nThis includes better connecting the dev team with end-users. (You could sort of think of that a bit like the “What would you say you do here?” scene from Office Space.)\nWhat are the benefits of agile?\n\nAgility -  More quickly respond to market changes or customer feedback.\nQuality - A focus on testing and sustained customer involvement means the chances of a product’s overall quality being high are greater.\nCollaboration - Agile is about people. By placing value on human interactions over processes and “that’s just the way it’s done,” organizations are able to let employees act guided by their experience and a shared set of values rather than being micro-managed or shackled to detailed documentation.\nOwnership - The trust required from leadership to have agile teams can create an increased feeling of ownership.\nCustomer satisfaction - With a focus on finding and fixing problems quickly and a direct line between customers and developers, customers are more likely to be satisfied and come back for more.\nPredictability - By doing away with big plans that are set in stone and often detached from reality, organizations can get a better picture of what’s going well and what’s not working right away rather than months down the road when it’s too late to do anything to correct it.\nIncrease productivity - Regularly used planning tools like Kanban and Scrum help teams keep tabs of work and progress toward goals.\n\nHow can a company be agile?\nMany organizations are used to working in a waterfall model. A waterfall model is a linear approach is a sequence of events that starts with a heavy upfront investment of time and resources in scoping out requirements and budgeting before moving into development, testing, and production.\nMoving this paradigm to agile and running agile at scale can be difficult and complex. This isn’t helped by the fact that “agile” is less of a thing you do and more of a basis for decision-making. For organizations guided by process above all else, this can require a major shift and definitely takes some backing from management.\nWhat is DevOps?\nWith a name that sounds like a covert military team and the kind of goofy capitalization you’ve come to expect from tech terminology, DevOps combines software development and IT operations. Take the “dev” from software development and the “ops” from IT operations and you get this portmanteau, which is the fun-to-say term that describes mashing together of multiple words, like spork, phablet, brunch, jorts, and bromance.\nDevOps isn’t just a process. It’s a shift in workplace culture. It’s a collaboration between teams. Doubling down on automation and installing all of the right software won’t get you there alone. Like agile, people are the key component.\nInterested in upscaling or beginning your journey with DevOps? A Cloud Guru’s DevOps Learning Paths offers custom courses fit for beginners and advanced gurus!\nAWS offers the following definition of the DevOps model:\nDevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.\n\n\nAutomating AWS Cost Optimization\nAWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this free, on-demand webinar, you’ll get an overview of AWS cost-optimization tools and strategies.\nWatch Now\n\nWhy combine software development and IT operations?\nHow did these two groups get combined into one? And why would you want to combine development and IT operations? \nWay back when (around 2007 or so), IT operations teams were separate from development teams. Then, people in the development and operations communities realized there was some issues in the way these teams were structured and how they interacted (or didn’t interact).\n\nDevelopment and IT operations teams had different objectives\nDevelopment and operations teams had different performance indicators\nDevelopment and operations teams were siloed physically in different rooms, buildings, or offices\n\nSo they started talking. This gave birth to DevOps and the DevOps engineer. \nPatrick Debois, often called the Godfather of DevOps, brought these groups together at devopsdays in 2009 where they discussed ways to bridge the gap between the two fields. Now, thousands of enterprises have adapted or are working toward adapting these practices.\nThese new approaches basically make ops everyone’s job to a degree, which makes the future of ops jobs very interesting. \nDevOps concepts and key terms\n\nContainer - A lightweight, standalone, executable piece of software. It includes everything needed to run that software.\nContinuous delivery (CD) - The ongoing and manual or automatic release of software to production. It’s aimed around new cycles. Some orgs release a new version with any changes.\nContinuous integration (CI) - The ongoing programming, building, and testing of code. This is done before turning it over to the continuous delivery (CD) system for deployment. With CD, it makes up CI/CD.\nInfrastructure as Code (IaC) - Defining infrastructure you want to use with programming code that can be understood by cloud services. These cloud services then create the infrastructure for you based on this code. This allows you to define standards and reuse code, which saves time.\nMicroservices - Application architecture that is broken into multiple small pieces. Containers are often used to deploy microservices.\nOpen source - Computer software code released under a license for free, like Linux or Kubernetes.\nPipeline - A set of connected processes where the output of one process is the input for the next.\nServerless - Running a service or microservice on cloud-based infrastructure without worrying about the servers running your code. You simply provide the code, and the cloud provider runs the code and gives you the results. See more on the Function as a Service (FaaS) services of AWS, Azure, and GCP.\nSource code repository - A place to upload and track the history of your code, like GitHub or AWS CodeCommit.\nUnit testing - Breaking your application down into small parts to test that each features works.\n\nWhat are the benefits of DevOps?\nDevOps is all about producing higher-quality software faster and saving a lot of time and money. Here’s a more detailed breakdown of the benefits.\n\nSpeed - Release updates and new features faster, adapt to the changing market, and become more efficient.\nRapid delivery - Increase deployment frequency and the pace of releases. Respond to customers’ needs faster and build a competitive advantage. \nReliability - Automatic testing is built-in. Ensures rollouts are of the highest quality and that you have less downtime because you build for stability and test before deploying.\nScale - Implements automation. With the use of cloud and container technology, you can scale usage up and down and save money while you do so.\nCollaboration - Allow teams who used to be apart to work together. Workflows can be combined, inefficiency is reduced, and time is saved.\nSecurity - Allows infrastructure to be created by code, just like software. By doing this (instead of doing it manually), you can define policies to stay compliant no matter how many servers will be deployed.\n\nWant an overview of the benefits with a heavy dash of sarcasm? Check out our post 5 reasons to NOT move to DevOps.\nIs DevOps a replacement for agile?\nDevOps and agile can speed up the delivery of and increase the quality of software. Agile replaced the old-school waterfall model, but DevOps isn’t a replacement for agile.\nDevOps exists because of agile, and the two can coexist and be used together. You don’t have to choose between DevOps and agile. Ideally, your organization will practice both. \nSee the top DevOps skills people are learning at ACG right now\nWhat is CI/CD?\nContinuous integration and continuous development (or CI/CD) is a DevOps tactic — a way to deliver apps to customers with the introduction of automation into the agile development process. \nThe CI/CD pipeline (as it’s called when these practices are combined) has become an integral part of DevOps culture. A variety of tools and techniques are used for implementing such a pipeline. (More on those tools below.)\nThe CI/CD pipeline is supported by teams working in an agile way with either DevOps or a site reliability engineering (SRE) approach. \nRead more about why you need SRE skills to thrive in a serverless world and check out our Site Reliability Engineering Concepts or Google Cloud DevOps and SREs courses.\nWhat are the benefits of CI/CD?\nCI/CD pipelines smooth and speed up the flow of code from development through operations and QA into production by automating manual processes and increasing confidence in your releases. \nThis is the exact opposite of the waterfall release approach still practiced by many large organizations, where developers throw code “over the wall” to ops, devs don’t get access to production, and ops doesn’t have much inside knowledge of how the code works.\nCI/CD allows organizations to:\n\nBuild faster\nDecrease code review time\nAutomatic\nFaster fault isolation\nAdditional deployment features\n\nAs mentioned before, you can be doing the right things and using the right tools and still not be agile or properly implementing DevOps. A broken and messy team can result in broken and messy CI/CD pipelines. You can almost predict how your CI/CD pipeline looks based on how your dev and ops teams work together.\n\n\nGet the Cloud Dictionary of Pain\nSpeaking cloud doesn’t have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this cloud guide for succinct definitions of some of the most painful cloud terms.\nGet the Goods\n\nWhat are some common DevOps tools?\nHere are some of the most popular DevOps CI/CD tools you might use if working in an AWS cloud environment.\n\n\nGit - Free open-source version control system. It stores the entire history of your code that your developers will continue to push new code to It has a tiny footprint and fast performance. It also supports branching, allowing others to work on features separately without having developers trip over code.\n\n\nGitHub - Online service to host Git repositories. GitHub is owned by Microsoft, which offers a similar DevOps tool called Azure DevOps. Whether you are a developer or not, working alone or in a team, Azure DevOps training can help you organize the way you plan, create and deliver software. Get the lowdown on Azure DevOps vs GitHub in this comparison of Microsoft DevOps tools.\n\n\nAWS CodeCommit - Fully managed server for hosting Git repositories. It’s secure and encrypted, highly available, and easily integrated with other AWS services.\n\n\nAWS CodeBuild - Fully managed continuous integration code that complies code, runs tests, and produces software packages. When developers write code and push it into Git, CodeBuild compiles that code and produces a software package. It scales automatically and can process multiple builds concurrently.\n\n\nAWS CodeDeploy - Fully managed deployment service that takes the software package (or files you updated and pushed into your Git repository) and deploys it to AWS or on-premises servers. It integrates well with AWS services and your own servers. It completely automates software deployment, eliminating error-prone manual operations.\n\n\nAWS CodePipeline - Fully management continuous delivery service that helps you completely automate your release pipeline. It can automate the build, test, and deploy phases of software development and it can integrate with CodeCommit and GitHub, giving you the flexibility to use the source-control system of your choice. Read more about automating CI/CD with AWS CodePipeline.\n\n\nOther common DevOps tools include Docker, Jira, Jenkins, Puppet, Chef, Splunk, Ansible, Kubernetes, Bamboo, and Nagios."},"AWS/For-Reading/Amazon-Web-Services-In-Plain-English":{"title":"Amazon Web Services In Plain English","links":[],"tags":[],"content":"I was searching one liner meaning for aws services and I came across some interesting websites:\n\n\n                  \n                  Meaning of AWS Services in one line \n                  \n                \n\n\nadayinthelifeof.nl/2020/05/20/aws.html\nexpeditedsecurity.com/aws-in-plain-english/\n\n\n\n\n\n                  \n                  Those I need \n                  \n                \n\nEC2 - Virtual Private Servers\nLambda - Programming Functions you can run but can costs a fortune\nS3 - File Storage (unmountable)\nEFS - Mountable Network Disks\nRDS - Managed Relational Database\nVPC - Virtual Private Network (consider as VLANs)\nDynamoDB - Large &amp; scalable non-relational database\nCloudFront - Content Delivery Network\nRoute 53 - Managed domains names and records\nCloudWatch - Monitoring and Logs\nAutoscaling - Scale resources dynamically or manually\nCloudTrail - Spying on your users (Tracking Users activity)\nIAM - Users and their Permissions, policies and roles\n\n"},"AWS/For-Reading/Authorized-vs-Authonticated":{"title":"Authorized vs Authonticated","links":[],"tags":[],"content":"Authentication vs. Authorization\nWhile often used interchangeably, authentication and authorization represent fundamentally different functions. In this article, we compare and contrast the two to show how they protect applications in complementary ways.\nWhat are authentication and authorization?\nIn simple terms, authentication is the process of verifying who a user is, while authorization is the process of verifying what they have access to.\nComparing these processes to a real-world example, when you go through security in an airport, you show your ID to authenticate your identity. Then, when you arrive at the gate, you present your boarding pass to the flight attendant, so they can authorize you to board your flight and allow access to the plane.\nAuthentication vs. authorization\nHere’s a quick overview of the differences between authentication and authorization:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthenticationAuthorizationDetermines whether users are who they claim to beDetermines what users can and cannot accessChallenges the user to validate credentials (for example, through passwords, answers to security questions, or facial recognition)Verifies whether access is allowed through policies and rulesUsually done before authorizationUsually done after successful authenticationGenerally, transmits info through an ID TokenGenerally, transmits info through an Access TokenGenerally governed by the OpenID Connect (OIDC) protocolGenerally governed by the OAuth 2.0 frameworkExample: Employees in a company are required to authenticate through the network before accessing their company emailExample: After an employee successfully authenticates, the system determines what information the employees are allowed to access\nIn short, access to a resource is protected by both authentication and authorization. If you can’t prove your identity, you won’t be allowed into a resource. And even if you can prove your identity, if you are not authorized for that resource, you will still be denied access.\nAuth0 has products and services for authentication, like passwordless, multi-factor authentication (MFA), and Single-Sign On (SSO) you can configure using Auth0 Dashboard or Management API. For authorization, Auth0 offers role-based access control (RBAC) or fine grained authorization FGA)."},"AWS/For-Reading/Difference-between-Load-balancers":{"title":"Difference between Load balancers","links":[],"tags":[],"content":"AWS Load Balancer Types\nAWS offers four types of load balancers:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.Load BalancerDescription1Classic Load Balancers (CLB)Support HTTP and HTTPS traffic, route based on path, host, HTTP header, query string, and source IP.2Application Load Balancers (ALB)Support HTTP and HTTPS traffic, route based on path, host, HTTP header, query string, and source IP, with additional features such as support for WebSocket and HTTP/2.3Network Load Balancers (NLB)Support TCP and TLS traffic, route based on static IP, preserve source IP, and offer high throughput.4Gateway Load Balancers (GWLB)Not explicitly mentioned in the search results, but according to AWS documentation, they are a type of load balancer that operates at the network layer (Layer 4) and supports both IPv4 and IPv6 traffic."},"AWS/For-Reading/Git-Fetch-vs-Git-pull":{"title":"Git Fetch vs Git pull","links":[],"tags":[],"content":"Git fetch and git pull basics\nGit fetch and git pull are both Git commands used to retrieve update information from a remote repository. So, how do they differ? Git fetch downloads the changes from the remote repository to the local repository but does not make any changes to the current working directory. Since the changes are not merged into the local branch, you can check the changes from the remote repository without interrupting your current work. On the other hand, git pull retrieves the latest changes from the remote repository like git fetch, but it also automatically merges those changes into the current branch. In contrast to git fetch, git pull directly applies the changes from the remote repository to the local working directory.\nWhat is git fetch?\nThe git fetch command retrieves the latest commit history from the remote repository, but it does not affect the local working directory. Even after fetching remote changes, they are not reflected in the local branch. It is primarily used when you want to retrieve the latest status from the remote repository and review the changes before they are reflected in the local repository. To apply the retrieved changes to the local branch, you need to manually run git merge or git rebase.\nWhat is git pull?\nThe git pull command combines git fetch and git merge (or git rebase) into a single command. This allows you to fetch changes from the remote repository and automatically integrate them into the current local branch.\nWhile git fetch retrieves changes from the remote repository without applying them to the local branch, running git pull automatically integrates the changes from the remote repository into the local branch.\nGit pull is suitable for quickly reflecting remote changes in the local branch, but it can lead to conflicts, so caution is needed, especially when working with multiple people.\nWhen to use git fetch\nGit fetch is a command used to retrieve the latest information from a remote repository. The retrieved information is not directly reflected in the local branch. Using git pull will reflect all remote branches, including incorrect or problematic ones, in the local branch.\nWhen changes are made simultaneously on both remote and local branches, or when there are new users on the team, it is safer to use git fetch to retrieve the remote branch contents first and then perform merge or rebase.\nWhen to use git pull\nGit pull is a command that performs more processes compared to git fetch. Git pull can perform both git fetch and additionally execute git merge or git rebase. For this reason, git pull is recommended when you want to quickly reflect changes from the remote repository in the local branch.\nGit fetch and git pull FAQs\nWhat is the difference between git pull and git fetch?\nGit pull is a command that performs git fetch followed by git merge or git rebase. While git fetch does not affect the local repository, git pull automatically synchronizes changes from the remote repository with the local repository.\nWhat precautions should be taken when using git pull?\nWhen executing git pull, there may be conflicts between remote and local changes. Merge conflicts are particularly likely to occur, so if conflicts arise, they need to be resolved manually. Additionally, using git pull —rebase allows you to incorporate the latest changes while performing a rebase.\nWhat is git fetch used for?\nGit fetch is useful for checking and retrieving the latest status of the remote repository. However, the changes retrieved are not automatically reflected in the local branch; git fetch is used to synchronize the local and remote repositories."},"AWS/For-Reading/Host-Tomcat-Vanshit's-Version":{"title":"Host Tomcat Vanshit's Version","links":["tags/git"],"tags":["git"],"content":"…Deploying Application…\n1.Create an instance by using ec2-service and create database by RDS service.\n2.Create custom security group by adding SSH, custom tcp 8080 , for maria db add a port custom tcp 3306.\n3.Add this Security group to EC2 instance and Database instance.\n4.Login to instance.\n5.Search tomcat 9 on google and Download its zip file.\n6.Use curl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip.\n7.unzip it\n8. Install java-17 package:-\nyum install java-17 -y\n9.Change directory :- cd apache-tomcat-9.0.97/bin\n10.bash catalina.sh start\n11.After this steps search with your instance public ip (For ex, 192.25.26.244:8080)\n12.Install git :- yum install git -y\n13.Run command :- git clone github.com/Pritam-Khergade/student-ui\n14. 2 folders will be downloaded in bin dir.\n15.cd student-ui\n16.Install maven :- yum install maven -y\n17.mvn clean package.\n18.After cleaning package u should see build success notification.\n19.cd target\n20.Rename file :- mv studentapp-2.2-SNAPSHOT.war studentapp.war\n21.Move this folder in tomcat/webapps dir :-\nmv studentapp.war /root/apache-tomcat-9.0.97/webapps/\n22.Search with public ip u should see a registeration form :- 192.25.26.244:8080/studentapp\n…Database part…\n1.Install maria db in instance :-  yum install mariadb105 -y\n2.Connect to database :- MySQL -h van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com(endpoint) -u  -p \n3.After this u will get into database.\n4.Create DATABASE with name studentapp and tables,\nCREATE DATABASE studentapp;\nuse studentapp;\nCREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,\n\tstudent_name VARCHAR(100) NOT NULL,\n    student_addr VARCHAR(100) NOT NULL,\n\tstudent_age VARCHAR(3) NOT NULL,\n\tstudent_qual VARCHAR(20) NOT NULL,\n\tstudent_percent VARCHAR(10) NOT NULL,\n\tstudent_year_passed VARCHAR(10) NOT NULL,\n\tPRIMARY KEY (student_id)\n);\n5.After this exit from database.\n6.vim /apache-tomcat-9.0.97/conf/context.xml\n7.Write this configuration :-\n&lt;context&gt;\n&lt;Resource name=&quot;jdbc/TestDB&quot; auth=&quot;Container&quot; type=&quot;javax.sql.DataSource&quot;\n           maxTotal=&quot;500&quot; maxIdle=&quot;30&quot; maxWaitMillis=&quot;1000&quot;\n           username=&quot;admin&quot; password=&quot;Remote#123&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;\n           url=&quot;jdbc:mysql://van-db.cduscwa84wb4.ap-south-1.rds.amazonaws.com:3306/studentapp\n&lt;context&gt;\n8.save and exit.\n9.After this download mysqlconnector.jar file from this link:-\ns3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar\n10.cd apache-tomcat-9.0.97/lib/\n11.Download this file in lib folder :- curl -O s3-us-west-2.amazonaws.com/studentapi-cit/mysqlconnector.jar\n12.After this the data u entered in your form should be stored in this database."},"AWS/For-Reading/How-DNS-Works":{"title":"How DNS Works","links":[],"tags":[],"content":"What is DNS?\nThe Domain Name System (DNS) is the phonebook of the Internet. Humans access information online through domain names, like nytimes.com or espn.com. Web browsers interact through Internet Protocol (IP) addresses. DNS translates domain names to IP addresses so browsers can load Internet resources.\nEach device connected to the Internet has a unique IP address which other machines use to find the device. DNS servers eliminate the need for humans to memorize IP addresses such as 192.168.1.1 (in IPv4), or more complex newer alphanumeric IP addresses such as 2400:cb00:2048:1::c629:d7a2 (in IPv6).\n\nHow does DNS work?\nThe process of DNS resolution involves converting a hostname (such as www.example.com) into a computer-friendly IP address (such as 192.168.1.1). An IP address is given to each device on the Internet, and that address is necessary to find the appropriate Internet device - like a street address is used to find a particular home. When a user wants to load a webpage, a translation must occur between what a user types into their web browser (example.com) and the machine-friendly address necessary to locate the example.com webpage.\nIn order to understand the process behind the DNS resolution, it’s important to learn about the different hardware components a DNS query must pass between. For the web browser, the DNS lookup occurs “behind the scenes” and requires no interaction from the user’s computer apart from the initial request.\nReport\n2023 GigaOm Radar for DNS Security\nGet the report\nReport\nRead the Q4 2023 DDoS Threat Landscape Report\nGet the report\nThere are 4 DNS servers involved in loading a webpage:\n\nDNS recursor - The recursor can be thought of as a librarian who is asked to go find a particular book somewhere in a library. The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client’s DNS query.\nRoot nameserver - The root server is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations.\nTLD nameserver - The top level domain server (TLD) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is “com”).\nAuthoritative nameserver - This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.\n\nFast &amp; Secure DNS\nFree DNS included with any Cloudflare plan\nStart for free\nWhat’s the difference between an authoritative DNS server and a recursive DNS resolver?\nBoth concepts refer to servers (groups of servers) that are integral to the DNS infrastructure, but each performs a different role and lives in different locations inside the pipeline of a DNS query. One way to think about the difference is the recursive resolver is at the beginning of the DNS query and the authoritative nameserver is at the end.\nRecursive DNS resolver\nThe recursive resolver is the computer that responds to a recursive request from a client and takes the time to track down the DNS record. It does this by making a series of requests until it reaches the authoritative DNS nameserver for the requested record (or times out or returns an error if no record is found). Luckily, recursive DNS resolvers do not always need to make multiple requests in order to track down the records needed to respond to a client; caching is a data persistence process that helps short-circuit the necessary requests by serving the requested resource record earlier in the DNS lookup.\n\nAuthoritative DNS server\nPut simply, an authoritative DNS server is a server that actually holds, and is responsible for, DNS resource records. This is the server at the bottom of the DNS lookup chain that will respond with the queried resource record, ultimately allowing the web browser making the request to reach the IP address needed to access a website or other web resources. An authoritative nameserver can satisfy queries from its own data without needing to query another source, as it is the final source of truth for certain DNS records.\n\nIt’s worth mentioning that in instances where the query is for a subdomain such as foo.example.com or blog.cloudflare.com, an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s CNAME record.\n\nThere is a key difference between many DNS services and the one that Cloudflare provides. Different DNS recursive resolvers such as Google DNS, OpenDNS, and providers like Comcast all maintain data center installations of DNS recursive resolvers. These resolvers allow for quick and easy queries through optimized clusters of DNS-optimized computer systems, but they are fundamentally different than the nameservers hosted by Cloudflare.\nCloudflare maintains infrastructure-level nameservers that are integral to the functioning of the Internet. One key example is the f-root server network which Cloudflare is partially responsible for hosting. The F-root is one of the root level DNS nameserver infrastructure components responsible for the billions of Internet requests per day. Our Anycast network puts us in a unique position to handle large volumes of DNS traffic without service interruption.\nWhat are the steps in a DNS lookup?\nFor most situations, DNS is concerned with a domain name being translated into the appropriate IP address. To learn how this process works, it helps to follow the path of a DNS lookup as it travels from a web browser, through the DNS lookup process, and back again. Let’s take a look at the steps.\nNote: Often DNS lookup information will be cached either locally inside the querying computer or remotely in the DNS infrastructure. There are typically 8 steps in a DNS lookup. When DNS information is cached, steps are skipped from the DNS lookup process which makes it quicker. The example below outlines all 8 steps when nothing is cached.\nThe 8 steps in a DNS lookup:\n\nA user types ‘example.com’ into a web browser and the query travels into the Internet and is received by a DNS recursive resolver.\nThe resolver then queries a DNS root nameserver (.).\nThe root server then responds to the resolver with the address of a Top Level Domain (TLD) DNS server (such as .com or .net), which stores the information for its domains. When searching for example.com, our request is pointed toward the .com TLD.\nThe resolver then makes a request to the .com TLD.\nThe TLD server then responds with the IP address of the domain’s nameserver, example.com.\nLastly, the recursive resolver sends a query to the domain’s nameserver.\nThe IP address for example.com is then returned to the resolver from the nameserver.\nThe DNS resolver then responds to the web browser with the IP address of the domain requested initially.\n\nOnce the 8 steps of the DNS lookup have returned the IP address for example.com, the browser is able to make the request for the web page:\n\nThe browser makes a HTTP request to the IP address.\nThe server at that IP returns the webpage to be rendered in the browser (step 10).\n\n\nWhat is a DNS resolver?\nThe DNS resolver is the first stop in the DNS lookup, and it is responsible for dealing with the client that made the initial request. The resolver starts the sequence of queries that ultimately leads to a URL being translated into the necessary IP address.\nNote: A typical uncached DNS lookup will involve both recursive and iterative queries.\nIt’s important to differentiate between a recursive DNS query and a recursive DNS resolver. The query refers to the request made to a DNS resolver requiring the resolution of the query. A DNS recursive resolver is the computer that accepts a recursive query and processes the response by making the necessary requests.\n\nWhat are the types of DNS queries?\nIn a typical DNS lookup three types of queries occur. By using a combination of these queries, an optimized process for DNS resolution can result in a reduction of distance traveled. In an ideal situation cached record data will be available, allowing a DNS name server to return a non-recursive query.\n3 types of DNS queries:\n\nRecursive query - In a recursive query, a DNS client requires that a DNS server (typically a DNS recursive resolver) will respond to the client with either the requested resource record or an error message if the resolver can’t find the record.\nIterative query - in this situation the DNS client will allow a DNS server to return the best answer it can. If the queried DNS server does not have a match for the query name, it will return a referral to a DNS server authoritative for a lower level of the domain namespace. The DNS client will then make a query to the referral address. This process continues with additional DNS servers down the query chain until either an error or timeout occurs.\nNon-recursive query - typically this will occur when a DNS resolver client queries a DNS server for a record that it has access to either because it’s authoritative for the record or the record exists inside of its cache. Typically, a DNS server will cache DNS records to prevent additional bandwidth consumption and load on upstream servers.\n\nWhat is DNS caching? Where does DNS caching occur?\nThe purpose of caching is to temporarily stored data in a location that results in improvements in performance and reliability for data requests. DNS caching involves storing data closer to the requesting client so that the DNS query can be resolved earlier and additional queries further down the DNS lookup chain can be avoided, thereby improving load times and reducing bandwidth/CPU consumption. DNS data can be cached in a variety of locations, each of which will store DNS records for a set amount of time determined by a time-to-live (TTL).\nBrowser DNS caching\nModern web browsers are designed by default to cache DNS records for a set amount of time. The purpose here is obvious; the closer the DNS caching occurs to the web browser, the fewer processing steps must be taken in order to check the cache and make the correct requests to an IP address. When a request is made for a DNS record, the browser cache is the first location checked for the requested record.\nIn Chrome, you can see the status of your DNS cache by going to chrome://net-internals/#dns.\nOperating system (OS) level DNS caching\nThe operating system level DNS resolver is the second and last local stop before a DNS query leaves your machine. The process inside your operating system that is designed to handle this query is commonly called a “stub resolver” or DNS client. When a stub resolver gets a request from an application, it first checks its own cache to see if it has the record. If it does not, it then sends a DNS query (with a recursive flag set), outside the local network to a DNS recursive resolver inside the Internet service provider (ISP).\nWhen the recursive resolver inside the ISP receives a DNS query, like all previous steps, it will also check to see if the requested host-to-IP-address translation is already stored inside its local persistence layer.\nThe recursive resolver also has additional functionality depending on the types of records it has in its cache:\n\nIf the resolver does not have the A records, but does have the NS records for the authoritative nameservers, it will query those name servers directly, bypassing several steps in the DNS query. This shortcut prevents lookups from the root and .com nameservers (in our search for example.com) and helps the resolution of the DNS query occur more quickly.\nIf the resolver does not have the NS records, it will send a query to the TLD servers (.com in our case), skipping the root server.\nIn the unlikely event that the resolver does not have records pointing to the TLD servers, it will then query the root servers. This event typically occurs after a DNS cache has been purged.\n\nLearn about what differentiates Cloudflare DNS from other DNS providers."},"AWS/For-Reading/IAM-Policies":{"title":"IAM Policies","links":[],"tags":[],"content":"Difference between inline policy and managed policy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureManaged PolicyInline PolicyDefinitionPre-defined policies created and managed by AWSCustom policies created and managed by the userScopeCan be attached to multiple users, groups, or rolesAttached to a single user, group, or roleManagementManaged by AWS, updates are automatically appliedManaged by the user, requires manual updatesReusabilityHighly reusable across multiple identitiesLess reusable, specific to a single identityBest PracticeUse for common permissions and centralized managementUse for specific, unique permissions that need to be isolated to a single identity\nIAM Policy Types\nBased on the provided search results, IAM (Identity and Access Management) in AWS offers four types of policies:\n\nIdentity-Based Policies: Attached to an identity (user, group, or role), these policies define the permissions for that specific identity. They are used to control access to AWS resources and services.\nResource-Based Policies: Attached to a specific resource (e.g., S3 bucket, EC2 instance), these policies define the permissions for that resource. They specify which identities have access to the resource and under what conditions.\nService Control Policies (SCPs): These policies control the actions that can be performed by IAM users and roles across AWS services. They are used to enforce organizational compliance and governance.\nPermissions Boundary Policies: These policies define the maximum possible permissions that an IAM role can have. They are used to restrict the permissions granted to roles created by CI/CD pipelines or other automated processes.\n\nHere’s a summary of the policies and their uses:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy TypeAttached ToPurposeIdentity-BasedIdentity (user, group, role)Define permissions for a specific identityResource-BasedResource (e.g., S3 bucket, EC2 instance)Define permissions for a specific resourceService Control Policies (SCPs)N/AEnforce organizational compliance and governance across AWS servicesPermissions Boundary PoliciesIAM roleRestrict permissions granted to roles created by automated processes\nNote that Managed Policies and Customer Managed Policies are not separate policy types, but rather categories of policies. Managed Policies are created and managed by AWS, while Customer Managed Policies are created and managed by users in their AWS accounts. Inline Policies are embedded within an identity or resource, but are not a distinct policy type.\nFor more information, refer to the AWS documentation and the provided search results.\nTypes of IAM Users\nBased on the provided search results, AWS Identity and Access Management (IAM) supports the following types of users:\n\nIAM User: An IAM user is an entity you create in AWS. This user is an identity within your AWS account that’s granted specific custom permissions. IAM users have a name and password used to sign in to the AWS Management Console.\nRoot User: The root user is the initial sign-in identity for an AWS account. It has complete access to all AWS services and resources in the account. However, AWS strongly recommends not using the root user for everyday tasks and instead creating IAM users with limited permissions.\nIAM Identity Center User: IAM Identity Center users are members of AWS Organizations and can be ==granted access to multiple AWS accounts== and applications through the AWS access portal. They can use their corporate credentials to sign in if their company has integrated Active Directory or another identity provider with IAM Identity Center.\nFederated Identity: With web identity federation, you can receive an authentication token and exchange it for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Federated identities sign in using an external identity provider.\nAWS Builder ID User: AWS Builder ID users are a type of user that represents an individual and ==allows access to AWS services and tools without an AWS account==. They sign in to specific AWS services or tools using their AWS Builder ID profile.\n\nNote that these types of users are not mutually exclusive, and you may use a combination of them in your AWS environment. For example, you might create IAM users for your workforce and use federated identities for external partners or customers."},"AWS/For-Reading/Load-Balancer":{"title":"Load Balancer","links":[],"tags":[],"content":"Last Updated : 04 Oct, 2024\nWhen a website becomes extremely popular, the traffic on that website increases, and the load on a single server also increases. The concurrent traffic overloads the single server and the website becomes slower for the users. To meet the request of these high volumes of data and to return the correct response in a fast and reliable manner, we need to scale the server. This can be done by adding more servers to the network and distributing all the requests across these servers. \n\nImportant Topics for the Load Balancer – System Design Interview Question\n\nWhat is a Load Balancer?\nWhat will happen if there is No Load Balancer?\nHow Load Balancer Works?\nWhere Are Load Balancers Typically Placed?\nTypes of Load Balancers\nLoad Balancing Algorithms\nHow to Use Load Balancing During System Design Interviews?\n\n1. What is a Load Balancer?\nA load balancer is a networking device or software application that distributes and balances the incoming traffic among the servers to provide high availability, efficient utilization of servers, and high performance.\nThis article outlines various strategies for implementing load balancing and its importance in high-availability systems. To build a solid foundation in system design, the System Design Course will help you understand the underlying principles and techniques for designing robust and scalable systems.\n\nLoad balancers are highly used in cloud computing domains, data centers, and large-scale web applications where traffic flow needs to be managed. \nThe primary goal of using a load balancer is, not to overburden with huge incoming traffic which may lead to server crashes or high latency.\n\n2. What will happen if there is No Load Balancer?\nBefore understanding how a load balancer works, let’s understand what problem will occur without the load balancer through an example.\n\nConsider a scenario where an application is running on a single server and the client connects to that server directly without load balancing.\n \n\n\nThere are two main problems with this model:\n\nSingle Point of Failure: \n\nIf the server goes down or something happens to the server the whole application will be interrupted and it will become unavailable for the users for a certain period. It will create a bad experience for users which is unacceptable for service providers.\n\n\nOverloaded Servers: \n\nThere will be a limitation on the number of requests that a web server can handle. If the business grows and the number of requests increases the server will be overloaded. \nTo solve the increasing number of requests we need to add a few more servers and we need to distribute the requests to the cluster of servers. \n\n\n\n3. How Load Balancer Works?\nLets understand how Load Balancer works through the above discussed example:\nTo solve the above issue and to distribute the number of requests we can add a load balancer in front of the web servers and allow our services to handle any number of requests by adding any number of web servers in the network. \n\nWe can spread the request across multiple servers. \nFor some reason, if one of the servers goes offline the service will be continued.\nAlso, the latency on each request will go down because each server is not bottlenecked on RAM/Disk/CPU anymore.\n\n\nLoad balancers minimize server response time and maximize throughput. Load balancer ensures high availability and reliability by sending requests only to online servers Load balancers do continuous health checks to monitor the server’s capability of handling the request. Depending on the number of requests or demand load balancers add or remove the number of servers.\n4. Where Are Load Balancers Typically Placed?\nBelow is the image where a load balancer can be placed…\n\n\nIn between the client application/user and the server\nIn between the server and the application/job servers\nIn between the application servers and the cache servers\nIn between the cache servers the database servers\n\n5. Types of Load Balancers\n5.1. Types of Load Balancer – Based on Configurations\nThere are mainly three typers of load balancers based on configurations:\n1. Software Load Balancers\nSoftware load balancers are applications or components that run on general-purpose servers. They are implemented in software, making them flexible and adaptable to various environments.\n2. Hardware Load Balancers\nAs the name suggests we use a physical appliance to distribute the traffic across the cluster of network servers. These load balancers are also known as Layer 4-7 Routers and these are capable of handling all kinds of HTTP, HTTPS, TCP, and UDP traffic.\n\nThese load balancers are expensive to acquire and configure, which is the reason a lot of service providers use them only as the first entry point for user requests.\nLater the internal software load balancers are used to redirect the data behind the infrastructure wall. \n\n3. Virtual Load Balancers\nA virtual load balancer is a type of load balancing solution implemented as a virtual machine (VM) or software instance within a virtualized environment ,such as data centers utilizing virtualization technologies like VMware, Hyper-V, or KVM.. It plays a crucial role in distributing incoming network traffic across multiple servers or resources to ensure efficient utilization of resources, improve response times, and prevent server overload.\n5.2. Types of Load Balancer – Based on Functions\nThere are mainly three typers of load balancers based on functions:\n1. Layer 4 (L4) Load Balancer\nLayer-4 load balancers operate at the transport layer of the OSI model. They make forwarding decisions based on information available in network layer protocols (such as IP addresses and port numbers). \n2. Layer 7 (L7) Load Balancer\nLayer-7 load balancers operate at the application layer of the OSI model. They can make load balancing decisions based on content, including information such as URLs, HTTP headers, or cookies.  \n3. Global Server Load Balancing (GSLB)\nGSLB stands for Global Server Load Balancer. This type of load balancer goes beyond the traditional local load balancing and is designed for distributing traffic across multiple data centers or geographically distributed servers. It takes into account factors such as server proximity, server health, and geographic location to intelligently distribute traffic across multiple locations.\nFurther Read: Layer-4 vs. Layer-7 vs. GSLB\n6. Load Balancing Algorithms\nWe need a load-balancing algorithm to decide which request should be redirected to which backend server. The different system uses different ways to select the servers from the load balancer. Companies use varieties of load-balancing algorithm techniques depending on the configuration. Some of the common load-balancing algorithms are given below:\n1. Round Robin\nThe Round Robin algorithm is a simple static load balancing approach in which requests are distributed across the servers in a sequential or rotational manner. It is easy to implement but it doesn’t consider the load already on a server so there is a risk that one of the servers receives a lot of requests and becomes overloaded.\n2. Weighted Round Robin\nThe Weighted Round Robin algorithm is also a static load balancing approach which is much similar to the round-robin technique. The only difference is, that each of the resources in a list is provided a weighted score. Depending on the weighted score the request is distributed to these servers. \n3. Source IP Hash\nThe Source IP Hash cLoad Balancing Algorithm is a static method used in network load balancing to distribute incoming requests among a set of servers based on the hash value of the source IP address. This algorithm aims to ensure that requests originating from the same source IP address are consistently directed to the same server.\n4. Least Connection Method\nThe Least Connections algorithm is a dynamic load balancing approach that assigns new requests to the server with the fewest active connections. The idea is to distribute incoming workloads in a way that minimizes the current load on each server, aiming for a balanced distribution of connections across all available resources. \n5. Least Response Time Method\nThe Least Response method is a dynamic load balancing approach that aims to minimize response times by directing new requests to the server with the quickest response time. \n7. How to Use Load Balancing During System Design Interviews?\nIn your system design interview, you’ll be asked some sort of scalability question where you’ll have to explain how load balancers help distribute the traffic and how it ensures scalability and availability of services in your application. The overall concept that you need to keep in mind from this article is…\n\nA load balancer enables elastic scalability which improves the performance and throughput of data. It allows you to keep many copies of data (redundancy) to ensure the availability of the system. In case a server goes down or fails you’ll have the backup to restore the services. \nLoad balancers can be placed at any software layer.\nMany companies use both hardware and software to implement load balancers, depending on the different scale points in their system.\n"},"AWS/For-Reading/Managed-policies-and-inline-policies":{"title":"Managed policies and inline policies","links":[],"tags":[],"content":"Managed policies and inline policies\nWhen you set the permissions for an identity in IAM, you must decide whether to use an AWS managed policy, a customer managed policy, or an inline policy. The following topics provide more information about each of the types of identity-based policies and when to use them.\nTopics\n\nAWS managed policies\nCustomer managed policies\nInline policies\nChoose between managed policies and inline policies\nConvert an inline policy to a managed policy\nDeprecated AWS managed policies\n\nAWS managed policies\nAn AWS managed policy is a standalone policy that is created and administered by AWS. A standalone policy means that the policy has its own Amazon Resource Name (ARN) that includes the policy name. For example, arn:aws:iam::aws:policy/IAMReadOnlyAccess is an AWS managed policy. For more information about ARNs, see IAM ARNs. For a list of AWS managed policies for AWS services, see AWS managed policies.\nAWS managed policies make it convenient for you to assign appropriate permissions to users, IAM groups, and roles. It is faster than writing the policies yourself, and includes permissions for many common use cases.\nYou cannot change the permissions defined in AWS managed policies. AWS occasionally updates the permissions defined in an AWS managed policy. When AWS does this, the update affects all principal entities (IAM users, IAM groups, and IAM roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API calls become available for existing services. For example, the AWS managed policy called ReadOnlyAccess provides read-only access to all AWS services and resources. When AWS launches a new service, AWS updates the ReadOnlyAccess policy to add read-only permissions for the new service. The updated permissions are applied to all principal entities that the policy is attached to.\nFull access AWS managed policies: These define permissions for service administrators by granting full access to a service. Examples include:\n\n\nAmazonDynamoDBFullAccess\n\n\nIAMFullAccess\n\n\nPower-user AWS managed policies: These provide full access to AWS services and resources, but do not allow managing users and IAM groups. Examples include:\n\n\nAWSCodeCommitPowerUser\n\n\nAWSKeyManagementServicePowerUser\n\n\nPartial-access AWS managed policies: These provide specific levels of access to AWS services without allowing permissions management access level permissions. Examples include:\n\n\nAmazonMobileAnalyticsWriteOnlyAccess\n\n\nAmazonEC2ReadOnlyAccess\n\n\nJob function AWS managed policies: These policies align closely with commonly used job functions in the IT industry and facilitate granting permissions for these job functions. One key advantage of using job function policies is that they are maintained and updated by AWS as new services and API operations are introduced. For example, the AdministratorAccess job function provides full access and permissions delegation to every service and resource in AWS. We recommend that you use this policy only for the account administrator. For power users that require full access to every service except limited access to IAM and Organizations, use the PowerUserAccess job function. For a list and descriptions of the job function policies, see AWS managed policies for job functions.\nThe following diagram illustrates AWS managed policies. The diagram shows three AWS managed policies: AdministratorAccess, PowerUserAccess, and AWSCloudTrail_ReadOnlyAccess. Notice that a single AWS managed policy can be attached to principal entities in different AWS accounts, and to different principal entities in a single AWS account.\n\nCustomer managed policies\nYou can create standalone policies in your own AWS account that you can attach to principal entities (IAM users, IAM groups, and IAM roles). You create these customer managed policies for your specific use cases, and you can change and update them as often as you like. Like AWS managed policies, when you attach a policy to a principal entity, you give the entity the permissions that are defined in the policy. When you update permissions in the policy, the changes are applied to all principal entities that the policy is attached to.\nA great way to create a customer managed policy is to start by copying an existing AWS managed policy. That way you know that the policy is correct at the beginning and all you need to do is customize it to your environment.\nThe following diagram illustrates customer managed policies. Each policy is an entity in IAM with its own Amazon Resource Name (ARN) that includes the policy name. Notice that the same policy can be attached to multiple principal entities—for example, the same DynamoDB-books-app policy is attached to two different IAM roles.\nFor more information, see Define custom IAM permissions with customer managed policies\n\nInline policies\nAn inline policy is a policy created for a single IAM identity (a user, user group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity. You can create a policy and embed it in an identity, either when you create the identity or later. If a policy could apply to more than one entity, it’s better to use a managed policy.\nThe following diagram illustrates inline policies. Each policy is an inherent part of the user, group, or role. Notice that two roles include the same policy (the DynamoDB-books-app policy), but they are not sharing a single policy. Each role has its own copy of the policy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureManaged PolicyInline PolicyDefinitionPre-defined policies created and managed by AWSCustom policies created and managed by the userScopeCan be attached to multiple users, groups, or rolesAttached to a single user, group, or roleManagementManaged by AWS, updates are automatically appliedManaged by the user, requires manual updatesReusabilityHighly reusable across multiple identitiesLess reusable, specific to a single identityBest PracticeUse for common permissions and centralized managementUse for specific, unique permissions that need to be isolated to a single identity"},"AWS/For-Reading/Network-Protocols":{"title":"Network Protocols","links":[],"tags":[],"content":"Types of Network Protocols and Their Uses\nLast Updated : 22 May, 2024\nNetwork protocols are a set of rules that are responsible for the communication of data between various devices in the network. These protocols define guidelines and conventions for transmitting and receiving data, ensuring efficient and reliable data communication.\nWhat is Network Protocol?\nA network protocol is a set of rules that govern data communication between different devices in the network. It determines what is being communicated, how it is being communicated, and when it is being communicated. It permits connected devices to communicate with each other, irrespective of internal and structural differences.\nHow do Network Protocols Work?\nIt is essential to understand how devices communicate over a network by recognizing network protocols. The Open Systems Interconnection (OSI), the most widely used model, illustrates how computer systems interact with one another over a network. The communication mechanism between two network devices is shown by seven different layers in the OSI model. Every layer in the OSI model works based on different network protocols. At every layer, one or more protocols are there for network communication. To enable network-to-network connections, the Internet Protocol (IP), for instance, routes data by controlling information like the source and destination addresses of data packets. It is known as a network layer protocol.\nTypes of Network Protocols\nIn most cases, communication across a network like the Internet uses the OSI model. The OSI model has a total of seven layers. Secured connections, network management, and network communication are the three main tasks that the network protocol performs. The purpose of protocols is to link different devices.\nThe protocols can be broadly classified into three major categories:\n\nNetwork Communication\nNetwork Management\nNetwork Security\n\n1. Network Communication\nCommunication protocols are really important for the functioning of a network. They are so crucial that it is not possible to have computer networks without them. These protocols formally set out the rules and formats through which data is transferred. These protocols handle syntax, semantics, error detection, synchronization, and authentication. Below mentioned are some network communication protocol:\nHypertext Transfer Protocol(HTTP)\nIt is a layer 7 protocol that is designed for transferring a hypertext between two or more systems. HTTP works on a client-server model, most of the data sharing over the web is done through using HTTP.\nTransmission Control Protocol(TCP)\nTCP layouts a reliable stream delivery by using sequenced acknowledgment. It is a connection-oriented protocol i.e., it establishes a connection between applications before sending any data. It is used for communicating over a network. It has many applications such as emails, FTP, streaming media, etc.\nUser Datagram Protocol(UDP)\nIt is a connectionless protocol that lay-out a basic but unreliable message service. It adds no flow control, reliability, or error-recovery functions. UPD is functional in cases where reliability is not required. It is used when we want faster transmission, for multicasting and broadcasting connections, etc.\nBorder Gateway Protocol(BGP)\nBGP is a routing protocol that controls how packets pass through the router in an independent system one or more networks run by a single organization and connect to different networks. It connects the endpoints of a LAN with other LANs and it also connects endpoints in different LANs to one another.\nAddress Resolution Protocol(ARP)\nARP is a protocol that helps in mapping logical addresses to the physical addresses acknowledged in a local network. For mapping and maintaining a correlation between these logical and physical addresses a table known as ARP cache is used.\nInternet Protocol(IP)\nIt is a protocol through which data is sent from one host to another over the internet. It is used for addressing and routing data packets so that they can reach their destination.\nDynamic Host Configuration Protocol(DHCP)\nit’s a protocol for network management and it’s used for the method of automating the process of configuring devices on IP networks. A DHCP server automatically assigns an IP address and various other configurational changes to devices on a network so they can communicate with other IP networks. it also allows devices to use various services such as NTP, DNS, or any other protocol based on TCP or UDP.\n2. Network Management \nThese protocols assist in describing the procedures and policies that are used in monitoring, maintaining, and managing the computer network. These protocols also help in communicating these requirements across the network to ensure stable communication. Network management protocols can also be used for troubleshooting connections between a host and a client.\nInternet Control Message Protocol(ICMP)\nIt is a layer 3 protocol that is used by network devices to forward operational information and error messages. ICMP is used for reporting congestions, network errors, diagnostic purposes, and timeouts.\nSimple Network Management Protocol(SNMP)\nIt is a layer 7 protocol that is used for managing nodes on an IP network. There are three main components in the SNMP protocol i.e., SNMP agent, SNMP manager, and managed device. SNMP agent has the local knowledge of management details, it translates those details into a form that is compatible with the SNMP manager. The manager presents data acquired from SNMP agents, thus helping in monitoring network glitches, and network performance, and troubleshooting them.\nGopher\nIt is a type of file retrieval protocol that provides downloadable files with some description for easy management, retrieving, and searching of files. All the files are arranged on a remote computer in a stratified manner. Gopher is an old protocol and it is not much used nowadays.\nFile Transfer Protocol(FTP)\nFTP is a Client/server protocol that is used for moving files to or from a host computer, it allows users to download files, programs, web pages, and other things that are available on other services.\nPost Office Protocol****(POP3)****\nIt is a protocol that a local mail client uses to get email messages from a remote email server over a TCP/IP connection. Email servers hosted by ISPs also use the POP3 protocol to hold and receive emails intended for their users. Eventually, these users will use email client software to look at their mailbox on the remote server and to download their emails. After the email client downloads the emails, they are generally deleted from the servers.\nTelnet\nIt is a protocol that allows the user to connect to a remote computer program and to use it i.e., it is designed for remote connectivity. Telnet creates a connection between a host machine and a remote endpoint to enable a remote session.\n3. Network Security\nThese protocols secure the data in passage over a network. These protocols also determine how the network secures data from any unauthorized attempts to extract or review data. These protocols make sure that no unauthorized devices, users, or services can access the network data. Primarily, these protocols depend on encryption to secure data.\nSecure Socket Layer(SSL)\nIt is a network security protocol mainly used for protecting sensitive data and securing internet connections. SSL allows both server-to-server and client-to-server communication. All the data transferred through SSL is encrypted thus stopping any unauthorized person from accessing it.\nHypertext Transfer Protocol(HTTPS)\nIt is the secured version of HTTP. this protocol ensures secure communication between two computers where one sends the request through the browser and the other fetches the data from the web server.\nTransport Layer Security(TLS)\nIt is a security protocol designed for data security and privacy over the internet, its functionality is encryption, checking the integrity of data i.e., whether it has been tampered with or not, and authentication. It is generally used for encrypted communication between servers and web apps, like a web browser loading a website, it can also be used for encryption of messages, emails, and VoIP.\nSome Other Protocols\nInternet Message Access Protocol (IMAP)\n\nICMP protocol is used to retrieve message from the mail server. By using ICMP mail user can view and manage mails on his system.\n\nSession Initiation Protocol (SIP)\n\nSIP is used in video, voice, and messaging application. This protocol is used to initiating, Managing, Terminating the session between two users while they are communicating.\n\nReal-Time Transport Protocol (RTP)\n\nThis protocol is used to forward audio, video over IP network. This protocol is used with SIP protocol to send audio, video at real-time.\n\nRout Access Protocol (RAP)\n\nRAP is used in network management. It helps to user for accessing the nearest router for communication. RAP is less efficient as compared to SNMP.\n\nPoint To Point Tunnelling Protocol (PPTP)\n\nIt is used to implement VPN ( Virtual Private Network ). PPTP protocol append PPP frame in IP datagram for transmission through IP based network.\n\nTrivial File Transfer Protocol (TFTP)\n\nTFTP is the simplified version of FTP. TFTP is also used to transfer file over internet\n\nResource Location Protocol (RLP)\n\nRLP is used to assign the resource such as server, printer, or other devices over the internet to the user. It is used to locate the resource to the client for broadcast query.\n\nFrequently Asked Question on Network Protocols – FAQs\nWhat is the need of network protocols?\n\nNetwork protocol is a set of rules that shows how data is transferred between various devices connected to the same network.\n\nWhich protocol suite is used when accessing the Internet?\n\nThe protocol used while accessing the internet are TCP and UDP.\n\nWhat is meant by IP multicasting?\n\nIP Multicasting is defined as the types of group communication in which data is sent simultaneously to multiple computers.\n\nWhat are some important protocols of transport layer?\n\nImportant protocols of transport layer include-\n\nTransmission Control Protocol (TCP).\nUser Datagram Protocol (UDP).\nStream Control Transmission Protocol (SCTP).\n\n\nWhat are important protocols of Application layer?\n\nSome important protocols of Application Layer include-\n\nHyper Text Transfer Protocol (HTTP).\nFile transfer Protocol (FTP).\nSimple Mail Transfer protocol (SMTP).\nDomain Name System (DNS).\n\n\nWhat is the full form of DHCP?\n\nFull form of DHCP is Dynamic Host Configuration Protocol.\n\nWhat is the function of DHCP?\n\nFunction of DHCP is to assign IP address to device on a network automatically.\n\nWhat is Virtual Local Area Network in networking?\n\nA virtual local area network (VLAN) is a virtualized link that unites various network nodes and devices from several LANs into a single logical network.\n"},"AWS/For-Reading/OSI-Model":{"title":"OSI Model","links":[],"tags":[],"content":"What is OSI Model? – Layers of OSI Model\nLast Updated : 11 Nov, 2024\nThe OSI (Open Systems Interconnection) Model is a set of rules that explains how different computer systems communicate over a network. OSI Model was developed by the International Organization for Standardization (ISO). The OSI Model consists of 7 layers and each layer has specific functions and responsibilities.\nThis layered approach makes it easier for different devices and technologies to work together. OSI Model provides a clear structure for data transmission and managing network issues. The OSI Model is widely used as a reference to understand how network systems function.\nIn this article, we will discuss the OSI Model and each layer of the OSI Model in detail. We will also discuss the flow of data in the OSI Model and how the OSI Model is different from the TCP/IP Model.\n\nOSI Model\nFor those preparing for competitive exams like GATE, a strong understanding of networking concepts, including the OSI model, is crucial. To deepen your knowledge in this area and other key computer science topics, consider enrolling in the GATE CS Self-Paced course . This course offers comprehensive coverage of the syllabus, helping you build a solid foundation for your exam preparation.\nLayers of the OSI Model\nThere are 7 layers in the OSI Model and each layer has its specific role in handling data. All the layers are mentioned below:\n\nPhysical Layer\nData Link Layer\nNetwork Layer\nTransport Layer\nSession Layer\nPresentation Layer\nApplication Layer\n\nLayer 1 – Physical Layer\nThe lowest layer of the OSI reference model is the Physical Layer. It is responsible for the actual physical connection between the devices. The physical layer contains information in the form of bits. Physical Layer is responsible for transmitting individual bits from one node to the next. When receiving data, this layer will get the signal received and convert it into 0s and 1s and send them to the Data Link layer, which will put the frame back together. Common physical layer devices are Hub, Repeater, Modem, and Cables.\n\nPhysical Layer\nFunctions of the Physical Layer\n\nBit Synchronization: The physical layer provides the synchronization of the bits by providing a clock. This clock controls both sender and receiver thus providing synchronization at the bit level.\nBit Rate Control: The Physical layer also defines the transmission rate i.e. the number of bits sent per second.\nPhysical Topologies: Physical layer specifies how the different, devices/nodes are arranged in a network i.e. bus topology, star topology, or mesh topology.\nTransmission Mode: Physical layer also defines how the data flows between the two connected devices. The various transmission modes possible are Simplex, half-duplex and full-duplex.\n\nLayer 2 – Data Link Layer (DLL)\nThe data link layer is responsible for the node-to-node delivery of the message. The main function of this layer is to make sure data transfer is error-free from one node to another, over the physical layer. When a packet arrives in a network, it is the responsibility of the DLL to transmit it to the Host using its MAC address. Packet in the Data Link layer is referred to as Frame. Switches and Bridges are common Data Link Layer devices.\nThe Data Link Layer is divided into two sublayers:\n\nLogical Link Control (LLC)\nMedia Access Control (MAC)\n\nThe packet received from the Network layer is further divided into frames depending on the frame size of the NIC(Network Interface Card). DLL also encapsulates Sender and Receiver’s MAC address in the header.\nThe Receiver’s MAC address is obtained by placing an ARP(Address Resolution Protocol) request onto the wire asking “Who has that IP address?” and the destination host will reply with its MAC address.\nFunctions of the Data Link Layer\n\nFraming: Framing is a function of the data link layer. It provides a way for a sender to transmit a set of bits that are meaningful to the receiver. This can be accomplished by attaching special bit patterns to the beginning and end of the frame.\nPhysical Addressing: After creating frames, the Data link layer adds physical addresses (MAC addresses) of the sender and/or receiver in the header of each frame.\nError Control: The data link layer provides the mechanism of error control in which it detects and retransmits damaged or lost frames.\nFlow Control: The data rate must be constant on both sides else the data may get corrupted thus, flow control coordinates the amount of data that can be sent before receiving an acknowledgment.\nAccess Control: When a single communication channel is shared by multiple devices, the MAC sub-layer of the data link layer helps to determine which device has control over the channel at a given time.\n\nLayer 3 – Network Layer\nThe network layer works for the transmission of data from one host to the other located in different networks. It also takes care of packet routing i.e. selection of the shortest path to transmit the packet, from the number of routes available. The sender and receiver’s IP address are placed in the header by the network layer. Segment in the Network layer is referred to as Packet. Network layer is implemented by networking devices such as routers and switches.\nFunctions of the Network Layer\n\nRouting: The network layer protocols determine which route is suitable from source to destination. This function of the network layer is known as routing.\nLogical Addressing: To identify each device inter-network uniquely, the network layer defines an addressing scheme. The sender and receiver’s IP addresses are placed in the header by the network layer. Such an address distinguishes each device uniquely and universally.\n\nLayer 4 – Transport Layer\nThe transport layer provides services to the application layer and takes services from the network layer. The data in the transport layer is referred to as Segments. It is responsible for the end-to-end delivery of the complete message. The transport layer also provides the acknowledgment of the successful data transmission and re-transmits the data if an error is found. Protocols used in Transport Layer are TCP, UDP  NetBIOS, PPTP.\nAt the sender’s side, the transport layer receives the formatted data from the upper layers, performs Segmentation, and also implements Flow and error control to ensure proper data transmission. It also adds Source and Destination port number in its header and forwards the segmented data to the Network Layer.\n\nGenerally, this destination port number is configured, either by default or manually. For example, when a web application requests a web server, it typically uses port number 80, because this is the default port assigned to web applications. Many applications have default ports assigned.\n\nAt the Receiver’s side, Transport Layer reads the port number from its header and forwards the Data which it has received to the respective application. It also performs sequencing and reassembling of the segmented data.\nFunctions of the Transport Layer\n\nSegmentation and Reassembly: This layer accepts the message from the (session) layer, and breaks the message into smaller units. Each of the segments produced has a header associated with it. The transport layer at the destination station reassembles the message.\nService Point Addressing: To deliver the message to the correct process, the transport layer header includes a type of address called service point address or port address. Thus by specifying this address, the transport layer makes sure that the message is delivered to the correct process.\n\nServices Provided by Transport Layer\n\nConnection-Oriented Service\nConnectionless Service\n\nLayer 5 – Session Layer\nSession Layer in the OSI Model is responsible for the establishment of connections, management of connections, terminations of sessions between two devices. It also provides authentication and security. Protocols used in the Session Layer are NetBIOS, PPTP.\nFunctions of the Session Layer\n\nSession Establishment, Maintenance, and Termination: The layer allows the two processes to establish, use, and terminate a connection.\nSynchronization: This layer allows a process to add checkpoints that are considered synchronization points in the data. These synchronization points help to identify the error so that the data is re-synchronized properly, and ends of the messages are not cut prematurely and data loss is avoided.\nDialog Controller: The session layer allows two systems to start communication with each other in half-duplex or full-duplex.\n\nExample\nLet us consider a scenario where a user wants to send a message through some Messenger application running in their browser. The “Messenger” here acts as the application layer which provides the user with an interface to create the data. This message or so-called Data is compressed, optionally encrypted (if the data is sensitive), and converted into bits (0’s and 1’s) so that it can be transmitted.\n\nCommunication in Session Layer\nLayer 6 – Presentation Layer\nThe presentation layer is also called the Translation layer. The data from the application layer is extracted here and manipulated as per the required format to transmit over the network. Protocols used in the Presentation Layer are JPEG, MPEG, GIF, TLS/SSL, etc.\nFunctions of the Presentation Layer\n\nTranslation: For example, ASCII to EBCDIC.\nEncryption/ Decryption: Data encryption translates the data into another form or code. The encrypted data is known as the ciphertext and the decrypted data is known as plain text. A key value is used for encrypting as well as decrypting data.\nCompression: Reduces the number of bits that need to be transmitted on the network.\n\nLayer 7 – Application Layer\nAt the very top of the OSI Reference Model stack of layers, we find the Application layer which is implemented by the network applications. These applications produce the data to be transferred over the network. This layer also serves as a window for the application services to access the network and for displaying the received information to the user. Protocols used in the Application layer are SMTP, FTP, DNS, etc.\n\nApplication Layer\nFunctions of the Application Layer\nThe main functions of the application layer are given below.\n\nNetwork Virtual Terminal(NVT): It allows a user to log on to a remote host.\nFile Transfer Access and Management(FTAM): This application allows a user to access files in a remote host, retrieve files in a remote host, and manage or control files from a remote computer.\nMail Services: Provide email service.\nDirectory Services: This application provides distributed database sources and access for global information about various objects and services.\n\nHow Data Flows in the OSI Model****?****\nWhen we transfer information from one device to another, it travels through 7 layers of OSI model. First data travels down through 7 layers from the sender’s end and then climbs back 7 layers on the receiver’s end.\nData flows through the OSI model in a step-by-step process:\n\nApplication Layer: Applications create the data.\nPresentation Layer: Data is formatted and encrypted.\nSession Layer: Connections are established and managed.\nTransport Layer: Data is broken into segments for reliable delivery.\nNetwork Layer: Segments are packaged into packets and routed.\nData Link Layer: Packets are framed and sent to the next device.\nPhysical Layer: Frames are converted into bits and transmitted physically.\n\nEach layer adds specific information to ensure the data reaches its destination correctly, and these steps are reversed upon arrival.\n\nWe can understand how data flows through OSI Model with the help of an example mentioned below.\nLet us suppose, Person A sends an e-mail to his friend Person B.\nStep 1: Person A interacts with e-mail application like Gmail, outlook, etc. Writes his email to send. (This happens at Application Layer).\nStep 2: At Presentation Layer, Mail application prepares for data transmission like encrypting data and formatting it for transmission.\nStep 3: At Session Layer, There is a connection established between the sender and receiver on the internet.\nStep 4: At Transport Layer, Email data is broken into smaller segments. It adds sequence number and error-checking information to maintain the reliability of the information.\nStep 5: At Network Layer, Addressing of packets is done in order to find the best route for transfer.\nStep 6: At Data Link Layer, data packets are encapsulated into frames, then MAC address is added for local devices and then it checks for error using error detection.\nStep 7: At Physical Layer, Frames are transmitted in the form of electrical/ optical signals over a physical network medium like ethernet cable or WiFi.\nAfter the email reaches the receiver i.e. Person B, the process will reverse and decrypt the e-mail content. At last, the email will be shown on Person B email client.\nProtocols Used in the OSI Layers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayerWorkingProtocol Data UnitProtocols1 – Physical LayerEstablishing Physical Connections between Devices.BitsUSB, SONET/SDH, etc.2 – Data Link LayerNode to Node Delivery of Message.FramesEthernet, PPP, etc.3 – Network LayerTransmission of data from one host to another, located in different networks.PacketsIP, ICMP, IGMP, OSPF, etc.4 – Transport LayerTake Service from Network Layer and provide it to the Application Layer.Segments (for TCP) or Datagrams (for UDP)TCP, UDP, SCTP, etc.5 – Session LayerEstablishes Connection, Maintenance, Ensures Authentication and Ensures security.DataNetBIOS, RPC, PPTP, etc.6 – Presentation LayerData from the application layer is extracted and manipulated in the required format for transmission.DataTLS/SSL, MIME, JPEG, PNG, ASCII, etc.7 – Application LayerHelps in identifying the client and synchronizing communication.DataFTP, SMTP, DNS, DHCP, etc.\nWhy Does The OSI Model Matter?\nThe OSI Model matters because it provides the user a clear structure of “how the data moves in the network?”. As the OSI Model consists of 7 layers, each layer has its specific role, and due to which it helps in understanding, identifying and solving the complex network problems easily by focusing on one of the layers not the entire network.\nAs the modern Internet does not prefer the OSI Model, but still, the OSI Model is still very helpful for solving network problems. It helps people understanding network concepts very easily.\nDifference Between OSI and TCP/IP Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOSI ModelTCP/IP ModelOSI stands for Open Systems Interconnection.TCP/IP stands for Transmission Control Protocol/Internet Protocol.OSI model has 7 layers.TCP/IP model consists of 4 layers.Package delivery is guaranteed in OSI Model.Package delivery is not guaranteed in the TCP/IP Model.In the OSI model, Only layers 1,2 and 3 are necessary for data transmission.All layers of the TCP/IP model are needed for data transmission.Protocols at each layer is independent of the other layer.Layers are integrated, some layers are required by other layers of TCP/IP model.OSI Model is a conceptual framework, less used in practical applications.Widely used in actual networks like Internet and Communication Systems.\n\nOSI vs TCP/IP\nAdvantages of OSI Model\nThe OSI Model defines the communication of a computing system into 7 different layers. Its advantages include:\n\nIt divides network communication into 7 layers which makes it easier to understand and troubleshoot.\nIt standardizes network communications, as each layer has fixed functions and protocols.\nDiagnosing network problems is easier with the OSI model.\nIt is easier to improve with advancements as each layer can get updates separately.\n\nDisadvantages of OSI Model\n\nThe OSI Model has seven layers, which can be complicated and hard to understand for beginners.\nIn real-life networking, most systems use a simpler model called the Internet protocol suite (TCP/IP), so the OSI Model is not always directly applicable.\nEach layer in the OSI Model adds its own set of rules and operations, which can make the process more time-consuming and less efficient.\nThe OSI Model is more of a theoretical framework, meaning it’s great for understanding concepts but not always practical for implementation.\n\nConclusion\nIn conclusion, the OSI (Open Systems Interconnection) model helps us understand how data moves in networks. It consists of seven distinct layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. Each layer has specific responsibilities and interacts with the layers directly above and below it. Since it is a conceptual model, but the OSI framework is still widely used to troubleshoot and understand networking issues.\nFrequently Asked Questions on OSI Model – FAQs\nCan OSI layers work independently?\n\nNo, OSI layers do not work independently. Each layer depends on the services provided by the layer below it and, in turn, provides services to the layer above it. This layered approach ensures that data is transmitted smoothly from the source to the destination.\n\nHow does the OSI Model help in troubleshooting network issues?\n\nBy breaking down communication into layers, the OSI Model helps network administrators isolate problems more easily.\n\nWhat happens if a layer in the OSI Model fails?\n\nIf a particular OSI layer fails, data transmission may be disrupted or fail entirely. Network administrator will check layer by layer to identify and resolve the issue, make sure that each layer is functioning correctly or not.\n\nHow does DNS fit into the OSI Model?\n\nThe Domain Name System (DNS) operates at Layer 7 (Application Layer). It translates domain names into IP addresses, facilitating communication between users and services across the network.\n"},"AWS/For-Reading/SDLC---Software-Developement-LifeCycle":{"title":"SDLC - Software Developement LifeCycle","links":[],"tags":[],"content":"Software Development Life Cycle (SDLC)\nLast Updated : 20 Nov, 2024\nSoftware development life cycle (SDLC) is a structured process that is used to design, develop, and test good-quality software. SDLC, or software development life cycle, is a methodology that defines the entire procedure of software development step-by-step. The goal of the SDLC life cycle model is to deliver high-quality, maintainable software that meets the user’s requirements. SDLC in software engineering models outlines the plan for each stage so that each stage of the software development model can perform its task efficiently to deliver the software at a low cost within a given time frame that meets users requirements. In this article we will see Software Development Life Cycle (SDLC) in detail.\nWhat is the Software Development Life Cycle (SDLC)?\nSDLC is a process followed for software building within a software organization. SDLC consists of a precise plan that describes how to develop, maintain, replace, and enhance specific software. The life cycle defines a method for improving the quality of software and the all-around development process.  \n\nStages of the Software Development Life Cycle\nSDLC specifies the task(s) to be performed at various stages by a software engineer or developer. It ensures that the end product is able to meet the customer’s expectations and fits within the overall budget. Hence, it’s vital for a software developer to have prior knowledge of this software development process. SDLC is a collection of these six stages, and the stages of SDLC are as follows:\n\nSoftware Development Life Cycle Model SDLC Stages\nThe SDLC Model involves six phases or stages while developing any software.\nStage-1: Planning and Requirement Analysis\nPlanning is a crucial step in everything, just as in software development. In this same stage, requirement analysis is also performed by the developers of the organization. This is attained from customer inputs, and sales department/market surveys. \nThe information from this analysis forms the building blocks of a basic project. The quality of the project is a result of planning. Thus, in this stage, the basic project is designed with all the available information.\n\nStage-1 : Planning and Requirement Analysis\nStage-2: Defining Requirements\nIn this stage, all the requirements for the target software are specified. These requirements get approval from customers, market analysts, and stakeholders. \nThis is fulfilled by utilizing SRS (Software Requirement Specification). This is a sort of document that specifies all those things that need to be defined and created during the entire project cycle. \n\nStage-2 : Defining Requirements\nStage-3: Designing Architecture\nSRS is a reference for software designers to come up with the best architecture for the software. Hence, with the requirements defined in SRS, multiple designs for the product architecture are present in the Design Document Specification (DDS). \nThis DDS is assessed by market analysts and stakeholders. After evaluating all the possible factors, the most practical and logical design is chosen for development.\n\nStage 3: Design\nStage-4: Developing Product\nAt this stage, the fundamental development of the product starts. For this, developers use a specific programming code as per the design in the DDS. Hence, it is important for the coders to follow the protocols set by the association. Conventional programming tools like compilers, interpreters, debuggers, etc. are also put into use at this stage. Some popular languages like C/C++, Python, Java, etc. are put into use as per the software regulations. \n\nStage 4: Development\nStage-5: Product Testing and Integration\nAfter the development of the product, testing of the software is necessary to ensure its smooth execution. Although, minimal testing is conducted at every stage of SDLC. Therefore, at this stage, all the probable flaws are tracked, fixed, and retested. This ensures that the product confronts the quality requirements of SRS. \nDocumentation, Training, and Support: Software documentation is an essential part of the software development life cycle. A well-written document acts as a tool and means to information repository necessary to know about software processes, functions, and maintenance. Documentation also provides information about how to use the product. Training in an attempt to improve the current or future employee performance by increasing an employee’s ability to work through learning, usually by changing his attitude and developing his skills and understanding. \n\nStage 5: Testing\nStage-6: Deployment and Maintenance of Products\nAfter detailed testing, the conclusive product is released in phases as per the organization’s strategy. Then it is tested in a real industrial environment. It is important to ensure its smooth performance. If it performs well, the organization sends out the product as a whole. After retrieving beneficial feedback, the company releases it as it is or with auxiliary improvements to make it further helpful for the customers. However, this alone is not enough. Therefore, along with the deployment, the product’s supervision. \n\nStage 6: Deployment and Maintenance\nSoftware Development Life Cycle Models\nTo this day, we have more than 50 recognized SDLC models in use. But None of them is perfect, and each brings its favorable aspects and disadvantages for a specific software development project or a team.\nHere, we have listed the top five most popular SDLC models:\n1. Waterfall Model\nIt is the fundamental model of the software development life cycle. This is a very simple model. The waterfall model is not in practice anymore, but it is the basis for all other SDLC models. Because of its simple structure, the waterfall model is easier to use and provides a tangible output. In the waterfall model, once a phase seems to be completed, it cannot be changed, and due to this less flexible nature, the waterfall model is not in practice anymore. \n2. Agile Model\nThe agile model in SDLC was mainly designed to adapt to changing requests quickly. The main goal of the Agile model is to facilitate quick project completion. The agile model refers to a group of development processes. These processes have some similar characteristics but also possess certain subtle differences among themselves.\n3. Iterative Model\nIn the Iterative model in SDLC, each cycle results in a semi-developed but deployable version; with each cycle, some requirements are added to the software, and the final cycle results in the software with the complete requirement specification. \n4. Spiral Model\nThe spiral model in SDLC is one of the most crucial SDLC models that provides support for risk handling. It has various spirals in its diagrammatic representation; the number of spirals depends upon the type of project. Each loop in the spiral structure indicates the Phases of the Spiral model.  \n5. V-Shaped Model\nThe V-shaped model in SDLC is executed in a sequential manner in V-shape. Each stage or phase of this model is integrated with a testing phase. After every development phase, a testing phase is associated with it, and the next phase will start once the previous phase is completed, i.e., development &amp; testing. It is also known as the verification or validation model. \n6. Big Bang Model\nThe Big Bang model in SDLC is a term used to describe an informal and unstructured approach to software development, where there is no specific planning, documentation, or well-defined phases.\nWhat is the need for SDLC?\nSDLC is a method, approach, or process that is followed by a software development organization while developing any software. SDLC models were introduced to follow a disciplined and systematic method while designing software. With the software development life cycle, the process of software design is divided into small parts, which makes the problem more understandable and easier to solve. SDLC comprises a detailed description or step-by-step plan for designing, developing, testing, and maintaining the software.\n\nFollow the project Library Management System or E Portfolio Website to see the use of Software Development Life Cycle in a Software Projects.\n\nHow does SDLC Address Security?\nA frequent issue in software development is the delay of security-related tasks until the testing phase, which occurs late in the software development life cycle (SDLC) and occurs after the majority of crucial design and implementation has been finished. During the testing phase, security checks may be minimal and restricted to scanning and penetration testing, which may fail to identify more complicated security flaws.\nSecurity issue can be address in SDLC by following DevOps. Security is integrated throughout the whole SDLC, from build to production, through the use of DevSecOps. Everyone involved in the DevOps value chain have responsibility for security under DevSecOps.\nReal Life Example of SDLC\nDeveloping a banking application using SDLC:\n\nPlanning and Analysis: During this stage, business stakeholders’ requirements about the functionality and features of banking application will be gathered by program managers and business analysts. Detailed SRS (Software Requirement Specification) documentation will be produced by them. Together with business stakeholders, business analysts will analyse and approve the SRS document.\nDesign: Developers will receive SRS documentation. Developers will read over the documentation and comprehend the specifications. Web pages will be designed by designers. High level system architecture will be prepared by developers.\nDevelopment: During this stage, development will code. They will create the web pages and APIs needed to put the feature into practice.\nTesting: Comprehensive functional testing will be carried out. They will guarantee that the banking platform is glitch-free and operating properly.\nDeployment and Maintenance: The code will be made available to customers and deployed. Following this deployment, the customer can access the online banking. The same methodology will be used to create any additional features.\n\nHow to Choose an SDLC Model?\nChoosing the right SDLC (Software Development Life Cycle) model is essential for project success. Here are the key factors to consider:\n\nProject Requirements:\n\nClear Requirements: Use Waterfall or V-Model if requirements are well-defined and unlikely to change.\nChanging Requirements: Use Agile or Iterative models if requirements are unclear or likely to evolve.\n\n\nProject Size and Complexity:\n\nSmall Projects: Use Waterfall or RAD for small, simple projects.\nLarge Projects: Use Agile, Spiral, or DevOps for large, complex projects that need flexibility.\n\n\nTeam Expertise:\n\nExperienced Teams: Use Agile or Scrum if the team is familiar with iterative development.\nLess Experienced Teams: Use Waterfall or V-Model for teams needing structured guidance.\n\n\nClient Involvement:\n\nFrequent Client Feedback: Use Agile, Scrum, or RAD if regular client interaction is needed.\nMinimal Client Involvement: Use Waterfall or V-Model if client involvement is low after initial planning.\n\n\nTime and Budget Constraints:\n\nFixed Time and Budget: Use Waterfall or V-Model if you have strict time and budget limits.\nFlexible Time and Budget: Use Agile or Spiral if you can adjust time and budget as needed.\n\n\nRisk Management:\n\nHigh-Risk Projects: Use Spiral for projects with significant risks and uncertainties.\nLow-Risk Projects: Use Waterfall for projects with minimal risks.\n\n\nProduct Release Timeline:\n\nQuick Release Needed: Use Agile or RAD to deliver products quickly.\nLonger Development Time: Use Waterfall or V-Model for projects with no urgent deadlines.\n\n\nMaintenance and Support:\n\nLong-Term Maintenance: Use Agile or DevOps for projects needing continuous updates and support.\nMinimal Maintenance: Use Waterfall or V-Model if little future maintenance is expected.\n\n\nStakeholder Expectations:\n\nHigh Stakeholder Engagement: Use Agile or Scrum if stakeholders want ongoing involvement.\nLow Stakeholder Engagement: Use Waterfall or V-Model if stakeholders prefer involvement only at major milestones.\n\n\n\nNote:\n\n\nWaterfall: Best for clear, stable projects with minimal changes.\nV-Model: Good for projects with clear requirements and a strong focus on testing.\nAgile/Scrum: Ideal for projects with changing requirements and frequent client interaction.\nSpiral: Suitable for high-risk projects with evolving requirements.\nRAD: Useful for projects needing rapid development.\nDevOps: Best for continuous integration and ongoing support\n\n\nConclusion\nIn conclusion, we now know that the Software Development Life Cycle (SDLC) in software engineering is an important framework for the better and more structured development of optimized software programs. In a world full of rapid evolution in technology, SDLC phases plays a crucial role in enabling some good and innovative solutions for helping users and organizations. Also, it’s better to adapt SDLC principles to achieve software development goals effectively.\nImportant Questions on Software Development Life Cycle (SDLC)\n1. Which of the following is not a life cycle model?\n(A) Spiral model\n(B) Prototyping model\n(C) Waterfall model\n(D) Capability maturity model\n\nSolution: The correct Answer is (D).\n\n2. What is the appropriate pairing of items in the two columns listing various activities encountered in a software life cycle?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nP. Requirements Capture2. Domain AnalysisQ. Design3. Structural and Behavioral ModelingR. Implementation1. Module Development and IntegrationS. Maintenance4. Performance Tuning\n(A) P-3, Q-2, R-4, S-1\n(B) P-2, Q-3, R-1, S-4\n(C) P-3, Q-2, R-1, S-4\n(D) P-2, Q-3, R-4, S-1\n\nSolution: The correct Answer is (B).\n\nFrequently Asked Questions related to Software Development Life Cycle (SDLC)\nHow does SDLC work?\n\nThe SDLC involves planning the project, gathering requirements, designing the system, coding the software, testing it for issues, deploying it to users, and maintaining it post-release. Each phase ensures the software meets user needs and functions correctly, from start to finish.\n\nWhat are the main phases of SDLC?\n\nThe main phases of SDLC include Requirements, Design, Implementation (Coding), Testing, Deployment, and Maintenance. These phases represent the stages a software project goes through from initiation to completion.\n\nWhy is SDLC important?\n\nSDLC ensures a structured and organized approach to software development, leading to the creation of reliable and high-quality software. It helps manage resources efficiently, reduces development time, and minimizes the risk of project failure.\n\nWhat are the key objectives of SDLC?\n\nThe key objectives of SDLC include delivering a high-quality product, meeting customer requirements, managing project resources effectively, minimizing risks, and providing a clear and transparent development process.\n"},"AWS/For-Reading/Simple-Definitions-To-give-in-Interview":{"title":"Simple Definitions To give in Interview","links":[],"tags":[],"content":"What is Linux?\nLinux is Unix like operating system kernel.\nLinux is a powerful, flexible, and open-source operating system that’s widely used in server environments.\nWhat is kernel?\nkernel is the core component of an operating system. It acts as bridge between software and hardware."},"AWS/For-Reading/Some-talks-about-git-in-reddit":{"title":"Some talks about git in reddit","links":[],"tags":[],"content":"What is git\n[reddit user]\n\ngit is a program that is used to track changes in code. It’s very useful and allows you to “go back in time” if something is broken and see what’s changed which might have caused it to break. It’s also very handy when several people work on the same code together, they can share their changes easily.\nGitHub is a website, now owned by Microsoft, that you can use with git. There are other websites like it, e.g. GitLab. All of them allow you to put your changes on their servers to make it easier to share them with other people. They may also have other features like more advanced permissions so you can only share your code with specific people, or build platforms where you can automatically build your program after you’ve pushed new code to them.\n\n[reddit user]\n\nJust in case that is still a bit dense for you or your current computer knowledge a simpler version is:\nLinus Torvalds, the man who Linux is named after (Linus + Unix = Linux) made a manager for managing the revisions of computer programming he was doing making the Linux Kernel. Every different thing he tried to do to solve for how to make something one way, or another or the third way was saved. if he had to go back and change from the third way back to the second or first as all saved unlike usual computer coding which wouldn’t save that.\nMaking fun of himself that he needed this helpful feature he called it “git” after the British Slang for someone being a dummy. Now some people prefer to think of it as “Go-back In Time” instead of thinking of the creator of the Linux Kernel as ‘dumb’.\nNow Git was so useful, and like Linux, Open Source and Free, and the more it was developed by the community the more powerful, and to some people, seen as a required tool for doing programming projects.\nWhat was shared versions of work stored locally, called versioning, would be stored in folders of single coders work, or multiple coders putting their proposed version(s) for a piece of code, or multiple people sharing and collaborating on the same code. The folders for these were called repositories. Later the finished/published pieces of code would be published to a folder which was also called a repository. When you install software on Linux or just using package managers in general, they largely use collections of published repositories either by pointing at all those final version repositories or by collecting copies of them on a central server called itself a repository, as in a repository of repositories.\nNow this is where GitHub and its siblings/cousins come in. They can act as an online collection of not just the published version of the repositories, but a central place for people to collaborate with Git online, store the work away from the person’s local computer alone, preventing loss of data, and even act as a place users can come to learn about and install and get help with the published program/code.\nIn short you use Git to help you develop code and track all the versions of your work over time.\nGit is so ubiquitous that the format it finishes off the code in can be used to distribute to code and update it later.\nGitHub (and GitLab, SourceForge, etc.) are places where code project can be worked on and collaborated with that also distribute and help creators support users that is done with Git and Git related software.\nRepositories is just a Git related Jargon for the folders that hold the different versions of a Git Project, but when you are talking Package Managers or Code Sites like GitHub, they are talking about the stored published versions of the code that people can download, install, and use.\nTo you the end user just think of repositories as “Folders of Code” be it programs or extensions or mods or even collections of settings and that it is just jargon.\n"},"AWS/For-Reading/Storage-Classes-in-S3":{"title":"Storage Classes in S3","links":[],"tags":[],"content":"Amazon S3 Storage Classes\nOverview\nAmazon S3 offers a range of storage classes that you can choose from based on the performance, data access, resiliency, and cost requirements of your workloads. S3 storage classes are purpose-built to provide the lowest cost storage for different access patterns. S3 storage classes are ideal for virtually any use case, including those with demanding performance needs, data lakes, data residency requirements, unknown or changing access patterns, or archival storage.\nThe S3 storage classes include S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns, S3 Standard for frequently accessed data, S3 Express One Zone for your most frequently accessed data, S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-Infrequent Access (S3 One Zone-IA) for less frequently accessed data, S3 Glacier Instant Retrieval for archive data that needs immediate access, S3 Glacier Flexible Retrieval (formerly S3 Glacier) for rarely accessed long-term data that does not require immediate access, and Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud.\nAmazon S3 provides the most durable storage in the cloud. Based on its unique architecture, S3 is designed to exceed 99.999999999% (11 nines) data durability. Additionally, S3 stores data redundantly across a minimum of 3 Availability Zones by default, providing built-in resilience against widespread disaster. Customers can store data in a single AZ to minimize storage cost or latency, in multiple AZs for resilience against the permanent loss of an entire data center, or in multiple AWS Regions to meet geographic resilience requirements. If you have data residency requirements that can’t be met by an existing AWS Region, you can use S3 storage classes for AWS Dedicated Local Zones or S3 on Outposts racks to store your data in a specific data perimeter. \nYou can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. Amazon S3 also offers capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application. S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone, and do not support S3 Lifecycle transitions.\nView the Amazon S3 storage classes overview infographic.\nGeneral purpose\nAmazon S3 Standard (S3 Standard)\nS3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. \nKey features:\n\nGeneral purpose storage for frequently accessed data\nLow latency and high throughput performance\nDesigned to deliver 99.99% availability with an availability SLA of 99.9%\n\nUnknown or changing access\nAmazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)\nAmazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. S3 Intelligent-Tiering delivers milliseconds latency and high throughput performance for frequently, infrequently, and rarely accessed data in the Frequent, Infrequent, and Archive Instant Access tiers. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.\nFor a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier that is optimized for frequent access, a 40% lower-cost tier that is optimized for infrequent access, and a 68% lower-cost tier optimized for rarely accessed data. S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier. For data that does not require immediate retrieval, you can set up S3 Intelligent-Tiering to monitor and automatically move objects that aren’t accessed for 180 days or more to the Deep Archive Access tier to realize up to 95% in storage cost savings.\nThere are no retrieval charges in S3 Intelligent-Tiering. If an object in the Infrequent or Archive Instant Access tier is accessed later, it’s automatically moved back to the Frequent Access tier. If the object you’re retrieving is stored in the optional Deep Archive tiers, before you can retrieve the object, you must first restore a copy using RestoreObject.  For information about restoring archived objects, see Restoring Archived Objects. No additional tiering charges apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class.\nKey features:\n\nAutomatic cost savings for data with unknown or changing access patterns\nFrequent, Infrequent, and Archive Instant Access tiers have the same low-latency and high-throughput performance of S3 Standard\nThe Infrequent Access tier saves up to 40% on storage costs\nThe Archive Instant Access tier saves up to 68% on storage costs\nOpt-in asynchronous archive capabilities for objects that become rarely accessed\nDeep Archive Access tier has the same performance as Glacier Deep Archive and saves up to 95% for rarely accessed objects\nDesigned to deliver 99.9% availability with an availability SLA of 99%\nSmall monthly monitoring and automation charge\nNo operational overhead, no lifecycle charges, no retrieval charges, and no minimum storage duration\nObjects smaller than 128KB can be stored in S3 Intelligent-Tiering but will always be charged at the Frequent Access tier rates, and are not charged the monitoring and automation charge.\n\nHigh performance\nAmazon S3 Express One Zone\nAmazon S3 Express One Zone is a high-performance, single-Availability Zone storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications. S3 Express One Zone can improve data access speeds by 10x and reduce request costs by 50% compared to S3 Standard. While you have always been able to choose a specific AWS Region to store your S3 data, with S3 Express One Zone you can select a specific AWS Availability Zone within an AWS Region to store your data. You can choose to co-locate your storage and compute resources in the same Availability Zone to further optimize performance, which helps lower compute costs and run workloads faster. With S3 Express One Zone, data is stored in a different bucket type—an Amazon S3 directory bucket—which supports hundreds of thousands of requests per second. Additionally, you can use S3 Express One Zone with services such as Amazon SageMaker Model Training, Amazon Athena, Amazon EMR, and AWS Glue Data Catalog to accelerate your ML and analytics workloads. With S3 Express One Zone, storage automatically scales up or down based on your consumption and need, and you no longer need to manage multiple storage systems for low-latency workloads.\nKey features:\n\nHigh performance storage for your most frequently accessed data\nConsistent single-digit millisecond request latency\nImprove access speeds by 10x and reduce request costs by 50% compared to S3 Standard\nSelect an AWS Availability Zone and have the option to co-locate storage and compute resources for even lower latency, with reduced processing time and more efficient use of compute resources contributing to lower overall total cost of ownership\nAccelerate analytics and ML workloads with AWS service integrations\nScale to handle millions of requests per minute\nOptimized for large datasets with many small objects\nUse existing Amazon S3 APIs with different bucket type – directory buckets\nDesigned to deliver 99.95% availability with an availability SLA of 99.9%\n\nInfrequent access\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA)\nS3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. You can configure S3 storage classes at the object level, and a single bucket can contain objects stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.\nKey features:\n\nInfrequently accessed data that needs millisecond access\nSame low latency and high throughput performance of S3 Standard\nDesigned to deliver 99.9% availability with an availability SLA of 99%\n\nAmazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\nS3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. It’s a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 Cross-Region Replication.\nS3 One Zone-IA offers the same high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge. Using similar engineering designs as S3 Regional storage classes, S3 One Zone-IA also offers 11 nines of durability, but may be susceptible to data loss in the unlikely case of the loss or damage to all or part of an AWS Availability Zone. You can configure S3 storage classes at the object level, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.\nKey features:\n\nRe-creatable infrequently accessed data\nSame low latency and high throughput performance of S3 Standard\nDesigned to deliver 99.5% availability with an availability SLA of 99%\n\nArchive\nOverview\nThe Amazon S3 Glacier storage classes are purpose-built for data archiving, and are designed to provide you with the highest performance, the most retrieval flexibility, and the lowest cost archive storage in the cloud. You can choose from three archive storage classes optimized for different access patterns and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5—12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval from 12—48 hours.\nAmazon S3 Glacier Instant Retrieval\nAmazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. With S3 Glacier Instant Retrieval, you can save up to 68% on storage costs compared to using the S3 Standard-Infrequent Access (S3 Standard-IA) storage class, when your data is accessed once per quarter. S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives. You can upload objects directly to S3 Glacier Instant Retrieval, or use S3 Lifecycle policies to transfer data from the S3 storage classes. For more information, visit the Amazon S3 Glacier Instant Retrieval page »\nKey features:\n\nLong-lived data that is accessed a few times per year with instant retrievals\nData retrieval in milliseconds with the same performance as S3 Standard\nDesigned to deliver 99.9% availability with an availability SLA of 99%\n128 KB minimum object size\nS3 PUT API for direct uploads to S3 Glacier Instant Retrieval, and S3 Lifecycle management for automatic migration of objects\n\nAmazon S3 Glacier Flexible Retrieval (Formerly S3 Glacier)\nS3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1—2 times per year and is retrieved asynchronously. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, S3 Glacier Flexible Retrieval (formerly S3 Glacier) is the ideal storage class. S3 Glacier Flexible Retrieval delivers the most flexible retrieval options that balance cost with access times ranging from minutes to hours and with free bulk retrievals. It is an ideal solution for backup, disaster recovery, offsite data storage needs, and for when some data occasionally need to be retrieved in minutes, and you don’t want to worry about costs. S3 Glacier Flexible Retrieval is designed for 99.999999999% (11 nines) of data durability and 99.99% availability by redundantly storing data across multiple physically separated AWS Availability Zones in a given year. For more information, visit the Amazon S3 Glacier storage classes page »\nKey features:\n\nBackup and archive data that is rarely accessed and low cost\nDesigned to deliver 99.99% availability with an availability SLA of 99.9%\nSupports SSL for data in transit and encryption of data at rest\nIdeal for backup and disaster recovery use cases when large sets of data occasionally need to be retrieved in minutes, without concern for costs\nConfigurable retrieval times, from minutes to hours, with free bulk retrievals\nS3 PUT API for direct uploads to S3 Glacier Flexible Retrieval, and S3 Lifecycle management for automatic migration of objects\n\nAmazon S3 Glacier Deep Archive\nS3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers—particularly those in highly-regulated industries, such as financial services, healthcare, and public sectors—that retain data sets for 7—10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. All objects stored in S3 Glacier Deep Archive are replicated and stored across at least three geographically-dispersed Availability Zones, protected by 99.999999999% of durability, and can be restored within 12 hours. For more information, visit the Amazon S3 Glacier storage classes page »\nKey features:\n\nArchive data that is very rarely accessed and very low cost\nDesigned to deliver 99.99% availability with an availability SLA of 99.9%\nIdeal alternative to magnetic tape libraries\nRetrieval time within 12 hours\nS3 PUT API for direct uploads to S3 Glacier Deep Archive, and S3 Lifecycle management for automatic migration of objects\n\nData residency and isolation\nAmazon S3 storage classes for AWS Dedicated Local Zones\nIn AWS Dedicated Local Zones, the S3 Express One Zone and S3 One Zone-Infrequent access storage classes are purpose-built to store data in a specific data perimeter to support your data isolation and data residency use cases. Dedicated Local Zones are a type of AWS infrastructure that is fully managed by AWS, built for exclusive use by you or your community, and placed in a location or data center specified by you to help you comply with regulatory requirements. Both storage classes store data in a single Dedicated Local Zone and are supported in directory buckets. S3 supports the AWS Management Console, AWS SDKs, and S3 APIs, so you can run S3 based applications in Dedicated Local Zones. S3 Express One Zone is a high-performance storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed data and latency-sensitive applications. Amazon S3 One Zone-Infrequent Access is designed for data that is accessed less frequently and is ideal for backups. \nKey features:\n\nStore S3 objects in a specific data perimeter\nEnforce security within a data perimeter using AWS Identity and Access Management (IAM)\nAudit bucket and object-level access for governance and compliance use cases with AWS CloudTrail\nDesigned to durably and redundantly store data in a single Dedicated Local Zone\n\nAmazon S3 on Outposts\nAmazon S3 on Outposts delivers object storage to your on-premises AWS Outposts environment. Using the S3 APIs and features available in AWS Regions today, S3 on Outposts makes it easy to store and retrieve data on your Outpost, as well as secure the data, control access, tag, and report on it. S3 on Outposts provides a single Amazon S3 storage class, named ‘OUTPOSTS’, which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your Outposts. The S3 Outposts storage class is ideal for workloads with local data residency requirements, and to satisfy demanding performance needs by keeping data close to on-premises applications.\nKey features:\n\nStore S3 objects in your on-premises AWS Outposts environment \nS3 Object compatibility and bucket management through the S3 SDK\nDesigned to durably and redundantly store data on your AWS Outposts rack\nEncryption using SSE-S3 and SSE-C\nAuthentication and authorization using IAM and S3 Access Points\n"},"AWS/For-Reading/Subnetting":{"title":"Subnetting","links":[],"tags":[],"content":"Subnetting in Computer Networks\nsource: www.javatpoint.com/subnetting-in-computer-networks\nIn this tutorial, we will learn about Subnetting in Computer Networks Subject. First and foremost, the most crucial concept we are going to learn today is that while studying the subject of computer networks is subnetting. The most crucial idea known as Subnetting will help to lessen or disperse the pressure that the networks’ heavy load causes. Let’s now quickly go through the idea of subnetting for everyone.\nNow, let us know the definition of Subnetting. But before going into the Subnetting concept let us know the overview of the concept named Subnetting in Computer Networks.\nSubnetting is a part of Network Layer. The duty of the network layer is to divide the received message into separate components and activities. The Network layer can be called as the heart of Computer Networks.\nBasic idea of the Concept\nFinding a network and delivering data to it was simpler when the IP (Internet Protocol) system was initially implemented since there were fewer individuals online. Sending a data packet to the desired machine in a network is getting more and more challenging these days due to the rise in internet users. Network performance becomes a major issue once a network is large enough to serve an enterprise.\nIn order to divide larger networks logically (firewalls, etc.) or physically (for example), an organization can employ IP subnets (smaller broadcast domains, etc.). To put it another way, routers base their routing choices on subnets. We shall learn more about these ideas in this post.\nIntroduction to Subnetting\nSubnetting is a combination of two words i.e. Sub and Netting. Here Sub word means Substitute and netting word means Network. The Substitute Network created for a function to happen is known as Subnetting.\nHere, Substitute Network does not mean a new network is created. A full piece of network is broken into small pieces and each piece a different is assigned.\nSubnet is the name given to piece of the broken network or can also be called as the Substitute network is known as Subnet. Subnets are the legal small parts of IP (Internet Protocol) Addressing process\nSubnetting should be done in such a way that network does not gets affected. This means that we can divide the network into different parts but all when put together should perform the same task when done before splitting in to small parts.\nSubnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed\nSubnetting is a technique for creating logical sub-networks from a single physical network (subnets). A company can grow its network via subnetting without asking for a new network number from its ISP. Subnetting hides network complexity while assisting in the reduction of network traffic. Here, a network which is unique has to provide its services to many Local Area Networks i.e. (LAN). So, for this reason Subnetting is extensively used.\nDo you know what these little subnets are? As we all know, subnetting divides networks into them. A subnet is a smaller network, also referred to as a sub network. An IP network is logically divided into several smaller network components by subnets. A subnet is used to divide a large network into a number of smaller, linked networks, which helps to minimize traffic. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network. To help with the lack of IP addresses on the internet, subnets were developed.\nA rapid, effective, and reliable computer network is what subnetting is meant to create. Network traffic must find more effective routes as they become larger and more complicated. If all network traffic used the same path and moved through the system at once, bottlenecks and congestion would form, creating sluggish and inefficient backlogs. You may reduce the number of routers that network traffic must transit through by setting up a subnet. In order to make traffic go the shortest distance feasible inside a bigger network, an engineer will effectively create smaller mini routes.\nPurpose of Subnetting in Computer Networks\n\nEfficiency of the Network\n\nBy removing the need for extra routers, subnetting makes network traffic simpler. This makes sure the data being transmitted can get to its destination as fast as possible, eliminating or avoiding any potential diversions that may slow it down.\n\nProvides Network Security\n\nBy isolating or removing vulnerable network regions and making it harder for intruders to move through a company’s network, subnetting helps the network managers in reducing network-wide risks.\n\nInternet Protocol (IP) Addressing Relocation\n\nEach class has a finite amount of possible host allocations; for instance, networks with more than 254 devices require a Class B allocation. Assume that you are a network administrator. Now, you have a task of allocating 150 hosts among three physical networks in three distinct cities for a Class B or C network. If so, we must either ask for additional address blocks for each network or split the single big network into small parts named subnets so that we could utilize a single address block across a number of physical networks.\nWe will learn about this concept deeper in the upcoming topics.\n\nReduction of Network Traffic\n\nPlacing all of the computers on the same subnet can assist minimize network traffic if a significant amount of an organization’s traffic is intended to be shared routinely among a number of devices. Without a subnet, all computers and servers on the network would be able to see data packets from every other machine.\n\nNetwork Speed Improvement\n\nThe main network is divided into smaller subnets through the process of subnetting, and the goal of these smaller, linked networks is to split the large network into a collection of smaller, less-busy networks. Subnets reduce the need for traffic to use unnecessary routes, which speeds up the network.\n\nDivision of IP Addresses\n\nAn IP address is split into its network address and host address via subnetting.\nThe split address may then be further divided into units using the subnet mask approach, and those units can be assigned to different network devices.\n\nHere, X refers to the Host ID. This is the only thing which gets changed in the Internet Protocol Address\nNow, we are going to learn how these subnets provide the different addresses to different devices and also the process of subnetting in computer networks. So, by this example we would easily understand the working of the Subnet.\nWe are going to learn how Subnets are formed for Internet Protocol version 4 (IPv4) Addressing.\nThe IPv4 Addressing has five different classes. They are:\n\nClass A Network\nClass B Network\nClass C Network\nClass D Network\nClass E Network\n\nThe total number of Internet Protocol Addresses (IP Address) gives the total number of Subnets that can be formed by using a network.\n\nClass A has 24 Host ID Bits\nClass B has 16 Host ID Bits\nClass C has 8 Host ID Bits\n\nThe number of usable IP Addresses that can be created is\nThe total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.\nClass A Network can have 224 - 2\nClass B Network can have 216 - 2\nClass C Network can have 28 - 2\nClass D and Class E do not contribute for IP Address creation.\nClass D is used for multicasting purpose\nClass E is used for Address Range Calculator\nThey are saved for future purposes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass NetworkTotal Number of Hosts that can be accommodatedTotal Number of IP Addresses Formula SubstitutionTotal Number of IP AddressesTotal Number of IP Addresses in WordsClass A224224 - 21, 67, 77, 214One Crore Sixty Seven Lakhs Seventy Seven Thousand Two Hundred And Thirty FourClass B216216 - 265, 534Sixty Five Thousand Five Hundred and Thirty FourClass C2828 - 2254Two Hundred And Fifty Four\nSubnetting\nWe have arrived at the subject at hand, Subnetting, thanks to the problem of IP address waste. By taking bits from the Host ID section of the address, subnetting enables the creation of smaller networks (sub networks; subnets) within of a larger network. With the help of those borrowed bits, we can build more networks with a reduced overall size.\nA Subnet is created from the bits taken from the Host ID.\nTo understand about this concept let take an example of a network this belongs to class C.\n\nOur goal is to create to build a network. The capacity of each network must be Thirty (30) Devices. We have three networks of type Class C Network based on IPv4 Addressing.\nEach Class C Network can provide Two Hundred and Fifty Four (254) Internet Protocol Addresses.\nThe Capacity of each device which we require is very less than the Capacity which we require.\nSo, now we divide the four networks based on the requirement. Let us see how this division happens.\nWe have four Class C Networks of imaginary Internet Protocol (IP) Addresses like:\n\nNetwork 1 : 255.147.1.0\nNetwork 2 : 255.147.2.0\nNetwork 3 : 255.147.3.0\nNetwork 4 : 255.147.4.0\n\nWe know that each network can produce 254 IP Addresses alone. This means four networks can produce 254 * 4 = 1016 (Thousand and Sixteen ) Internet Protocol Addresses can be formed. But what we require is only thirty Internet Protocol Addresses from each Network. This means we only need hundred and Twenty (120) IP Addresses only.\nThis means 1016 - 120 = 896\nEight Hundred and Ninety-Six Addresses created are wasted. So, we need to use the Host ID bits wisely.\nSo, by some calculation we will get to know that if we take 5 bits from each network we will be able to get 30 IP Addresses from each Network.\nThe formula for number of IP Addresses is:\nThe total number of IP Addresses creatable = 2 The total number of Host ID Bits - 2.\nSo, now we will consider 5 Host ID Bits.\n25 - 2 = 30 Internet Protocol Addresses from each Network.\nSo, by considering we can create 30 Usable IP Addresses from each Class C Network.\nSo, now we have 3 more Host ID Bits left over unused. We also have different ways in using these remaining bits.\nOther Ways are:\n\nThese remaining Host ID Bits can be used to increase the capacity of the IP Addresses to be created in future, if required.\nWe can also create a new six subnets from each network using these three Host ID Bits.\n\nFirst method is usually chosen because creation of two different subnets causes wastage of IP Addresses. Let me explain this problem with the help of the above example.\nExample:\nThe network belongs to Class C Network which has 8 Host ID Bits.\nIn the above first created Subnet we have only used 30 IP Addresses only.\nIn the newly created Subnet we have created only 6 IP Addresses only.\nThis means we have used the full potential of the Class C Network. We might have used the whole 8 bits. But, this is considered as wastage of resources.\nThis is called wastage because we have now a capacity of 36 IP Addresses to be created.\nBut, the actual capacity of the Class C is 254 IP Addresses.\nThis means 254 - 36 = 218 IP Addresses are wasted now because of this Host ID Bits Division.\nSo, it is better to save the remaining Host ID Bits for future purpose rather than dividing it for these kind of resource wasting purpose.\nWorking of Subnets in Computer Networks\nSubnetting, as we all know, separates the network into small subnets. While each subnet permits communication between the devices connected to it, subnets are connected together by routers. The network technology being utilized and the connectivity requirements define the size of a subnet. Each organization is responsible for selecting the number and size of the subnets it produces, within the constraints of the address space available for its use.\n\nFor the construction of the subnets, we usually check the MSB (Most Significant Bit) bits of the host ID and if found wrong we make it right. In order to create two network subnets, we fix one of the host’s MSB (Most Significant Bit) bits in the table below. We are unable to alter network bits since doing so would alter the entire network.\n\n\nWe need a subnet mask to identify a subnet, which is created by substituting the number “1” for each Network ID bit and the amount of bits we reserve for Host ID to create the subnet. A data packet from the internet is intended to be forwarded to the specified subnet network using the subnet mask.\nA part of an address should be used as the Subnet ID is also specified by the subnet mask. In order to apply the subnet mask to the whole network address, a binary AND operation is utilized. When performing an AND operation, it is assumed that the result will be “true” if both inputs are. If not, “false” is presented. This is only possible when both bits are 1.\nThe Subnet ID results from this. The Subnet ID is used by routers to choose the best route among the sub - networks.\n\n\nThe two components that make up an IP address are the Network Prefix (sometimes called the Network ID) and the Host ID. Depending on whether the address is Class A, B, or C, either the Network Prefix or the Host ID must be separated. A Class B IPv4 address, 172.16.37.5, is seen in the image below. The Network Prefix is 172.16.0.0, and the Host ID is 37.5.\n\n\n\nWe use permutations to the amount of bits set aside to form subnets if we wish to produce subnets of varied length. Variable Length Subnet Masking is the name of this subnetting (VLSM).\nAfter setting aside some bits to indicate the subnet, the broadcast address of a subnet is computed by setting all the remaining bits of the host id to 1.The message is sent to all network hosts using the broadcast address.\n\nAdvantages of Subnetting\n\nSubnetting is used to decrease the presence of Internet Protocol (IP) range.\nSubnets helps in stopping the devices or gadgets from occupying the whole network, only allowing the hosts to control which kind of user can have access to the important information. Simply, we can tell that network is safe just because of the subnetting concept.\nSubnetting concept increases the performance of the total network by deleting the repeated traffic causing errors.\nWe can convert the whole big network into smaller networks by using the concept of subnetting as discussed earlier.\n\nDisadvantages of Subnetting\n\nIf the number of subnets increases, then the number of routers must also increase along with the subnet increase number. This happens because each subnet has its own subnet mask, broadcast address and network address.\nAs told earlier, if we create many subnets many IP Addresses are wasted because of the wastage of Host ID Bits\nThe cost of the entire network is increased by subnetting, which calls for the acquisition of pricey internal routers, switches, hubs, and bridges, among other things.\nThe complexity of the network is increased through subnetting. The subnet network must be managed by a skilled network administrator.\n\nThis is all about Subnetting Concept in the subject named Computer Networks."},"AWS/For-Reading/Types-of-Autoscaling":{"title":"Types of Autoscaling","links":[],"tags":[],"content":"source: www.geeksforgeeks.org/amazon-web-services-scaling-amazon-ec2/\nAuto Scaling\nAuto Scaling is a feature in cloud computing that allows a cloud-based application to automatically adjust the resources it uses such as servers, compute instances based on demand. The goal of Auto Scaling is to ensure that the application has sufficient resources to meet performance goals and maintain availability, while also optimizing resource utilization and minimizing costs. To know the difference between Auto scaling and load balancer refer to the Auto Scaling vs Load Balancer.\nAWS(Amazon Web Services) Auto Scaling\nAWS auto-scaling is used to scale up and scale down the EC2-instance by depending on the incoming traffic. You can scale up and scale down the applications in a few minutes based on the traffic which will decrease the latency of the application to the end-users.\nBenefits of Auto Scaling\nDynamical scaling: AWS auto-scaling service doesn’t require any type of manual intervention it will automatically scale the application down and up by depending up on the incoming traffic.\n\nPay For You Use: In auto scaling the resource will be utilised in the optimised way where the demand is low the resource utilisation will be low and the demand will high the resource utilisation will increase so the AWS is going to charge you only for the amount of resources you really used.\nAutomatic Performance Maintenance: AWS auto scaling maintains the optimal application performance with considering the workloads it will ensures that the application is running to desired level which will decrease the latency and also the capacity will be increased by based on your application\n\nAuto Scaling Components\nFollowing are the components of AWS Scaling Components.\n\n****Groups:****For scaling and managing the EC2 instances are grouped together so that they may be thought of as a single logical entity. You can mention the minimum and maximum no.of EC2 instance are required based up on the demand of the incoming traffic.\nConfiguration templates: Configuration template or an launch template which is used by the EC2 autoscaling group for the EC2 instance. In which you can specify the Amazon Machine Image ID,keypair,security group and so on.\nScaling options: Aws Autoscaling provides no.of options some of them are mentioned as following.\n\nDynamic scaling\nPredictive scaling\nScheduled scaling\nManual scaling\n\n\n\n\nThat’s the point where Amazon EC2 Autoscaling comes into the picture. You may use Amazon EC2 Auto Scaling in order to add or delete Amazon EC2 instances with respect to changes in your application demand. You can maintain a higher feeling of application availability by dynamically scaling your instances in and out as needed.\nFeatures Of AWS (Amazon Web Services) Auto Scaling\nYou can use three scaling techniques within Amazon EC2 Auto Scaling i.e. Dynamic Scaling, Predictive Scaling, and Scheduled Scaling. They are explained in detail below:\n\n\nDynamic Scaling: Adapts to changing environments and responds with the EC2 instances as per the demand. It helps the user to follow the demand curve for the application, which ultimately helps the maintainer/user to scale the instances ahead of time. Target tracking scaling policies, for example, may be used to choose a loaded statistic for your application, such as CPU use. Alternatively, you might use Application Load Balancer’s new “Request Count Per Target” measure, which is a load balancing option for the Elastic Load Balancing service. After that, Amazon EC2 Auto Scaling will modify the number of EC2 instances as needed to keep you on track. \n\n\nPredictive Scaling: Helps you to schedule the right number of EC2 instances based on the predicted demand. You can use both dynamic and predictive scaling approaches together for faster scaling of the application. Predictive Scaling forecasts future traffic and allocates the appropriate number of EC2 instances ahead of time. Machine learning algorithms in Predictive Scaling identify changes in daily and weekly patterns and automatically update projections. In this way, the need to manually scale the instances on particular days is relieved. \n\n\nScheduled Scaling: As the name suggests allows you to scale your application based on the scheduled time you set. For e.g. A coffee shop owner may employ more baristas on weekends because of the increased demand and frees them on weekdays because of reduced demand.\n\n\nComputing power is a programmed resource in the cloud, so you may take a more flexible approach to scale your applications. When you add Amazon EC2 Auto Scaling to an application, you may create new instances as needed and terminate them when they’re no longer in use. In this way, you only pay for the instances you use, when they’re in use.\nTypes Of AWS (Amazon Web Services) Autoscaling\n\nHorizontal Scaling: Horizontal scaling involves adding more instances to your application to handle increased demand. This can be done manually by launching additional instances, or automatically using Amazon EC2 Auto Scaling, which monitors your application’s workload and adds or removes instances based on predefined rules.\nVertical Scaling: Vertical scaling involves increasing the resources of existing instances, such as CPU, memory, or storage. This can be done manually by resizing instances, or automatically using Amazon EC2 Auto Scaling with launch configurations that specify instance sizes based on the workload.\nLoad Balancing: Load balancing involves distributing incoming traffic across multiple instances to improve performance and availability. Amazon Elastic Load Balancing (ELB) is a service that automatically distributes incoming traffic across multiple instances in one or more Availability Zones.\nMulti-Availability Zone Deployment: Multi-Availability Zone (AZ) deployment involves launching instances in multiple AZs to improve availability and fault tolerance. Amazon EC2 Auto Scaling can be used to automatically launch instances in additional AZs to maintain availability in case of an AZ outage.\nContainerization: Containerization involves using containers to package and deploy applications, making them more portable and easier to manage. Amazon Elastic Container Service (ECS) is a service that makes it easy to run, stop, and manage Docker containers on a cluster of EC2 instances.\n\n\n\n                  \n                  NOTE\n                  \n                \n\nWhen we created the auto-scaling group, we configured the Desired capacity, Minimum capacity, maximum capacity, and CPU utilization. If CPU utilization increases by 60% in all instances, one more instance is created, and if CPU utilization decreases by 30% in all instances, one instance is terminated.\n\n\nAWS Auto Scaling – FAQ’S\n1. What Is The Difference Between AWS Auto Scaling And EC2 Auto Scaling?\n\nAWS auto scaling is an service provided by the AWS which is used to scale the EC2 by depending up the in coming traffic.\n\n2. What Are The Two Types Of Auto Scaling?\n\nAuto scaling is mainly used to scale up and scale down the application based on the load. There are four main types of AWS autoscaling:\n\nmanual scaling,\nscheduled scaling,\ndynamic scaling, and\npredictive scaling\n\n\n3. What Are The 3 Components Of Auto Scaling Group?\n\nThe main components of autoscaling was mentioned below.\n\nLoad Balancer.\nSnapshot.\nEC2 (Elastic Compute Cloud) Instance.\nAutoscaling group.\n\n\n4. AWS Autoscaling Group Terraform\n\nAWS Auto Scaling Group Terraform is a module that allows you to create and manage Auto Scaling groups using Terraform.\n"},"AWS/For-Reading/ssl-certificate":{"title":"ssl certificate","links":[],"tags":[],"content":"What is an SSL certificate?\nAn SSL certificate is a digital certificate that authenticates a website’s identity and enables an encrypted connection. SSL stands for Secure Sockets Layer, a security protocol that creates an encrypted link between a web server and a web browser.\nCompanies and organizations need to add SSL certificates to their websites to secure online transactions and keep customer information private and secure.\nIn short: SSL keeps internet connections secure and prevents criminals from reading or modifying information transferred between two systems. When you see a padlock icon next to the URL in the address bar, that means SSL protects the website you are visiting.\nSince its inception about 25 years ago, there have been several versions of SSL protocol, all of which at some point ran into security troubles. A revamped and renamed version followed — TLS (Transport Layer Security), which is still in use today. However, the initials SSL stuck, so the new version of the protocol is still usually called by the old name.\nHow do SSL certificates work?\nSSL works by ensuring that any data transferred between users and websites, or between two systems, remains impossible to read. It uses encryption algorithms to scramble data in transit, which prevents hackers from reading it as it is sent over the connection. This data includes potentially sensitive information such as names, addresses, credit card numbers, or other financial details.\nThe process works like this:\n\nA browser or server attempts to connect to a website (i.e., a web server) secured with SSL.\nThe browser or server requests that the web server identifies itself.\nThe web server sends the browser or server a copy of its SSL certificate in response.\nThe browser or server checks to see whether it trusts the SSL certificate. If it does, it signals this to the webserver.\nThe web server then returns a digitally signed acknowledgment to start an SSL encrypted session.\nEncrypted data is shared between the browser or server and the webserver.\n\nThis process is sometimes referred to as an “SSL handshake.” While it sounds like a lengthy process, it takes place in milliseconds.\nWhen a website is secured by an SSL certificate, the acronym HTTPS (which stands for HyperText Transfer Protocol Secure) appears in the URL. Without an SSL certificate, only the letters HTTP – i.e., without the S for Secure – will appear. A padlock icon will also display in the URL address bar. This signals trust and provides reassurance to those visiting the website.\nTo view an SSL certificate’s details, you can click on the padlock symbol located within the browser bar. Details typically included within SSL certificates include:\n\nThe domain name that the certificate was issued for\nWhich person, organization, or device it was issued to\nWhich Certificate Authority issued it\nThe Certificate Authority’s digital signature\nAssociated subdomains\nIssue date of the certificate\nThe expiry date of the certificate\nThe public key (the private key is not revealed)\n\nWhy you need an SSL certificate\nWebsites need SSL certificates to keep user data secure, verify ownership of the website, prevent attackers from creating a fake version of the site, and convey trust to users.\nIf a website is asking users to sign in, enter personal details such as their credit card numbers, or view confidential information such as health benefits or financial information, then it is essential to keep the data confidential. SSL certificates help keep online interactions private and assure users that the website is authentic and safe to share private information with.\nMore relevant to businesses is the fact that an SSL certificate is required for an HTTPS web address. HTTPS is the secure form of HTTP, which means that HTTPS websites have their traffic encrypted by SSL. Most browsers tag HTTP sites – those without SSL certificates – as “not secure.” This sends a clear signal to users that the site may not be trustworthy – incentivizing businesses who have not done so to migrate to HTTPS.\nAn SSL certificate helps to secure information such as:\n\nLogin credentials\nCredit card transactions or bank account information\nPersonally identifiable information — such as full name, address, date of birth, or telephone number\nLegal documents and contracts\nMedical records\nProprietary information\n\nTypes of SSL certificate\nThere are different types of SSL certificates with different validation levels. The six main types are:\n\nExtended Validation certificates (EV SSL)\nOrganization Validated certificates (OV SSL)\nDomain Validated certificates (DV SSL)\nWildcard SSL certificates\nMulti-Domain SSL certificates (MDC)\nUnified Communications Certificates (UCC)\n\nExtended Validation certificates (EV SSL)\nThis is the highest-ranking and most expensive type of SSL certificate. It tends to be used for high profile websites which collect data and involve online payments. When installed, this SSL certificate displays the padlock, HTTPS, name of the business, and the country on the browser address bar. Displaying the website owner’s information in the address bar helps distinguish the site from malicious sites. To set up an EV SSL certificate, the website owner must go through a standardized identity verification process to confirm they are authorized legally to the exclusive rights to the domain.\nOrganization Validated certificates (OV SSL)\nThis version of SSL certificate has a similar assurance similar level to the EV SSL certificate since to obtain one; the website owner needs to complete a substantial validation process. This type of certificate also displays the website owner’s information in the address bar to distinguish from malicious sites. OV SSL certificates tend to be the second most expensive (after EV SSLs), and their primary purpose is to encrypt the user’s sensitive information during transactions. Commercial or public-facing websites must install an OV SSL certificate to ensure that any customer information shared remains confidential.\nDomain Validated certificates (DV SSL)\nThe validation process to obtain this SSL certificate type is minimal, and as a result, Domain Validation SSL certificates provide lower assurance and minimal encryption. They tend to be used for blogs or informational websites – i.e., which do not involve data collection or online payments. This SSL certificate type is one of the least expensive and quickest to obtain. The validation process only requires website owners to prove domain ownership by responding to an email or phone call. The browser address bar only displays HTTPS and a padlock with no business name displayed.\nWildcard SSL certificates\nWildcard SSL certificates allow you to secure a base domain and unlimited sub-domains on a single certificate. If you have multiple sub-domains to secure, then a Wildcard SSL certificate purchase is much less expensive than buying individual SSL certificates for each of them. Wildcard SSL certificates have an asterisk * as part of the common name, where the asterisk represents any valid sub-domains that have the same base domain. For example, a single Wildcard certificate for *website can be used to secure:\n\npayments.yourdomain.com\nlogin.yourdomain.com\nmail.yourdomain.com\ndownload.yourdomain.com\nanything.yourdomain.com\n\nMulti-Domain SSL Certificate (MDC)\nA Multi-Domain certificate can be used to secure many domains and/or sub-domain names. This includes the combination of completely unique domains and sub-domains with different TLDs (Top-Level Domains) except for local/internal ones.\nFor example:\n\nwww.example.com\nexample.org\nmail.this-domain.net\nexample.anything.com.au\ncheckout.example.com\nsecure.example.org\n\nMulti-Domain certificates do not support sub-domains by default. If you need to secure both www.example.com and example.com with one Multi-Domain certificate, then both hostnames should be specified when obtaining the certificate.\nUnified Communications Certificate (UCC)\nUnified Communications Certificates (UCC) are also considered Multi-Domain SSL certificates. UCCs were initially designed to secure Microsoft Exchange and Live Communications servers. Today, any website owner can use these certificates to allow multiple domain names to be secured on a single certificate. UCC Certificates are organizationally validated and display a padlock on a browser. UCCs can be used as EV SSL certificates to give website visitors the highest assurance through the green address bar.\nIt is essential to be familiar with the different types of SSL certificates to obtain the right type of certificate for your website.\nHow to obtain an SSL certificate\nSSL certificates can be obtained directly from a Certificate Authority (CA). Certificate Authorities – sometimes also referred to as Certification Authorities – issue millions of SSL certificates each year. They play a critical role in how the internet operates and how transparent, trusted interactions can occur online.\nThe cost of an SSL certificate can range from free to hundreds of dollars, depending on the level of security you require. Once you decide on the type of certificate you require, you can then look for Certificate Issuers, which offer SSLs at the level you require.\nObtaining your SSL involves the following steps:\n\nPrepare by getting your server set up and ensuring your WHOIS record is updated and matches what you are submitting to the Certificate Authority (it needs to show the correct company name and address, etc.)\nGenerating a Certificate Signing Request (CSR) on your server. This is an action your hosting company can assist with.\nSubmitting this to the Certificate Authority to validate your domain and company details\nInstalling the certificate they provide once the process is complete.\n\nOnce obtained, you need to configure the certificate on your web host or on your own servers if you host the website yourself.\nHow quickly you receive your certificate depends on what type of certificate you get and which certificate provider you procure it from. Each level of validation takes a different length of time to complete. A simple Domain Validation SSL certificate can be issued within minutes of being ordered, whereas Extended Validation can take as long as a full week.\n\nCan an SSL certificate be used on multiple servers?\nIt is possible to use one SSL certificate for multiple domains on the same server. Depending on the vendor, you can also use one SSL certificate on multiple servers. This is because of Multi-Domain SSL certificates, which we discussed above.\nAs the name implies, Multi-Domain SSL Certificates work with multiple domains. The number is left up to the specific issuing Certificate Authority. A Multi-Domain SSL Certificate is different from a Single Domain SSL Certificate, which – again, as the name implies – is designed to secure a single domain.\nTo make matters confusing, you may hear Multi-Domain SSL Certificates, also referred to as SAN certificates. SAN stands for Subject Alternative Name. Every multi-domain certificate has additional fields (i.e., SANs), which you can use to list additional domains that you want to cover under one certificate.\nUnified Communications Certificates (UCCs) and Wildcard SSL Certificates also allow for multi-domains and, in the latter case, an unlimited number of subdomains.\nWhat happens when an SSL certificate expires?\nSSL certificates do expire; they don’t last forever. The Certificate Authority/Browser Forum, which serves as the de facto regulatory body for the SSL industry, states that SSL certificates should have a lifespan of no more than 27 months. This essentially means two years plus you can carry over up to three months if you renew with time remaining on your previous SSL certificate.\nSSL certificates expire because, as with any form of authentication, information needs to be periodically re-validated to check it is still accurate. Things change on the internet, as companies and also websites are bought and sold. As they change hands, the information relevant to SSL certificates also changes. The purpose of the expiry period is to ensure that the information used to authenticate servers and organizations is as up-to-date and accurate as possible.\nPreviously, SSL certificates could be issued for as long as five years, which was subsequently reduced to three and most recently to two years plus a potential extra three months. In 2020, Google, Apple, and Mozilla announced they would enforce one-year SSL certificates, despite this proposal being voted down by the Certificate Authority Browser Forum. This took effect from September 2020. It is possible that in the future, the length of validity will reduce still further.\nWhen an SSL certificate expires, it makes the site in question unreachable. When a user’s browser arrives at a website, it checks the SSL certificate’s validity within milliseconds (as part of the SSL handshake). If the SSL certificate has expired, visitors will receive a message to the effect of — “This site is not secure. Potential risk ahead”.\nWhile users do have the option to proceed, it is not advisable to do so, given the cybersecurity risks involved, including the possibility of malware. This will significantly impact bounce rates for website owners, as users rapidly click off the homepage and go elsewhere.\nKeeping on top of when SSL certificates expire presents a challenge for larger businesses. While smaller and medium-sized businesses (SMEs) may have one or only a few certificates to manage, enterprise-level organizations that potentially transact across markets – with numerous websites and networks – will have many more. At this level, allowing an SSL certificate to expire is usually the result of oversight rather than incompetence. The best way for larger businesses to stay on top of when their SSL certificates expire is by using a certificate management platform. There are various products on the market, which you can find using an online search. These allow enterprises to see and manage digital certificates across their entire infrastructure. If you do use one of these platforms, it is important to log in regularly so you can be aware of when renewals are due.\nIf you allow a certificate to expire, the certificate becomes invalid, and you will no longer be able to run secure transactions on your website. The Certification Authority (CA) will prompt you to renew your SSL certificate before the expiration date.\nWhichever Certificate Authority or SSL service you use to obtain your SSL certificates from will send you expiration notifications at set intervals, usually starting at 90 days out. Try to ensure that these reminders are being sent to an email distribution list — rather than a single individual, who may have left the company or moved to another role by the time the reminder is sent. Think about which stakeholders in your company are on this distribution list to ensure the right people see the reminders at the right time.\nHow to tell if a site has an SSL certificate\nThe easiest way to see if a site has an SSL certificate is by looking at the address bar in your browser:\n\nIf the URL begins with HTTPS instead of HTTP, that means the site is secured using an SSL certificate.\nSecure sites show a closed padlock emblem, which you can click on to see security details – the most trustworthy sites will have green padlocks or address bars.\nBrowsers also show warning signs when a connection is not secure — such as a red padlock, a padlock which is not closed, a line going through the website’s address, or a warning triangle on top of the padlock emblem.\n\nHow to ensure your online session is safe\nOnly submit your personal data and online payment details to websites with EV or OV certificates. DV certificates are not suitable for eCommerce websites. You can tell if a site has an EV or OV certificate by looking at the address bar. For an EV SSL, the organization’s name will be visible in the address bar itself. For an OV SSL, you can see the organization’s name’s details by clicking on the padlock icon. For a DV SSL, only the padlock icon is visible.\nRead the website’s privacy policy. This enables you to see how your data will be used. Legitimate companies will be transparent about how they collect your data and what they do with it.\nLook out for trust signals or indicators on websites.\nAs well as SSL certificates, these include reputable logos or badges which show the website meets specific security standards. Other signs that can help you determine if a site is real or not include checking for a physical address and telephone number, checking their returns or refunds policy, and making sure prices are believable and not too good to be true.\nStay alert to phishing scams.\nSometimes cyber attackers create websites that mimic existing websites to trick people into purchasing something or logging in to their phishing site. It is possible for a phishing site to obtain an SSL certificate and therefore encrypt all the traffic that flows between you and it. A growing proportion of phishing scams occur on HTTPS sites — deceiving users who feel reassured by the padlock icon’s presence.\nTo avoid these kinds of attacks:\n\nAlways examine the domain of the site you are on and ensure it is spelled correctly. The URL of a fake site might differ by only one character – e.g., amaz0n.com instead of amazon.com. If in doubt, type the domain directly into your browser to make sure you are connecting to the website you intend to visit.\nNever enter logins, passwords, banking credentials, or any other personal information on the site unless you are sure of its authenticity.\nAlways consider what a particular site is offering, whether it looks suspicious, and whether you really need to register on it.\nMake sure your devices are well protected: Kaspersky Internet Security checks URLs against an extensive database of phishing sites, and it detects scams regardless of how “safe” the resource looks.\n\nCybersecurity risks continue to evolve but understanding the types of SSL certificates to look out for and how to distinguish a safe site from a potentially dangerous one will help internet users avoid scams and protect their personal data from cybercriminals."},"AWS/Notes/Amazon-Web-Services-In-Plain-English":{"title":"Amazon Web Services In Plain English","links":[],"tags":[],"content":"I was searching one liner meaning for aws services and I came across some interesting websites:\n\n\n                  \n                  Meaning of AWS Services in one line \n                  \n                \n\n\nadayinthelifeof.nl/2020/05/20/aws.html\nexpeditedsecurity.com/aws-in-plain-english/\n\n\n\n\n\n                  \n                  Those I need \n                  \n                \n\nEC2 - Virtual Private Servers\nLambda - Programming Functions you can run but can costs a fortune\nS3 - File Storage (unmountable)\nEFS - Mountable Network Disks\nRDS - Managed Relational Database\nVPC - Virtual Private Network (consider as VLANs)\nDynamoDB - Large &amp; scalable non-relational database\nCloudFront - Content Delivery Network\nRoute 53 - Managed domains names and records\nCloudWatch - Monitoring and Logs\nAutoscaling - Scale resources dynamically or manually\nCloudTrail - Spying on your users (Tracking Users activity)\nIAM - Users and their Permissions, policies and roles\n\n"},"AWS/Notes/Aws-Services-Simply-Explained":{"title":"Aws Services Simply Explained","links":[],"tags":["notes","AWS","SimpleMeanings","Links","blog"],"content":"This is the list of AWS resources required for Entry level devops interview preparation. Sometimes we struggle to explain the definition we already know it’s usecase in simpler terms. So I curated this list. The ones written in Italian is my own hand crafted lines while others mostly generated from Gemini Ai.\n\n\nEC2 (Elastic Compute Cloud):  Like renting virtual computers in the cloud, you can run whatever software you want on them.\nIts like vps or vms in the cloud, they are scalable, flexible and cheaper.\n\n\nVPC is like having your own private network within the cloud. It lets you create a secure and isolated space for your resources, like your own little corner of the internet in AWS with all the required tools to create a network such as subnets, route table, internet gateway, VPNs, etc.\n\n\nLoad Balancer: Distributes traffic across multiple EC2 instances so your application can handle more users and is more resilient.\n\n\nCloudWatch:  Monitors your AWS resources and applications, collecting metrics and logs to help you understand performance and troubleshoot issues.\nAws provide cloudwatch service to monitor and manage logs of resources to help user getting better performance and troubleshoot issues.\n\n\nCloudFront: A content delivery network (CDN) that makes your websites and applications faster by caching content closer to your users.\nCloudfront is aws managed CDN to caching the content closer to users to make websites and applications faster.\n\n\nS3 (Simple Storage Service):  Like a hard drive in the cloud for storing all sorts of files, from websites to backups.\nAws managed storage service like a Google drive to store all sorts of files and store backups\n\n\nEFS (Elastic File System):  A network file system that lets you share files between multiple EC2 instances.\n\n\nRDS (Relational Database Service): Makes it easy to set up and manage databases in the cloud, like MySQL, PostgreSQL, and Oracle.\n\n\nRoute 53: A scalable DNS service that translates domain names (like google.com) into IP addresses.\n\n\nLambda: Lets you run code without provisioning or managing servers; you only pay for the compute time you use.\nServerless compute\n\n\nSNS (Simple Notification Service): A messaging service that can send notifications to various destinations, like email, SMS, and other applications.\n\n\nSQS (Simple Queue Service): Like a post office box in the cloud. You put messages in it (like tasks to be done), and other services can pick them up later. This helps keep different parts of your application loosely connected and make things more reliable.\n\n\nEBS (Elastic Block Store): Think of it as a fast hard drive in the cloud. You can attach it to an EC2 instance (your virtual computer) to store data. This is great for things like databases or file storage.\nEbs is like virtual hard drive that can be attached to ec2 instances and its scalable and you can adjust performance and type according to your need.\n\n"},"AWS/Notes/Simple-Definitions-To-give-in-Interview":{"title":"Simple Definitions To give in Interview","links":[],"tags":[],"content":"What is Linux?\nLinux is Unix like operating system kernel.\nLinux is a powerful, flexible, and open-source operating system that’s widely used in server environments.\nWhat is kernel?\nkernel is the core component of an operating system. It acts as bridge between software and hardware."},"AWS/Notes/aws-cli-cheatsheet":{"title":"aws-cli cheatsheet","links":[],"tags":[],"content":"Keypairs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseCommandconfigurationaws configurelist all keypairsaws ec2 describe-key-pairscreate a keypairaws ec2 create-key-pair --key-name --output textimport an existing keypairaws ec2 import-key-pair --key-name keyname_test --public-key-material file:///home/rkumar/id_rsa.pubdelete a keypairaws ec2 delete-key-pair --key-namedelete an AMIaws ec2 deregister-image --image-id ami-00000000list all instancesaws ec2 describe-instancescreate a new instanceaws ec2 run-instances --image-id ami-a0b1234 --instance-type t2.micro --security-group-ids sg-00000000 --dry-runstop an instanceaws ec2 terminate-instances --instance-ids &lt;instance_id&gt;list status of all instancesaws ec2 describe-instance-statuslist status of a specific instanceaws ec2 describe-instance-status --instance-ids &lt;instance_id&gt;list all running instance, Name tag and Public IP Addressaws ec2 describe-instances --filters Name=instance-state-name,Values=running --query &#039;Reservations[].Instances[].[PublicIpAddress, Tags[?Key==Name].Value | [0] ]&#039; --output textlist all security groupsaws ec2 describe-security-groupsdelete a security groupaws ec2 delete-security-group --group-id sg-00000000list all user’s infoaws iam list-userslist all user’s usernamesaws iam list-users --output text | cut -f 6list current user’s infoaws iam get-userlist current user’s access keysaws iam list-access-keyscrate new useraws iam create-user --user-name UserNamelist bucketsaws s3 lslist bucket contentaws s3 ls s3://&lt;bucketName&gt;make bucketaws s3 mb s3://&lt;bucketName&gt;remove empty bucketaws s3 rb s3://&lt;bucketName&gt;copy to bucketaws s3 cp &lt;object&gt; s3://&lt;bucketName&gt;copy from bucketaws s3 cp s3://&lt;bucketName&gt;/&lt;object&gt; &lt;destination&gt;move objectaws s3 mv s3://&lt;bucketName&gt;/&lt;object&gt; &lt;destination&gt;sync objectsaws s3 sync &lt;local&gt; s3://&lt;bucketName&gt;removed objectsaws s3 rm s3://&lt;bucketName&gt;/&lt;object&gt;"},"AWS/Tasks-Done/Create-notification-for-s3-bucket-activity-happen-of-put-and-delete":{"title":"Create notification for s3 bucket activity happen of put and delete","links":[],"tags":["SNS","aws_tasks","S3"],"content":"Scenario:\nWhenever a new object is added or deleted in s3 bucket you will get notification via email\nStep 1: Create a bucket from s3 service.\n\nStep 2: Create an SNS topic with the help of SNS service\n\n  Create an topic &gt; go to access policy and add a policy\n \n\nClick on Advanced and Paste this code.\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;Service&quot;: &quot;s3.amazonaws.com&quot;\n      },\n      &quot;Action&quot;: &quot;SNS:Publish&quot;,\n      &quot;Resource&quot;: &quot;arn:aws:sns:us-west-2:970547378605:sns-s3-topic123&quot;,\n      &quot;Condition&quot;: {\n        &quot;ArnLike&quot;: {\n          &quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::snsbucket123testing&quot;\n        }\n      }\n    }\n  ]\n}\n\n\n                  \n                  Tip\n                  \n                \n\nIn above JSON code edit “Resource”  block to replace the ARN id according to your sns topic’s Arn id and aws:SourceArn to your S3 bucket’s ARN id.\nor in last two fields just change your AWS account id and topic name\n&quot;Resource&quot;: &quot;arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;&quot;\n&quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::&lt;s3-bucket-name&gt;&quot;\n\n\nStep 3: Create subscription\nAfter Topic is created Create Subscription to get notifications\n\nSet:\n\nTopic ARN: Your previously created Topic’s ARN ID\nProtocol: Email\n   - Endpoint: Your Gmail address\nClick on create subscription button\n\nAfter your subscription is created, you’ll get a mail to confirm it.\n\nGoto your mail and click on Confirm Subscription link.\n\nStep 4: Setting up Event Notification in S3 Bucket\nNow go to your buckets\n\n\nSelect Your Bucket:\n\n\nChoose the bucket for which you want to set up notifications.\n\n\nGo to the Properties tab.\n\n\n\nScroll down to Event notifications and click Create event notification.\n\n\n\nClick on Create event notification\n\n\nEvent name: put_and_delete_event\n\n\nEvent types:\n\nSelect put checkbox\nPermanently Deleted checkbox\n\n\n\n\nScroll down u will see Destination Option\n\nselect SNS topic\nchoose your previously created SNS Topic\nClick on Save changes\n\nTry adding new files in bucket you should get notification in your mail.\n\n\n\n\n"},"AWS/Tasks-Done/☑️-How-to-store-aws-load-balancer-logs-in-s3-bucket":{"title":"☑️ How to store aws load balancer logs in s3 bucket","links":[],"tags":["aws_tasks","S3","load_balancer"],"content":"Steps:\n1.    Create 2 instances and add http port and also install nginx enable it .\n2.    For load balancer u will need target group\n3.    Go to EC2 service scroll down u will see target groups\n\n4.    Create target group &gt; add instances &gt; save and create .\n\n\n5.    Now create load balancer (Application load balancer) &gt; configure it give name , vpc ,select availability zones, select Target Groups u created and create load balancer.\n\n\n6.    Check it your load balancer is working or not.\n7.    If working , Create s3 bucket  and go to permission &gt;  bucket policy and edit it :-\n8.    Note :-  Find policy from this by searching on google enable access log of load balancer to s3\n\nPolicy :-\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;AWS&quot;: &quot;arn:aws-us-gov:iam::elb-account-id:root&quot;\n      },\n      &quot;Action&quot;: &quot;s3:PutObject&quot;,\n      &quot;Resource&quot;: &quot;s3-bucket-arn&quot;\n    }\n  ]\n}\n\n9.    Save it.\n10.  Now go to load balancer u created &gt; go to attributes section &gt;  scroll down and u will see this monitoring sec enable access logs and add our bucket\n\n11.           Save changes.\n12.           U will see ur logs folder in ur s3 bucket."},"AWS/Tasks-Done/☑️-Implement-autoscaling-on-memory-utilization":{"title":"☑️ Implement autoscaling on memory utilization","links":[],"tags":["autoscaling","EC2"],"content":"Scenario:\n\nImplement autoscaling on memory utilization\nSNS will notification each time instance scale up and scale down\n\nIn short:\nImplementing Autoscaling on Memory Utilization with SNS Notifications\nUnderstanding the Components:\nServices Required:\n\nAWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.\nAmazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.\nAmazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).\n\nStep 1: Create an SNS Topic\n\nNavigate to the SNS console.\nCreate a new topic.\nGive it a descriptive name (e.g., “AutoScalingNotifications”).\n\n\nStep 2: Create an Auto Scaling Group\n\nNavigate to the EC2 console and select “Auto Scaling Groups”.\nCreate a new Auto Scaling group with the desired configuration:\n\nLaunch Configuration: Specify the AMI, instance type, security groups, etc.\nScaling Policies:\n\nScaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.\nTarget Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.\n\n\n\n\n\n\nCreated Launch Template with this User data\n#!/bin/bash\nyum install nginx unzip -y\nsystemctl enable --now nginx\ncurl -O [www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)\nunzip oxer.zip\nrm oxer.zip\nmv oxer* /usr/share/nginx/html\nsystemctl restart nginx\n\nenable sns notification and select previously created sns topic\n\n\nStep 3: Configure CloudWatch Alarms\n\n\nNavigate to the CloudWatch console.\n- Create an alarm for the “MemoryUtilization” metric:\n\nMetric Name: MemoryUtilization\nNamespace: AWS/EC2\nStatistic: Average\nPeriod: 5 Minutes (or as needed)\nThreshold: Set the desired threshold (e.g., 80%)\nComparison Operator: Greater Than or Equal To\nAlarm Actions: Select the SNS topic created in step 1.\n\n\n\nAssociate the Alarm with the Auto Scaling Group:\n\nIn the Auto Scaling group settings, under “Notifications,” add the CloudWatch alarm created in step 3.\n\n\n\nCreate Alarm\n\nselect memory utilization metric\n\n"},"AWS/Tasks-Done/☑️-Task-1---Implement-autoscaling-on-memory-utilization":{"title":"☑️ Task 1 - Implement autoscaling on memory utilization","links":[],"tags":["autoscaling","EC2"],"content":"Scenario:\n\nImplement autoscaling on memory utilization\nSNS will notification each time instance scale up and scale down\n\nIn short:\nImplementing Autoscaling on Memory Utilization with SNS Notifications\nUnderstanding the Components:\nServices Required:\n\nAWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.\nAmazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.\nAmazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).\n\nStep 1: Create an SNS Topic\n\nNavigate to the SNS console.\nCreate a new topic.\nGive it a descriptive name (e.g., “AutoScalingNotifications”).\n\n\nStep 2: Create an Auto Scaling Group\n\nNavigate to the EC2 console and select “Auto Scaling Groups”.\nCreate a new Auto Scaling group with the desired configuration:\n\nLaunch Configuration: Specify the AMI, instance type, security groups, etc.\nScaling Policies:\n\nScaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.\nTarget Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.\n\n\n\n\n\n\nCreated Launch Template with this User data\n#!/bin/bash\nyum install nginx unzip -y\nsystemctl enable --now nginx\ncurl -O [www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)\nunzip oxer.zip\nrm oxer.zip\nmv oxer* /usr/share/nginx/html\nsystemctl restart nginx\n\nenable sns notification and select previously created sns topic\n\n\nStep 3: Configure CloudWatch Alarms\n\n\nNavigate to the CloudWatch console.\n- Create an alarm for the “MemoryUtilization” metric:\n\nMetric Name: MemoryUtilization\nNamespace: AWS/EC2\nStatistic: Average\nPeriod: 5 Minutes (or as needed)\nThreshold: Set the desired threshold (e.g., 80%)\nComparison Operator: Greater Than or Equal To\nAlarm Actions: Select the SNS topic created in step 1.\n\n\n\nAssociate the Alarm with the Auto Scaling Group:\n\nIn the Auto Scaling group settings, under “Notifications,” add the CloudWatch alarm created in step 3.\n\n\n\nCreate Alarm\n\nselect memory utilization metric\n\n"},"AWS/Tasks-Done/☑️-Task-4---How-to-store-aws-load-balancer-logs-in-s3-bucket":{"title":"☑️ Task 4 - How to store aws load balancer logs in s3 bucket","links":[],"tags":["aws_tasks","S3","load_balancer"],"content":"Steps:\n1.    Create 2 instances and add http port and also install nginx enable it .\n2.    For load balancer u will need target group\n3.    Go to EC2 service scroll down u will see target groups\n\n4.    Create target group &gt; add instances &gt; save and create .\n\n\n5.    Now create load balancer (Application load balancer) &gt; configure it give name , vpc ,select availability zones, select Target Groups u created and create load balancer.\n\n\n6.    Check it your load balancer is working or not.\n7.    If working , Create s3 bucket  and go to permission &gt;  bucket policy and edit it :-\n8.    Note :-  Find policy from this by searching on google enable access log of load balancer to s3\n\nPolicy :-\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;AWS&quot;: &quot;arn:aws-us-gov:iam::elb-account-id:root&quot;\n      },\n      &quot;Action&quot;: &quot;s3:PutObject&quot;,\n      &quot;Resource&quot;: &quot;s3-bucket-arn&quot;\n    }\n  ]\n}\n\n9.    Save it.\n10.  Now go to load balancer u created &gt; go to attributes section &gt;  scroll down and u will see this monitoring sec enable access logs and add our bucket\n\n11.           Save changes.\n12.           U will see ur logs folder in ur s3 bucket."},"AWS/Tasks-Done/✅-Create-5-IAM-users-and-5-S3-buckets-and-attach-them-each-other":{"title":"✅ Create 5 IAM users and 5 S3 buckets and attach them each other","links":[],"tags":["aws_tasks","IAM","S3"],"content":"Task:\nrequirement:\n\n Create 5 I am users\n Create 5 S3 Buckets\n assign each user their name specific bucket\n no one should able touch other users bucket\n\n\nCreated User s3-mango with default settings\nCreated s3 bucket s3-mango-bucket with default settings\nafter creation added following policy to bucket\n\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;AWS&quot;: &quot;arn:aws:iam::970547378605:user/s3-mango&quot;\n      },\n      &quot;Action&quot;: &quot;s3:*&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::s3-mango-bucket&quot;,\n        &quot;arn:aws:s3:::s3-mango-bucket/*&quot;\n      ]\n    }\n  ]\n}\nStep 4: Assign IAM Policies to Users\nExample for Alice:\nGo to the IAM Console and open Alice.\nClick Add Permissions, then Attach Policies Directly.\nCreate a custom policy for Alice:\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: &quot;s3:*&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::s3-mango-bucket&quot;,\n        &quot;arn:aws:s3:::s3-mango-bucket/*&quot;\n      ]\n    }\n  ]\n}\nAttach this policy to Alice.\ngranted =AmazonS3ReadOnlyAccess= to each users\nchat gpt\nCertainly! Let’s go step-by-step with specific names for users and their buckets to make it clearer.\nScenario\nWe want to:\nCreate 5 users: Alice, Bob, Charlie, Dave, and Eve.\nCreate 5 buckets: alice-bucket, bob-bucket, charlie-bucket, dave-bucket, and eve-bucket.\nEnsure that:\nAlice can access only alice-bucket.\nBob can access only bob-bucket.\nSimilarly, each user has access only to their bucket.\nStep 1: Create Users\nManually create the following users via the IAM Console:\nAlice\nBob\nCharlie\nDave\nEve\nStep 2: Create Buckets\nManually create the following buckets via the S3 Console:\nalice-bucket\nbob-bucket\ncharlie-bucket\ndave-bucket\neve-bucket\nStep 3: Set Permissions for Each Bucket\nExample for Alice:\nGo to the S3 Console and open alice-bucket.\nGo to the Permissions tab, then Bucket Policy.\nAdd this bucket policy:\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;AWS&quot;: &quot;arn:aws:iam::ACCOUNT_ID:user/Alice&quot;\n      },\n      &quot;Action&quot;: &quot;s3:*&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::alice-bucket&quot;,\n        &quot;arn:aws:s3:::alice-bucket/*&quot;\n      ]\n    }\n  ]\n}\nReplace ACCOUNT_ID with your AWS account ID.\nSave the policy.\nRepeat this step for:\nBob with bob-bucket\nCharlie with charlie-bucket\nDave with dave-bucket\nEve with eve-bucket\nStep 4: Assign IAM Policies to Users\nExample for Alice:\nGo to the IAM Console and open Alice.\nClick Add Permissions, then Attach Policies Directly.\nCreate a custom policy for Alice:\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: &quot;s3:*&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::alice-bucket&quot;,\n        &quot;arn:aws:s3:::alice-bucket/*&quot;\n      ]\n    }\n  ]\n}\n\nAttach this policy to Alice.\nRepeat this step for:\nBob with bob-bucket\nCharlie with charlie-bucket\nDave with dave-bucket\nEve with eve-bucket\nStep 5: Test Access\nLog in as Alice and try to:\nUpload a file to alice-bucket (should work).\nAccess bob-bucket (should fail).\nRepeat the process for Bob, Charlie, Dave, and Eve to ensure they can only access their respective buckets.\nHow This Works\nEach user has a bucket policy on their bucket that allows only them to access it.\nEach user also has an IAM policy that allows them to access only their bucket.\nThis ensures that the permissions are restricted at both the bucket level and the user level.\nLet me know if you need help applying this setup!\ncreating bucket\n\nbucket name: s3-mango bucket\n\nobject ownership -\nACLs disabled (recommended)\nBlock Public Access settings for this bucket\nblock all public access turn on\n\n\nCreate user =mango=\npassword- myPassw0rd\n\n\nCreated policy called =my-s3-mango-bucket-policy=\nPermissions defined in this policy\n\n\n {\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;s3:*&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:s3:::s3-mango-bucket&quot;\n        }\n    ]\n}\nattached this policy to user\n"},"AWS/Tasks-Done/✅-Create-notification-for-s3-bucket-activity-happen-of-put-and-delete":{"title":"✅ Create notification for s3 bucket activity happen of put and delete","links":[],"tags":["SNS","aws_tasks","S3"],"content":"Scenario:\nWhenever a new object is added or deleted in s3 bucket you will get notification via email\nStep 1: Create a bucket from s3 service.\n\nStep 2: Create an SNS topic with the help of SNS service\n\n  Create an topic &gt; go to access policy and add a policy\n \n\nClick on Advanced and Paste this code.\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;Service&quot;: &quot;s3.amazonaws.com&quot;\n      },\n      &quot;Action&quot;: &quot;SNS:Publish&quot;,\n      &quot;Resource&quot;: &quot;arn:aws:sns:us-west-2:970547378605:sns-s3-topic123&quot;,\n      &quot;Condition&quot;: {\n        &quot;ArnLike&quot;: {\n          &quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::snsbucket123testing&quot;\n        }\n      }\n    }\n  ]\n}\n\n\n                  \n                  Tip\n                  \n                \n\nIn above JSON code edit “Resource”  block to replace the ARN id according to your sns topic’s Arn id and aws:SourceArn to your S3 bucket’s ARN id.\nor in last two fields just change your AWS account id and topic name\n&quot;Resource&quot;: &quot;arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;&quot;\n&quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::&lt;s3-bucket-name&gt;&quot;\n\n\nStep 3: Create subscription\nAfter Topic is created Create Subscription to get notifications\n\nSet:\n\nTopic ARN: Your previously created Topic’s ARN ID\nProtocol: Email\n   - Endpoint: Your Gmail address\nClick on create subscription button\n\nAfter your subscription is created, you’ll get a mail to confirm it.\n\nGoto your mail and click on Confirm Subscription link.\n\nStep 4: Setting up Event Notification in S3 Bucket\nNow go to your buckets\n\n\nSelect Your Bucket:\n\n\nChoose the bucket for which you want to set up notifications.\n\n\nGo to the Properties tab.\n\n\n\nScroll down to Event notifications and click Create event notification.\n\n\n\nClick on Create event notification\n\n\nEvent name: put_and_delete_event\n\n\nEvent types:\n\nSelect put checkbox\nPermanently Deleted checkbox\n\n\n\n\nScroll down u will see Destination Option\n\nselect SNS topic\nchoose your previously created SNS Topic\nClick on Save changes\n\nTry adding new files in bucket you should get notification in your mail.\n\n\n\n\n"},"AWS/Tasks-Done/✅-Get-sns-alert-when-any-user-launches-new-instance":{"title":"✅ Get sns alert when any user launches new instance","links":[],"tags":["aws_tasks","EC2","SNS"],"content":"Scenario:\nWhenever u r launching instance u will get a event and also u have to get details\nStep 1: Create an SNS topic\n\nCreate Topic\n\nType:  Standard\nName: NotifyonInstanceLaunch\n\n\nCreate subscription\n\nSelect Topic ARN\nProtocol: E-mail\nEndpoint: archsarangx@gmail.com\ncheck your mails and confirm subscription\n\n\n\n4.    Go to Amazon Event bridge service .\n5.    In buses section go to rules and Create rule\n\nIn step 2 Build event pattern scroll down to bottom\nIn Event Pattern section add configuration like this:\n\nEvent Source: AWS Service\nAWS Service: EC2\nEvent Type: EC2 Instance State-change Notification\n\nClick Next and in Step 3 Select Target page Select\nTarget Type: AWS Service\nSelect a target: SNS Topic\nTopic: your topic name\n\n\n9.    Create target and at last create rule\n You will get a notifications while Launching and terminating instance"},"AWS/Tasks-Done/✅-Host-static-website-on-s3-bucket":{"title":"✅ Host static website on s3 bucket","links":[],"tags":["aws_tasks","S3"],"content":"Requirement:\n\n Create a bucket\n add website template inside bucket\n enable static website hosting option\n view the website\n\nStep 1 - Creating a bucket\n\n\nGoto aws S3 bucket creation page\nClick on create bucket button\nSelect Bucket Type as General Purpose\nGive any Bucket Name of your desire (name should be globally unique)\n\n\n\nHere I set s3-cdec-bucket\nLeave default setting as it is like ACLs disabled on\nClear all Block all public access option\nI set bucket versioning enabled\nLeave other setting as it is and click on create bucket\n\nHere my s3 bucket is created\n\nStep 2 - Uploading website files into bucket\nYou can upload your website files if you have but I’ll use free website template\nfrom www.free-css.com/\nDownload any template from this site and upload the extracted content from the zip over the bucket\nI downloaded this template called browny www.free-css.com/free-css-templates/page296/browny\n\nIts gave me zip file.\nI’ll be extracting it and uploading the folder into the bucket\n\nIt contains these files\n\n\nLets upload the files into the root of the s3 bucket\n\n\nClick on your bucket name in s3 bucket list and click on upload button\nthen click on add files\nyou can also drag n drop files and folder\nClick on Add files to add files and Add folder to upload folder\n\nAfter the files are shown like this finally click on upload button\nAfter the files gets uploaded click on your s3 bucket name and goto properties and\n\nand scroll down till you see Static Website Hosting option\n\nclick on edit button and enable it\nafter enabling it select Hosting type as host a static website\n\nand specify the index document as index.html\n\nNow Static website option should be updated with the link to access our newly hosted static website link\n\nClick on the Bucket website endpoint url or open it in another tab\nStep 3 - Create policy\nbut before this link to even work we need to specify a permission policy to be able to see our site\nUnder the permission tab click Edit bucket policy\ncopy and paste this policy and save changes\n{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;PublicReadGetObject&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: &quot;*&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::your-bucket-name/*&quot;\n            ]\n        }\n    ]\n}\n \nThe provided JSON defines an AWS IAM policy that grants public read access to all objects in an S3 bucket named “your-bucket-name”.\nreplace your-bucket-name with your bucket name\nsave changes and see the link now your site should be visible.\nnow our website is visible over this url s3-cdec-bucket.s3-website-us-west-2.amazonaws.com/\n"},"AWS/Tasks-Done/✅-Implement-Template-with-Scheduled-Autoscaling":{"title":"✅ Implement Template with Scheduled Autoscaling","links":[],"tags":["aws_tasks","autoscaling","EC2","LaunchTemplates"],"content":"In this task we will be launching instance with userdata attach for setuping and starting nginx sever. We will create template out of that instance attach it to the autoscaling target groups. So when stress will be added to instance it should create another instances to balance the load.\nRequired Actions\n\n✅ Launch instance\n✅ Pass userdata\n✅ Install nginx\n✅ Create template from instance\n✅ Implement that template into auto scaling\n✅ Create scheduled autoscaling\n✅ Start instance\n✅ Add stress into instance\n✅ Upgrade the template to nginx to httpd\n\nStep 1: Launch Instance\n\nName: MyEc2Instance\nAMI: Amazon Linux (Free Tier)\nInstance Type: t2.micro (Free Tier)\nSelect Key pair\nSelect Common Security Group (which has 80, 22 port enabled)\nClick on Advanced Details and add following Userdata\n\n#!/bin/bash\nsudo yum install nginx unzip -y\nsudo systemctl enable --now nginx\nsudo curl -O www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip\nsudo unzip oxer.zip\nsudo rm oxer.zip\nsudo mv oxer* /usr/share/nginx/html\nsudo systemctl restart nginx\nClick on Launch Instance.\nResult:\n\n✅ Nginx page was successfully visible on root\n✅ And our template site is also visible on http://35.160.157.246/oxer-html/\n\n\n\nStep 2: Creating Launch Template from instance\nTo create Launch Template from running instance\nSelect your Instance Click Actions &gt;&gt; Image and Templates &gt;&gt; Create Templates from Instance\n\nGive Your template name and similar setting you gave in Step 1 and in Advanced tab make sure to add previous bash script as Userdata and create launch Template\n\nStep 3: Implementing the template into autoscaling\nIn Ec2 Instance Left Sidebar scroll down to see Auto Scaling groups option\n\n\nClick on Create AutoScaling group\nGive name, Select Launch Template\nChoose availability Zones\nNo load balancer\nNo VPC Lattice\nSet Health check grace period to 100\nDesired Capacity: 1\nMax desired capacity: 3\nFor Autoscaling Policy Select Target Tracking Policy\nMetric Type: Average CPU Utillization\nTarget Value: 30\nInstance Warmup: 80 Seconds\nClick on Create\n\nAfter Creation Click on Template Name and in Automatic Scaling Tab select Scheduled Actions option\n\nCreate Scheduled Action\n\nNow it at that specific time your scheduled autoscaling will work the desired way and you’ll get 2 instance launched minimum and maximum 2 if needed."},"AWS/Tasks-Done/✅-Monitoring-Nginx-logs-with-Cloudwatch":{"title":"✅ Monitoring Nginx logs with Cloudwatch","links":[],"tags":["cloudwatch","nginx","logs"],"content":"A webserver like nginx can have some important logs that is crucial to the security of server. but as cloud provider its not convenient to always login manually to check the logs file yourself. to automate this task Cloudwatch can be a huge help to monitor the direct logs generated by nginx and other webserver.\nHere we will see how to set up Logs monitoring with Cloudwatch.\nStep 1: Create ec2 instance\n\ninstall nginx\nactivate service\ncheck port 80 on browser to generate some logs\ncheck if these files get created\n\nsudo ls /var/log/nginx/\naccess.log  error.log\nStep 2: to send these logs to cloudwatch\n\ncreate  IAM role\ngive that role ec2-describe permission\nattach role to instance\n\n\nStep 3: Setup Agent\n\ninstall agent\n\nsudo yum install amazon-cloudwatch-agent -y\n\nenable agent service\n\nsystemctl enable --now amazon-cloudwatch-agent\n\nGoto this directory\n\ncd /opt/aws/amazon-cloudwatch-agent/bin/\n\ncreate configuration file\n\nsudo ./amazon-cloudwatch-agent-config-wizard\nafter finishing whole setup it will generate config.json file in that folder\nLog file path:\n/var/log/nginx/access.log\nconfig.json generated in bin folder\nTry validating this config with amazon-cloudwatch-agent\n\n./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json\nCheck over cloudwatch Log events should be visible like this\n\nTroubleshooting\nif you get error of collectd folder or file not found\ncreate an empty collectd file in that said location\nReferences:\n\ndocs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html\n"},"AWS/Tasks-Done/✅Create-notification-for-s3-bucket-activity-happen-of-put-and-delete":{"title":"✅Create notification for s3 bucket activity happen of put and delete","links":[],"tags":["SNS","aws_tasks","S3"],"content":"Scenario:\nWhenever a new object is added or deleted in s3 bucket you will get notification via email\nStep 1: Create a bucket from s3 service.\n\nStep 2: Create an SNS topic with the help of SNS service\n\n  Create an topic &gt; go to access policy and add a policy\n \n\nClick on Advanced and Paste this code.\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Principal&quot;: {\n        &quot;Service&quot;: &quot;s3.amazonaws.com&quot;\n      },\n      &quot;Action&quot;: &quot;SNS:Publish&quot;,\n      &quot;Resource&quot;: &quot;arn:aws:sns:us-west-2:970547378605:sns-s3-topic123&quot;,\n      &quot;Condition&quot;: {\n        &quot;ArnLike&quot;: {\n          &quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::snsbucket123testing&quot;\n        }\n      }\n    }\n  ]\n}\n\n\n                  \n                  Tip\n                  \n                \n\nIn above JSON code edit “Resource”  block to replace the ARN id according to your sns topic’s Arn id and aws:SourceArn to your S3 bucket’s ARN id.\nor in last two fields just change your AWS account id and topic name\n&quot;Resource&quot;: &quot;arn:aws:sns:ap-south-1:&lt;your-aws-account-id&gt;:&lt;sns-topic-name&gt;&quot;\n&quot;aws:SourceArn&quot;: &quot;arn:aws:s3:::&lt;s3-bucket-name&gt;&quot;\n\n\nStep 3: Create subscription\nAfter Topic is created Create Subscription to get notifications\n\nSet:\n\nTopic ARN: Your previously created Topic’s ARN ID\nProtocol: Email\n   - Endpoint: Your Gmail address\nClick on create subscription button\n\nAfter your subscription is created, you’ll get a mail to confirm it.\n\nGoto your mail and click on Confirm Subscription link.\n\nStep 4: Setting up Event Notification in S3 Bucket\nNow go to your buckets\n\n\nSelect Your Bucket:\n\n\nChoose the bucket for which you want to set up notifications.\n\n\nGo to the Properties tab.\n\n\n\nScroll down to Event notifications and click Create event notification.\n\n\n\nClick on Create event notification\n\n\nEvent name: put_and_delete_event\n\n\nEvent types:\n\nSelect put checkbox\nPermanently Deleted checkbox\n\n\n\n\nScroll down u will see Destination Option\n\nselect SNS topic\nchoose your previously created SNS Topic\nClick on Save changes\n\nTry adding new files in bucket you should get notification in your mail.\n\n\n\n\n"},"AWS/Tasks-Done/✅Task-7---Get-sns-alert-when-any-user-launches-new-instance":{"title":"✅Task 7 - Get sns alert when any user launches new instance","links":[],"tags":["aws_tasks","EC2","SNS"],"content":"Scenario:\nWhenever u r launching instance u will get a event and also u have to get details\nStep 1: Create an SNS topic\n\nCreate Topic\n\nType:  Standard\nName: NotifyonInstanceLaunch\n\n\nCreate subscription\n\nSelect Topic ARN\nProtocol: E-mail\nEndpoint: archsarangx@gmail.com\ncheck your mails and confirm subscription\n\n\n\n4.    Go to Amazon Event bridge service .\n5.    In buses section go to rules and Create rule\n\nIn step 2 Build event pattern scroll down to bottom\nIn Event Pattern section add configuration like this:\n\nEvent Source: AWS Service\nAWS Service: EC2\nEvent Type: EC2 Instance State-change Notification\n\nClick Next and in Step 3 Select Target page Select\nTarget Type: AWS Service\nSelect a target: SNS Topic\nTopic: your topic name\n\n\n9.    Create target and at last create rule\n You will get a notifications while Launching and terminating instance"},"AWS/Tasks-Done/✅🐈‍⬛Host-tomcat-basesd-web-app-called-Student-app-with-RDS-and-ec2":{"title":"✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2","links":[],"tags":["EC2","RDS","tomcat"],"content":"Step 1: Create Database\n\n\nGoto RDS dashboard and click on Create Database\nSelect Standard create for database creation method\nI chose MariaDB engine\nEngine Version selected n-1 that is one step behind the latest one\nSelect Free Tier Template\nI leave database-1 as DB instance identifier name\nSelf Managed Credential Management\nand inputed desired Master Password i.e Passw0rd123\nIn instance configuration I selected Burstable classes for DB instance class\nand selected db.t3.micro\nfor storage I selected General Purpose SSD gp2 and allocated 20GB of storage\nI left default setting as it is and clicked on Create Database\n\n\nAlso add security group rules for port 3306 for database\n\nStep 2: Configure Instance for Mariadb\n\n\nLaunch ec2 instance\nI gave name of ec2 instance db-test\nSelected Amazon linux free tier\nand added security group for 3306 port\n[make sure to have same security group for both ec2 and rds]\nClicked on create instance\nLog into instance\n\nStep 3: Connect to RDS via ec2\n\nAfter login to ec2 install mariadb client\n\nyum install mariadb105\n\nLogin to endpoint of RDS we just created\n\nmysql -h &lt;hostname&gt; -u &lt;user&gt; -p\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\nInput the password and congrats you just logged into your fresh sql database.\n\nReference:\nwww.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql\nCreate User (Optional)\nCREATE USER &#039;sammy&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;password&#039;;\ngrant create, alter, drop, insert, update, delete, select on demo.persons to &#039;sammy&#039;@&#039;localhost&#039;;\ngrant select on demo.* to &#039;sammy&#039;@&#039;localhost&#039;;           \nshow grants for &#039;sammy&#039;@&#039;localhost&#039;;\n \nInstall packages\nInstall tomcat, git, maven\nInstall tomcat from source\nInstall this specific version from source\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip\nunzip apache-tomcat-9.0.97.zip\nyum install java-17 -y \ncd apache-tomcat-9.0.97/bin/\nbash ./catalina.sh start \n \nhit the instance ip:8080 / curl the ip\nYou should see tomcat webpage\n\nInstall git\nsudo yum install git -y\n\nInstall maven\nsudo yum install maven -y\n\nClone the webapp repo\ngit clone github.com/Pritam-Khergade/student-ui\nbuild application\ncd student-ui\nmvn clean package\nthis must have created .war file in target folder\nrename it to suitable short name and move to apache-tomcat’s webapps directory\nmv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war\nmv target/studentapp.war ../apache-tomcat-9.0.97/webapps/\nhit the instance ip:8080/studentapp\n\nUpon filling the form if you get this\n\nIt means you have not connected RDS with this webapp\nlets do this\nLogin to mysql\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\ncreate database with studentapp tables\nCREATE DATABASE studentapp;\nuse studentapp;\nCREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,\n\tstudent_name VARCHAR(100) NOT NULL,\n    student_addr VARCHAR(100) NOT NULL,\n\tstudent_age VARCHAR(3) NOT NULL,\n\tstudent_qual VARCHAR(20) NOT NULL,\n\tstudent_percent VARCHAR(10) NOT NULL,\n\tstudent_year_passed VARCHAR(10) NOT NULL,\n\tPRIMARY KEY (student_id)\n);\nshow database;\nuse studentapp;\nshow tables;\ndesc students;\n\nIr should look like this\n\nConfigure tomcat to talk to RDS\ngoto conf folder in apache-tomcat source folder\ncd ../conf\n#and edit context.xml file\nvi context.xml\nand paste this code between &lt;context&gt;&lt;/context&gt; block\n&lt;Resource name=&quot;jdbc/TestDB&quot; auth=&quot;Container&quot; type=&quot;javax.sql.DataSource&quot;\n           maxTotal=&quot;500&quot; maxIdle=&quot;30&quot; maxWaitMillis=&quot;1000&quot;\n           username=&quot;admin&quot; password=&quot;Passw0rd123&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;\n           url=&quot;jdbc:mysql://database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com:3306/studentapp\nmake sure to edit username and password values with your own and url to your hostaname of database\nInstalling the mysql connector library file\ncopy this url - s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar\nPlace the mysql connector library (mysql-connector.jar) into /lib folder of apache-tomcat directory by using curl or wget\ncd ../lib\ncurl -O s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar\nnow that library file is placed into its correct location lets restart catalina\nbut first navigate back to bin directory where catalina.sh binary is located\ncd ../bin\nbash catalina.sh stop\nbash catalina.sh start\nNow navigate to browser with this url\nec2-instance-ip:8080/studentapp\nnow registration data should be able to passed into database\nAfter the filling the registration form login to mysql database and check if the data is present in the table now.\nmysql -h &lt;rds endpoint&gt; -u admin -p\nshow databases;\nuse studentapp;\nshow tables;\nselect * from students;"},"AWS/Tasks-Done/🐈‍⬛Host-tomcat-basesd-web-app-called-Student-app-with-RDS-and-ec2":{"title":"🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2","links":[],"tags":["EC2","RDS","tomcat"],"content":"Step 1: Create Database\n\n\nGoto RDS dashboard and click on Create Database\nSelect Standard create for database creation method\nI chose MariaDB engine\nEngine Version selected n-1 that is one step behind the latest one\nSelect Free Tier Template\nI leave database-1 as DB instance identifier name\nSelf Managed Credential Management\nand inputed desired Master Password i.e Passw0rd123\nIn instance configuration I selected Burstable classes for DB instance class\nand selected db.t3.micro\nfor storage I selected General Purpose SSD gp2 and allocated 20GB of storage\nI left default setting as it is and clicked on Create Database\n\n\nAlso add security group rules for port 3306 for database\n\nStep 2: Configure Instance for Mariadb\n\n\nLaunch ec2 instance\nI gave name of ec2 instance db-test\nSelected Amazon linux free tier\nand added security group for 3306 port\n[make sure to have same security group for both ec2 and rds]\nClicked on create instance\nLog into instance\n\nStep 3: Connect to RDS via ec2\n\nAfter login to ec2 install mariadb client\n\nyum install mariadb105\n\nLogin to endpoint of RDS we just created\n\nmysql -h &lt;hostname&gt; -u &lt;user&gt; -p\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\nInput the password and congrats you just logged into your fresh sql database.\n\nReference:\nwww.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql\nCreate User (Optional)\nCREATE USER &#039;sammy&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;password&#039;;\ngrant create, alter, drop, insert, update, delete, select on demo.persons to &#039;sammy&#039;@&#039;localhost&#039;;\ngrant select on demo.* to &#039;sammy&#039;@&#039;localhost&#039;;           \nshow grants for &#039;sammy&#039;@&#039;localhost&#039;;\n \nInstall packages\nInstall tomcat, git, maven\nInstall tomcat from source\nInstall this specific version from source\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip\nunzip apache-tomcat-9.0.97.zip\nyum install java-17 -y \ncd apache-tomcat-9.0.97/bin/\nbash ./catalina.sh start \n \nhit the instance ip:8080 / curl the ip\nYou should see tomcat webpage\n\nInstall git\nsudo yum install git -y\n\nInstall maven\nsudo yum install maven -y\n\nClone the webapp repo\ngit clone github.com/Pritam-Khergade/student-ui\nbuild application\ncd student-ui\nmvn clean package\nthis must have created .war file in target folder\nrename it to suitable short name and move to apache-tomcat’s webapps directory\nmv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war\nmv target/studentapp.war ../apache-tomcat-9.0.97/webapps/\nhit the instance ip:8080/studentapp\n\nUpon filling the form if you get this\n\nIt means you have not connected RDS with this webapp\nlets do this\nLogin to mysql\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\ncreate database with studentapp tables\nCREATE DATABASE studentapp;\nuse studentapp;\nCREATE TABLE if not exists students(student_id INT NOT NULL AUTO_INCREMENT,\n\tstudent_name VARCHAR(100) NOT NULL,\n    student_addr VARCHAR(100) NOT NULL,\n\tstudent_age VARCHAR(3) NOT NULL,\n\tstudent_qual VARCHAR(20) NOT NULL,\n\tstudent_percent VARCHAR(10) NOT NULL,\n\tstudent_year_passed VARCHAR(10) NOT NULL,\n\tPRIMARY KEY (student_id)\n);\nshow database;\nuse studentapp;\nshow tables;\ndesc students;\n\nIr should look like this\n\nConfigure tomcat to talk to RDS\ngoto conf folder in apache-tomcat source folder\ncd ../conf\n#and edit context.xml file\nvi context.xml\nand paste this code between &lt;context&gt;&lt;/context&gt; block\n&lt;Resource name=&quot;jdbc/TestDB&quot; auth=&quot;Container&quot; type=&quot;javax.sql.DataSource&quot;\n           maxTotal=&quot;500&quot; maxIdle=&quot;30&quot; maxWaitMillis=&quot;1000&quot;\n           username=&quot;admin&quot; password=&quot;Passw0rd123&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;\n           url=&quot;jdbc:mysql://database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com:3306/studentapp\nmake sure to edit username and password values with your own and url to your hostaname of database\nInstalling the mysql connector library file\ncopy this url - s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar\nPlace the mysql connector library (mysql-connector.jar) into /lib folder of apache-tomcat directory by using curl or wget\ncd ../lib\ncurl -O s3-us-west-2.amazonaws.com/studentapi-cit/mysql-connector.jar\nnow that library file is placed into its correct location lets restart catalina\nbut first navigate back to bin directory where catalina.sh binary is located\ncd ../bin\nbash catalina.sh stop\nbash catalina.sh start\nNow navigate to browser with this url\nec2-instance-ip:8080/studentapp\nnow registration data should be able to passed into database\nAfter the filling the registration form login to mysql database and check if the data is present in the table now.\nmysql -h &lt;rds endpoint&gt; -u admin -p\nshow databases;\nuse studentapp;\nshow tables;\nselect * from students;"},"AWS/Tasks-Done/🤔-Task-1---Implement-autoscaling-on-memory-utilization":{"title":"🤔 Task 1 - Implement autoscaling on memory utilization","links":[],"tags":["autoscaling","EC2"],"content":"Scenario:\nImplement autoscaling on memory utilization trigger notification each time instance scale up and scale down with sns\nImplementing Autoscaling on Memory Utilization with SNS Notifications\nUnderstanding the Components:\n\nAWS Auto Scaling Group (ASG): Manages a group of EC2 instances, automatically scaling them up or down based on predefined policies.\nAmazon CloudWatch: Monitors various metrics, including memory utilization, from your EC2 instances.\nAmazon SNS: A messaging service to send notifications to subscribed endpoints (e.g., email, SMS, or other AWS services).\n\nStep 1: Create an SNS Topic\n\nNavigate to the SNS console.\nCreate a new topic.\nGive it a descriptive name (e.g., “AutoScalingNotifications”).\n\n\nStep 2: Create an Auto Scaling Group\n\nNavigate to the EC2 console and select “Auto Scaling Groups”.\nCreate a new Auto Scaling group with the desired configuration:\n\nLaunch Configuration: Specify the AMI, instance type, security groups, etc.\nScaling Policies:\n\nScaling Adjustment Policy: Define the scaling adjustment (e.g., add or remove instances) based on specific conditions.\nTarget Tracking Scaling Policy: Set a target value for a specific metric (e.g., memory utilization) and let the ASG automatically adjust the number of instances to maintain that target.\n\n\n\n\n\n\nCreated Launch Template with this User data\n#!/bin/bash\nyum install nginx unzip -y\nsystemctl enable --now nginx\ncurl -O [www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip](www.free-css.com/assets/files/free-css-templates/download/page296/oxer.zip)\nunzip oxer.zip\nrm oxer.zip\nmv oxer* /usr/share/nginx/html\nsystemctl restart nginx\n\nenable sns notification and select previously created sns topic\n\n\nStep 3: Configure CloudWatch Alarms\n\n\nNavigate to the CloudWatch console.\n- Create an alarm for the “MemoryUtilization” metric:\n\nMetric Name: MemoryUtilization\nNamespace: AWS/EC2\nStatistic: Average\nPeriod: 5 Minutes (or as needed)\nThreshold: Set the desired threshold (e.g., 80%)\nComparison Operator: Greater Than or Equal To\nAlarm Actions: Select the SNS topic created in step 1.\n\n\n\nAssociate the Alarm with the Auto Scaling Group:\n\nIn the Auto Scaling group settings, under “Notifications,” add the CloudWatch alarm created in step 3.\n\n\n\nCreate Alarm\n\nselect memory utilization metric\n\n"},"AWS/✅-Tasks-To-Do":{"title":"✅ Tasks To Do","links":["AWS/Tasks-Done/✅-Implement-Template-with-Scheduled-Autoscaling","Implement-load-balancer-with-weighted-policy","AWS/Tasks-Done/✅-Create-5-IAM-users-and-5-S3-buckets-and-attach-them-each-other","AWS/Tasks-Done/✅-Host-static-website-on-s3-bucket","AWS/Tasks-Done/✅-Monitoring-Nginx-logs-with-Cloudwatch","AWS/Tasks-Done/☑️-Implement-autoscaling-on-memory-utilization","AWS/Tasks-Done/✅-Create-notification-for-s3-bucket-activity-happen-of-put-and-delete","Task-3---add-sns-alert-for-policy,-lifecycle-changes-for-all-buckets","AWS/Tasks-Done/☑️-How-to-store-aws-load-balancer-logs-in-s3-bucket","Task-5---enable-vpc-flow-logs","Task-6---Set-up-a-SNS-alert-Notification-for-New-Instance-launched-with-port-20","AWS/Tasks-Done/✅-Get-sns-alert-when-any-user-launches-new-instance","AWS/Tasks-Done/✅🐈‍⬛Host-tomcat-basesd-web-app-called-Student-app-with-RDS-and-ec2"],"tags":[],"content":"Practical Tasks to perform\n✅ = finished\n☑️ = done but not satisfied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.Task TitleStatus1.✅ Implement Template with Scheduled Autoscaling✅2.Implement load balancer with weighted policy (with 40:60 policy ratio)confused3.Path based routing [(/laptop /mobile /idli /dosa) (1 ALB 2 TG 3 instance)]done but not documented4.Add load balancer Between two vpcs each contains at least one instancedone but not documented5.✅ Create 5 IAM users and 5 S3 buckets and attach them each other✅6.✅ Host static website on s3 bucket✅7.✅ Monitoring Nginx logs with Cloudwatch✅\nWeek’s Assignment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.Task TitleStatus1.☑️ Implement autoscaling on memory utilizationhalf done then confused2.✅ Create notification for s3 bucket activity happen of put and delete✅3.Task 3 - add sns alert for policy, lifecycle changes for all buckets4.☑️ How to store aws load balancer logs in s3 bucket✅5.Task 5 - enable vpc flow logs6.Task 6 - Set up a SNS alert Notification for New Instance launched with port 207.✅ Get sns alert when any user launches new instance✅\nLatest Task\n\n\n\n\n\n\n\n\n\n\n\n\n\nSr.  No.Task Title1.✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2"},"AWS/🗒️-Tasks-to-Read":{"title":"🗒️ Tasks to Read","links":["AWS/For-Reading/Types-of-Autoscaling","AWS/For-Reading/Difference-between-Load-balancers","AWS/For-Reading/AWS-Instance-Types","AWS/For-Reading/Managed-policies-and-inline-policies","AWS/For-Reading/IAM-Policies","AWS/For-Reading/Storage-Classes-in-S3","AWS/For-Reading/Authorized-vs-Authonticated","AWS/For-Reading/ssl-certificate","AWS/For-Reading/SDLC---Software-Developement-LifeCycle","Agile-Development","Agile-Vs-DevOps","Windows-Kernel-Architecture"],"tags":["aws_tasks"],"content":"📄 Tasks to read\n🏡 Read the topics in home.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.TitleStatus1.Types of Autoscaling☑️2.Difference between Load balancers☑️3.AWS Instance Types☑️4.Managed policies and inline policies☑️5.IAM Policies Types of IAM Policies☑️6.IAM Policies Types of users in aws in IAM☑️7.Storage Classes in S3☑️8.Authorized vs Authonticated☑️9.ssl certificate☑️10.SDLC - Software Developement LifeCycle☑️11.Agile Development☑️12.Agile Vs DevOps☑️13.Windows Kernel Architecture-Youtube Channel Recommendations\n\nnehra classes\ngaurav sharma\nabhishek veermalla\n"},"AWS/🤿-Tasks-To-Do":{"title":"🤿 Tasks To Do","links":["AWS/Tasks-Done/✅-Implement-Template-with-Scheduled-Autoscaling","Implement-load-balancer-with-weighted-policy","AWS/Tasks-Done/✅-Create-5-IAM-users-and-5-S3-buckets-and-attach-them-each-other","AWS/Tasks-Done/✅-Host-static-website-on-s3-bucket","AWS/Tasks-Done/✅-Monitoring-Nginx-logs-with-Cloudwatch","AWS/Tasks-Done/☑️-Task-1---Implement-autoscaling-on-memory-utilization","AWS/Tasks-Done/✅Create-notification-for-s3-bucket-activity-happen-of-put-and-delete","Task-3---add-sns-alert-for-policy,-lifecycle-changes-for-all-buckets","AWS/Tasks-Done/☑️-Task-4---How-to-store-aws-load-balancer-logs-in-s3-bucket","Task-5---enable-vpc-flow-logs","Task-6---Set-up-a-SNS-alert-Notification-for-New-Instance-launched-with-port-20","AWS/Tasks-Done/✅Task-7---Get-sns-alert-when-any-user-launches-new-instance","AWS/Tasks-Done/✅🐈‍⬛Host-tomcat-basesd-web-app-called-Student-app-with-RDS-and-ec2"],"tags":[],"content":"Practical Tasks to perform\n✅ = finished\n☑️ = done but not satisfied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.Task TitleStatus1.✅ Implement Template with Scheduled Autoscaling✅2.Implement load balancer with weighted policy (with 40:60 policy ratio)confused3.Path based routing [(/laptop /mobile /idli /dosa) (1 ALB 2 TG 3 instance)]done but not documented4.Add load balancer Between two vpcs each contains at least one instancedone but not documented5.✅ Create 5 IAM users and 5 S3 buckets and attach them each other✅6.✅ Host static website on s3 bucket✅7.✅ Monitoring Nginx logs with Cloudwatch✅\nWeek’s Assignment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.Task TitleStatus1.☑️ Task 1 - Implement autoscaling on memory utilizationhalf done then confused2.✅Create notification for s3 bucket activity happen of put and delete✅3.Task 3 - add sns alert for policy, lifecycle changes for all buckets4.☑️ Task 4 - How to store aws load balancer logs in s3 bucket✅5.Task 5 - enable vpc flow logs6.Task 6 - Set up a SNS alert Notification for New Instance launched with port 207.✅Task 7 - Get sns alert when any user launches new instance✅\nOngoing Task\n\n\n\n\n\n\n\n\n\n\n\n\n\nSr.  No.Task Title1.✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2"},"Daily-Notes/2024-11-18-EC2-VPC-and-stuffs":{"title":"2024-11-18 EC2 VPC and stuffs","links":[],"tags":[],"content":"EC2\n\ninstances\nsecurity groups\nkey\nebs, snapshot backup, ami\nlifecycle policy\nload balancer\nauto scaling groups\n\nVPC\n\nvpc\nsubnet\nroute table\nnetwork\nigw\nnat gat\npubli priv\npeering\n\nIAM\n\nIdentity Access Management\n\nIdenity -⇒ user\nPermissions ⇒ policy\ngroup -⇒ developer\ndevops\nTwo types of policies\n- aws managed policy\n- customer managed policy\niam components\nuser\ngroup\npolicy\nrole\ntasks:\n\n direct policy\n inline policy\n\n\n\n                  \n                  TO\n                  \n                \n\n\n difference between inline policy and customer managed policy\n Types of policies in IAM\n Types of users in aws in IAM\n\n\n\nTypes of policy\n\nAWS Managed Policy\nCustomer Managed Policy\nDirect Inline Policy\n\nTypes of User\n\nIAM\nRoot User\nIAM Identity Center User\nFedereted Identity\nAWS Builder ID User\n\nPolicy Types\n\nIdentity Based\nResource Based\nService control policies\nPermission Boundary Policies\n"},"Daily-Notes/2024-11-18":{"title":"2024-11-18","links":[],"tags":[],"content":"EC2\n\ninstances\nsecurity groups\nkey\nebs, snapshot backup, ami\nlifecycle policy\nload balancer\nauto scaling groups\n\nVPC\n\nvpc\nsubnet\nroute table\nnetwork\nigw\nnat gat\npubli priv\npeering\n\nIAM\n\nIdentity Access Management\n\nIdenity -⇒ user\nPermissions ⇒ policy\ngroup -⇒ developer\ndevops\nTwo types of policies\n- aws managed policy\n- customer managed policy\niam components\nuser\ngroup\npolicy\nrole\ntasks:\n\n direct policy\n inline policy\n\n\n\n                  \n                  TO\n                  \n                \n\n\n difference between inline policy and customer managed policy\n Types of policies in IAM\n Types of users in aws in IAM\n\n\n\nTypes of policy\n\nAWS Managed Policy\nCustomer Managed Policy\nDirect Inline Policy\n\nTypes of User\n\nIAM\nRoot User\nIAM Identity Center User\nFedereted Identity\nAWS Builder ID User\n\nPolicy Types\n\nIdentity Based\nResource Based\nService control policies\nPermission Boundary Policies\n"},"Daily-Notes/2024-11-19":{"title":"2024-11-19","links":[],"tags":[],"content":"S3\n\nS3 is global service\nbut bucket is region specific\n\nin bucket things stored in the form of object\nstorage classes\naws.amazon.com/s3/storage-classes/"},"Daily-Notes/2024-11-22-s3-task":{"title":"2024-11-22 s3 task","links":[],"tags":[],"content":"S3 Task\n\nCreate an IAM Policy:\n\nFirst, create an IAM policy that defines the permissions for the specific S3 bucket(s).\nThis policy should follow the principle of least privilege, granting only the necessary permissions.\n\n\nAttach the Policy to the IAM User:\n\nGo to the IAM console and select the user.\nChoose “Add permissions” and then “Attach existing policies directly”.\nFind and select the policy you created for S3 bucket access.\n\n\nExample Policy (customize as needed):\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:GetObject&quot;,\n        &quot;s3:ListBucket&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::your-bucket-name&quot;,\n        &quot;arn:aws:s3:::your-bucket-name/*&quot;\n      ]\n    }\n  ]\n}\n\n\n    \n4. Security Considerations:\n    - Always use specific bucket ARNs instead of wildcards.\n    - Grant only the necessary permissions (e.g., read-only if that&#039;s all that&#039;s required).\n    - Consider using IAM groups for easier management if multiple users need the same access.\n\t\n5. Best Practices:\n    - Regularly review and audit permissions.\n    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.\n    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.\n\t\n6. Testing:\n    - After applying the policy, test the access in a safe, non-production environment.\n    - Verify that the user can only access the intended buckets and perform only the allowed actions.\n\nRemember, it&#039;s crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.\n\n\n# Static web hosting\nCreate s3 bucket\nproperties --&gt; enable static web hosting\npermission --&gt; turn off block public access\n\t\t\t\t\t--&gt; object ownership --&gt; enable ACL\n\t\t\t\t\t--&gt; ACL --&gt; enable everyone list and read\n\t\t\t\t\t--&gt; upload files\n"},"Daily-Notes/2024-11-22":{"title":"2024-11-22","links":[],"tags":[],"content":"S3 Task\n\nCreate an IAM Policy:\n\nFirst, create an IAM policy that defines the permissions for the specific S3 bucket(s).\nThis policy should follow the principle of least privilege, granting only the necessary permissions.\n\n\nAttach the Policy to the IAM User:\n\nGo to the IAM console and select the user.\nChoose “Add permissions” and then “Attach existing policies directly”.\nFind and select the policy you created for S3 bucket access.\n\n\nExample Policy (customize as needed):\n{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:GetObject&quot;,\n        &quot;s3:ListBucket&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::your-bucket-name&quot;,\n        &quot;arn:aws:s3:::your-bucket-name/*&quot;\n      ]\n    }\n  ]\n}\n\n\n    \n4. Security Considerations:\n    - Always use specific bucket ARNs instead of wildcards.\n    - Grant only the necessary permissions (e.g., read-only if that&#039;s all that&#039;s required).\n    - Consider using IAM groups for easier management if multiple users need the same access.\n\t\n5. Best Practices:\n    - Regularly review and audit permissions.\n    - Use AWS Organizations and Service Control Policies for managing permissions across multiple accounts.\n    - Implement proper bucket policies and access control lists (ACLs) on the S3 buckets themselves.\n\t\n6. Testing:\n    - After applying the policy, test the access in a safe, non-production environment.\n    - Verify that the user can only access the intended buckets and perform only the allowed actions.\n\nRemember, it&#039;s crucial to tailor these permissions to your specific use case and security requirements. For the most up-to-date and detailed information on IAM policies and S3 bucket permissions, please refer to the official AWS documentation.\n\n\n# Static web hosting\nCreate s3 bucket\nproperties --&gt; enable static web hosting\npermission --&gt; turn off block public access\n\t\t\t\t\t--&gt; object ownership --&gt; enable ACL\n\t\t\t\t\t--&gt; ACL --&gt; enable everyone list and read\n\t\t\t\t\t--&gt; upload files\n"},"Daily-Notes/2024-11-23-does-not-belong-here":{"title":"2024-11-23 does not belong here","links":[],"tags":[],"content":"Containerizing-wordpress-app\nand migrating from database\nOld files\nDockerfile for wordpress\n\ndocker-compose file\n\nmysql docker file\n\nmyPassw0rd\n‘wpuser123’@‘localhost’ IDENTIFIED BY ‘wpuserPassw0rd’;\n|Site Title|wp-testsite|\n|Username|wpwebuser |\n|password|)5S@PsG1J$VACnJ@%Q|\n|email|myemail@example.com|\n|||\nDocker mariadb password\nwpuser123Passw0rd\nlinux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/\ndevhints.io/mysql\ndocs.docker.com/engine/install/ubuntu/\nkifarunix.com/how-to-deploy-wordpress-as-a-docker-container/\n#exporting/backuping old mysql data\nmysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql\n \n#current mysql configuration\nmysql user: wpuser123\nmysql passwd: wpuserPassw0rd\ndb name: new_db\n \n#login to ec2 and run the backup command\nmysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql\n \nsteps\n\ncreate new folder in wordpress\ncd into it\n\nmkdir wordpress &amp;&amp; cd wordpress\n\n\npull wordpress and mariadb containers\nsudo docker pull wordpress\nsudo docker pull mariadb\n\n\ncreate custom docker network\nsudo docker network wp-app\nsudo docker network ls\n\n\nCreation of mariadb database container\n\n\n\nhash the password of db first\n\necho `openssl passwd` | sudo tee $PWD/.db-pass\nnow hashed password file must be in .db-pass file\nhere i have set myPassw0rd password\ncat .db-pass\n$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0\n\nCreate mariaDB data directory to mount container database with /var/lib/mysql\nsudo mkdir -p data\n\n~/wordpress \n\t- data\n\nCreate and run Mariadb Docker container\n\nsudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v &#039;/home/ubuntu/wordpress/data:/var/lib/mysql&#039; --name wp-mariadb mariadb\n\ncheck health and logs\ndocker ps\n\nsudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log\nexample:\nsudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log\n\ncheck mounted volume for generated data\nls -1 data\n\nregarding migration of old database\ncopy the backup file into container db folder\ncp mywpold_db.sql wordpress/data/\n\nlogin to mariadb container\n\ndocker exec -it wp-mariadb bash\nrestore old db\ncd /var/lib/mysql\nmariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;\nREFERENCES:\n\nkifarunix.com/how-to-deploy-wordpress-as-a-docker-container/\nmariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811\nmariadb.com/kb/en/backup-and-restore-overview/\nwww.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/\n"},"Daily-Notes/2024-11-23":{"title":"2024-11-23","links":[],"tags":[],"content":"Containerizing-wordpress-app\nand migrating from database\nOld files\nDockerfile for wordpress\n\ndocker-compose file\n\nmysql docker file\n\nmyPassw0rd\n‘wpuser123’@‘localhost’ IDENTIFIED BY ‘wpuserPassw0rd’;\n|Site Title|wp-testsite|\n|Username|wpwebuser |\n|password|)5S@PsG1J$VACnJ@%Q|\n|email|myemail@example.com|\n|||\nDocker mariadb password\nwpuser123Passw0rd\nlinux.how2shout.com/how-to-install-wordpress-on-ubuntu-22-04-lts-server/\ndevhints.io/mysql\ndocs.docker.com/engine/install/ubuntu/\nkifarunix.com/how-to-deploy-wordpress-as-a-docker-container/\n#exporting/backuping old mysql data\nmysqldump -u wpuser123 -pwpuserPassw0rd new_db &gt; dump.sql\n \n#current mysql configuration\nmysql user: wpuser123\nmysql passwd: wpuserPassw0rd\ndb name: new_db\n \n#login to ec2 and run the backup command\nmysqldump -u wpuser123 -p new_db &gt; mywpold_db.sql\n \nsteps\n\ncreate new folder in wordpress\ncd into it\n\nmkdir wordpress &amp;&amp; cd wordpress\n\n\npull wordpress and mariadb containers\nsudo docker pull wordpress\nsudo docker pull mariadb\n\n\ncreate custom docker network\nsudo docker network wp-app\nsudo docker network ls\n\n\nCreation of mariadb database container\n\n\n\nhash the password of db first\n\necho `openssl passwd` | sudo tee $PWD/.db-pass\nnow hashed password file must be in .db-pass file\nhere i have set myPassw0rd password\ncat .db-pass\n$1$Nyql5zO2$H9e0n0m44qJ8xQfPt79Pn0\n\nCreate mariaDB data directory to mount container database with /var/lib/mysql\nsudo mkdir -p data\n\n~/wordpress \n\t- data\n\nCreate and run Mariadb Docker container\n\nsudo docker run -d --network=wp-app -e MARIADB_ROOT_PASSWORD_HASH=/home/ubuntu/wordpress/.db-pass --restart unless-stopped -v &#039;/home/ubuntu/wordpress/data:/var/lib/mysql&#039; --name wp-mariadb mariadb\n\ncheck health and logs\ndocker ps\n\nsudo tail -f /var/lib/docker/containers/&lt;container-ID&gt;/&lt;container-ID&gt;-json.log\nexample:\nsudo tail -f /var/lib/docker/containers/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6/8c07234611094796605b37b5822255dcdd35aa4325e7729aa2ff5c8be6dcefa6-json.log\n\ncheck mounted volume for generated data\nls -1 data\n\nregarding migration of old database\ncopy the backup file into container db folder\ncp mywpold_db.sql wordpress/data/\n\nlogin to mariadb container\n\ndocker exec -it wp-mariadb bash\nrestore old db\ncd /var/lib/mysql\nmariadb -u wpuser123 -p new_db &lt; mywpold_db.sql;\nREFERENCES:\n\nkifarunix.com/how-to-deploy-wordpress-as-a-docker-container/\nmariadb.com/kb/en/moving-mariadb-to-docker-installation/#comment_3811\nmariadb.com/kb/en/backup-and-restore-overview/\nwww.reddit.com/r/docker/comments/tj8qp2/containerize_an_existing_wordpress_site/\n"},"Daily-Notes/2024-11-25":{"title":"2024-11-25","links":[],"tags":[],"content":"aws configure\naws cli\nto create new bucket\naws s3 mb s3://spiderman-cdec-devops123\nto list all buckets\naws s3 ls"},"Daily-Notes/2024-11-26-Cloudwatch-class":{"title":"2024-11-26 Cloudwatch class","links":[],"tags":[],"content":"cloudwatch\nafter installing nginx logs must be present\nsudo ls /var/log/nginx/\naccess.log  error.log\nto send these logs to cloudwatch\ncreate role\n\nattach role to instance\ninstall agent\ndocs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html\nenable agent service\nsystemctl\ncd /opt/aws\ncreate configuration file\nsudo /opt/aws/amazon-cloudwatch-agent/b\nin/amazon-cloudwatch-agent-config-wizard\nLog file path:\n/var/log/nginx/access.log\nconfig.json generated in bin folder\nautorized vs authenticated read\n\n./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json\nrecap\n\nCreated ec2-instance\ninstall nginx\n\nec2-describe permission\ncollectd file create\n"},"Daily-Notes/2024-11-26":{"title":"2024-11-26","links":[],"tags":[],"content":"cloudwatch\nafter installing nginx logs must be present\nsudo ls /var/log/nginx/\naccess.log  error.log\nto send these logs to cloudwatch\ncreate role\n\nattach role to instance\ninstall agent\ndocs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html\nenable agent service\nsystemctl\ncd /opt/aws\ncreate configuration file\nsudo /opt/aws/amazon-cloudwatch-agent/b\nin/amazon-cloudwatch-agent-config-wizard\nLog file path:\n/var/log/nginx/access.log\nconfig.json generated in bin folder\nautorized vs authenticated read\n\n./amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:config.json\nrecap\n\nCreated ec2-instance\ninstall nginx\n\nec2-describe permission\ncollectd file create\n"},"Daily-Notes/2024-11-27":{"title":"2024-11-27","links":[],"tags":[],"content":"sns - notification service\nattach filter before creating alarm\ncreate alarm\nget graph\nget notification"},"Daily-Notes/2024-11-28-Creating-RDS":{"title":"2024-11-28 Creating RDS","links":[],"tags":[],"content":"Creating RDS database\nStep 1: Create Database\n\n\nGoto RDS dashboard and click on Create Database\nSelect Standard create for database creation method\nI chose MariaDB engine\nEngine Version selected n-1 that is one step behind the latest one\nSelect Free Tier Template\nI leave database-1 as DB instance identifier name\nSelf Managed Credential Management\nand inputed desired Master Password i.e Passw0rd123\nIn instance configuration I selected Burstable classes for DB instance class\nand selected db.t3.micro\nfor storage I selected General Purpose SSD gp2 and allocated 20GB of storage\nI left default setting as it is and clicked on Create Database\n\n\nStep 2: Configure Instance for Mariadb\n\n\nLaunch ec2 instance\nI gave name of ec2 instance db-test\nSelected Amazon linux free tier\nand added security group for 3306 port\n[make sure to have same security group for both ec2 and rds]\nClicked on create instance\nLog into instance\n\nStep 3: Connect to RDS via ec2\n\nAfter login to ec2 install mariadb client\n\nyum install mariadb105\n\nLogin to endpoint of RDS we just created\n\nmysql -h &lt;hostname&gt; -u &lt;user&gt; -p\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\nInput the password and congrats you just logged into your fresh sql database.\n\n\n\n                  \n                  Scratchpad\n                  \n                \n\nHA - high availability\nn-1\n3306 maridb sql\nsudo yum list\ninstalling mariadb client\nyum install mariadb105\n\ndiagram\ndefault vpc\n3 public subnet\n1 subnet ⇒ ec2\n2 subnet ⇒ rds\nendpoint access\nmysql -h  -u  -p&gt;\n\n\ncreate database demo;\n \nshow database;\n \nuse demo;\n \ndrop database demo;\n \nCREATE TABLE Persons (  \n    PersonID int,  \n    LastName varchar(255),  \n    FirstName varchar(255),  \n    Address varchar(255),  \n    City varchar(255)  \n);\n \ndesc Persons;\n \nInsert Into Persons (PersonID, LastName, FirstName) values (1, &quot;DemoName&quot;, &quot;Amir&quot;);\n \nselect * from Persons;\n \nwww.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql\n \n1. CREATE USER &#039; ==sammy==&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;==password==&#039;;\n \ngrant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;\n \ngrant select on demo.* to &quot;sammy&quot;@&quot;%&quot;;           \n \nshow grants for \n \ninstall tomcat\nbuild application\n\n\ncreate user\nCREATE USER &#039;sammy&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;password&#039;;\n\nshow user\nSELECT User, Host FROM mysql.user;\n"},"Daily-Notes/2024-11-28":{"title":"2024-11-28","links":[],"tags":[],"content":"Creating RDS database\nStep 1: Create Database\n\n\nGoto RDS dashboard and click on Create Database\nSelect Standard create for database creation method\nI chose MariaDB engine\nEngine Version selected n-1 that is one step behind the latest one\nSelect Free Tier Template\nI leave database-1 as DB instance identifier name\nSelf Managed Credential Management\nand inputed desired Master Password i.e Passw0rd123\nIn instance configuration I selected Burstable classes for DB instance class\nand selected db.t3.micro\nfor storage I selected General Purpose SSD gp2 and allocated 20GB of storage\nI left default setting as it is and clicked on Create Database\n\n\nStep 2: Configure Instance for Mariadb\n\n\nLaunch ec2 instance\nI gave name of ec2 instance db-test\nSelected Amazon linux free tier\nand added security group for 3306 port\n[make sure to have same security group for both ec2 and rds]\nClicked on create instance\nLog into instance\n\nStep 3: Connect to RDS via ec2\n\nAfter login to ec2 install mariadb client\n\nyum install mariadb105\n\nLogin to endpoint of RDS we just created\n\nmysql -h &lt;hostname&gt; -u &lt;user&gt; -p\nmysql -h database-1.czsumoyo8fx5.us-west-2.rds.amazonaws.com -u admin -p\nInput the password and congrats you just logged into your fresh sql database.\n\n\n\n                  \n                  Scratchpad\n                  \n                \n\nHA - high availability\nn-1\n3306 maridb sql\nsudo yum list\ninstalling mariadb client\nyum install mariadb105\n\ndiagram\ndefault vpc\n3 public subnet\n1 subnet ⇒ ec2\n2 subnet ⇒ rds\nendpoint access\nmysql -h  -u  -p&gt;\n\n\ncreate database demo;\n \nshow database;\n \nuse demo;\n \ndrop database demo;\n \nCREATE TABLE Persons (  \n    PersonID int,  \n    LastName varchar(255),  \n    FirstName varchar(255),  \n    Address varchar(255),  \n    City varchar(255)  \n);\n \ndesc Persons;\n \nInsert Into Persons (PersonID, LastName, FirstName) values (1, &quot;DemoName&quot;, &quot;Amir&quot;);\n \nselect * from Persons;\n \nwww.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql\n \n1. CREATE USER &#039; ==sammy==&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;==password==&#039;;\n \ngrant create , alter drop  insert update delete select on demo.persons to sammy@&amp;;\n \ngrant select on demo.* to &quot;sammy&quot;@&quot;%&quot;;           \n \nshow grants for \n \ninstall tomcat\nbuild application\n\n\ncreate user\nCREATE USER &#039;sammy&#039;@&#039;localhost&#039; IDENTIFIED BY &#039;password&#039;;\n\nshow user\nSELECT User, Host FROM mysql.user;\n"},"Daily-Notes/2024-11-30":{"title":"2024-11-30","links":["Daily-Notes/2024-12-02"],"tags":[],"content":"building tomcat from source\ndownload zip file\nunzip it\nsudo yum install java-17\n\n\n                  \n                  Info\n                  \n                \n\nin ubuntu java packages are called\njdk and jre\njava development kit and java runtime\n\n\ncd into extracted folder\ncd bin\nbash catalina.sh start\ngit clone github.com/cholekulche/BE-application-student.git\ncd into it\ninstall maven\nsudo yum install maven\nbuild appliaction\nmvn clean package\n\n\n                  \n                  Info\n                  \n                \n\nfirst install java before maven because\nmaven will install java latest version which might conflict with required versions\n\n\ncd target\ncode\nartifact application\nmv studentapp-2.2-SNAPSHOT.war studentapp.war\nmv studentapp.war ../../apache-tomcat-9.0.97/webapps/.\ncd ../../apache-tomcat-9.0.97/webapps/\n.war file will get extracted automatically into student app\ncheck ip:8080/studentapp\n\ncontinue to 2024-12-02"},"Daily-Notes/2024-12-02":{"title":"2024-12-02","links":["Daily-Notes/2024-11-30"],"tags":[],"content":"continued from 2024-11-30\ncreate database\ncopy whole command\nshow database;\nuse studentapp;\nshow tables;\nConnecting application with database\ncd apache-tomcat/conf\nvi context.xml\nedit jdbc connector\n&lt;Resource Name ... &gt;\n\t\t  edit stuffs\n\t\t  adding rds hosting etc\n\nurl/mysql-connector.jar\nput it into tomcat/lib/\ncurl -O url\nbash catalina stop\nbash catalina start\nrestart catalina\ntry filling form\nif get error check catalina*.log\nbuild another application\npritam sir repo student-ui app\nadding domain\nroute53"},"Daily-Notes/2024-12-03":{"title":"2024-12-03","links":[],"tags":[],"content":"Route 53\nip to name\nname to ip\ndns record max limit 10k\nroute53 &gt;&gt; hosted zone &gt;&gt; create\nupdate nameserver in godaddy\n\n\nbefore updating nameserver\n\n\ncreate s3 bucket\nenable static web hosting add files\nroute53 &gt;&gt; hosted zone &gt;&gt; domain name &gt;&gt; create records\nskip subdomain select A records\nalias on\nalias to s3 bucket endpoint\nregion select\nskip s3 bucket part\ncreate ec2 instance\ninstall nginx\nmake site available\nadd ip in values in create record option\nlearndevops.live\nsearch acm service in aws\ncertificate manager\ncreate\ncreate records in r53\nrouting policy\nsimple routing"},"Daily-Notes/2024-12-04":{"title":"2024-12-04","links":[],"tags":[],"content":"\ndeploy tomcat instance ⇒ no database attach\nmake ami of it\nautoscaling with load balancer\nmount efs which will have tomcat\ntwo instances\n\n\nnew task implement failover policy on nginx instance\nTo read ⇒ blue green deployment strategy\nmanually implement it on instance\nToread — &gt;canary deployment"},"Daily-Notes/2024-12-07":{"title":"2024-12-07","links":[],"tags":[],"content":"Cloud Front\ncache\nCDN - content delivery network\nx86 and arm\ndomain\nname to ip \nip to name\n\n.in .com — &gt; registrar\ndoc.aws.amazon.com  ⇒ it reads from right to left\nTask: read how url works\nlatency - time between request and response\nedge location (cdn location)"},"Daily-Notes/2024-12-10":{"title":"2024-12-10","links":[],"tags":[],"content":"cloudfront vs cloudwatch\ncloudfront\ncdn\ncache\ncloudwatch\nlogs monitoring tool\ns3\nloadbalancer\nautoscaling\n\nlambda\nserverless ⇒ managed by aws\nlambda function to start and stop ec2 instances\nboto3 library"},"Daily-Notes/2024-12-11-AWS-Task":{"title":"2024-12-11 AWS Task","links":[],"tags":[],"content":"End of the AWS Task\nRequirements:\n\n Make new user\n attach policy according to task\n granular level permission\n\nTask\nuser will create s3 bucket, instance, db, \ninstance will have:\n\ttomcat link to rds\n\tits logs goes to s3 bucket\n\ttomcat will be accessible via domain and not via instance ip\nrouting policy simple\nuser cant have admin access \nuser cant access iam\n\nTask done\nStep1 - Create Iam user\nCreated custom Policy policy4tom and added permission for ec2 full access\n{\n\t&quot;Version&quot;: &quot;2012-10-17&quot;,\n\t&quot;Statement&quot;: [\n\t\t{\n\t\t\t&quot;Sid&quot;: &quot;VisualEditor0&quot;,\n\t\t\t&quot;Effect&quot;: &quot;Allow&quot;,\n\t\t\t&quot;Action&quot;: [\n\t\t\t\t&quot;rds:*&quot;,\n\t\t\t\t&quot;s3:*&quot;,\n\t\t\t\t&quot;ec2:*&quot;\n\t\t\t],\n\t\t\t&quot;Resource&quot;: &quot;*&quot;\n\t\t}\n\t]\n}\nand set permission boundary for Administrator access\n\nCreated EC2 Instance with security group rule for port 8080 enabled\nand added this user data\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip\nunzip apache-tomcat-9.0.97.zip\nyum install java-17 -y \ncd apache-tomcat-9.0.97/bin/\nbash ./catalina.sh start \nafter running instance check if tomcat page shows in port 8080\n\nCreate s3 bucket"},"Daily-Notes/2024-12-11":{"title":"2024-12-11","links":[],"tags":[],"content":"End of the AWS Task\nRequirements:\n\n Make new user\n attach policy according to task\n granular level permission\n\nTask\nuser will create s3 bucket, instance, db, \ninstance will have:\n\ttomcat link to rds\n\tits logs goes to s3 bucket\n\ttomcat will be accessible via domain and not via instance ip\nrouting policy simple\nuser cant have admin access \nuser cant access iam\n\nTask done\nStep1 - Create Iam user\nCreated custom Policy policy4tom and added permission for ec2 full access\n{\n\t&quot;Version&quot;: &quot;2012-10-17&quot;,\n\t&quot;Statement&quot;: [\n\t\t{\n\t\t\t&quot;Sid&quot;: &quot;VisualEditor0&quot;,\n\t\t\t&quot;Effect&quot;: &quot;Allow&quot;,\n\t\t\t&quot;Action&quot;: [\n\t\t\t\t&quot;rds:*&quot;,\n\t\t\t\t&quot;s3:*&quot;,\n\t\t\t\t&quot;ec2:*&quot;\n\t\t\t],\n\t\t\t&quot;Resource&quot;: &quot;*&quot;\n\t\t}\n\t]\n}\nand set permission boundary for Administrator access\n\nCreated EC2 Instance with security group rule for port 8080 enabled\nand added this user data\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.zip\nunzip apache-tomcat-9.0.97.zip\nyum install java-17 -y \ncd apache-tomcat-9.0.97/bin/\nbash ./catalina.sh start \nafter running instance check if tomcat page shows in port 8080\n\nCreate s3 bucket"},"Daily-Notes/2024-12-14":{"title":"2024-12-14","links":[],"tags":[],"content":"aws exam"},"Daily-Notes/2024-12-17":{"title":"2024-12-17","links":[],"tags":[],"content":"SDLC\nAgile\nwaterfall"},"Daily-Notes/2024-12-18":{"title":"2024-12-18","links":[],"tags":[],"content":"Git Theory Explained"},"Daily-Notes/2024-12-19-Git-Class":{"title":"2024-12-19 Git Class","links":["DevOps/Notes/Git-Theory-and-Practical","Some-talks-about-git-in-reddit-1"],"tags":[],"content":"DVCS vs CVS — further expanded in Git Theory and Practical\nDecentralized Version Control System\n\nCentralized Versioning control system\n\nInstructions\n\ninstalling git\nsignup to github.com\n\n\nPractice Performed\n\ngit add\ngit commit\ngit push\ngit pull\n\n\nExtra stuff I learned\nGoing back to previous commit to and rebasing\nSeeing git logs\ngit log\nStart interactive rebase\ngit rebase -i &lt;commit hash&gt;\nIn the editor, choose to edit the commit identified in step 3 by changing pick to edit on the first line of the text.\nedit 8728dbe67 my second commit message\npick 03d69e5d3 my third commit message\npick 8053f7b27 my fourth commit message\nmake changes and git add .\n\ngit commit --amend\ngit rebase --continue\ngit push\n\nremoving some files from git index\ngit rm --cached -r folder or files\nLearned to create .gitignore file\n.trash/\n.obsidian/\nprivate\n\nReferences\n\nRemoving secret introduced by earler commit\nHow to ignore files in git\n\nExtra Stuff to read about git\n\njvns.ca/blog/2024/01/26/inside-git/\ngithowto.com/git_internals_git_directory\nSome talks about git in reddit 1\ngit-scm.com/book/en/v2\n"},"Daily-Notes/2024-12-23-docker-day":{"title":"2024-12-23 docker-day","links":["Drawings/Docker-Architecture-diagram_1.excalidraw"],"tags":[],"content":"\nBefore Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture\nMonolithic vs Microservice Apps\nMonolithic Application\n\nSingle unified application\n\nMicroservice app\n\none app divided in chunks\n\nWhat is docker?\nTransclude of Docker-Architecture-diagram_1.excalidraw\nWhat is container?\n\nits like small virtual machines\ncontainers are free to use all the rams they needs\nwhereas in vms softwares has limited prealloted ram to use\n\nReferences:\n\nwww.geeksforgeeks.org/monolithic-vs-microservices-architecture/\nadventofdocker.com\n"},"Daily-Notes/2024-12-24-docker-basics":{"title":"2024-12-24 docker basics","links":[],"tags":[],"content":"\ndocker client -⇒ docker commands\ndocker machine -⇒ docker host\nubuntu package name docker.i\nin amazon-linux\nyum install docker -y\nenable daemon\nsystemctl enable --now docker\ndocker pull nginx\n\n\n                  \n                  Tip\n                  \n                \n\nImages can also be pulled from SHA Id.\n\n\ndocker run nginx\n-d detached mode\nps list running containers\n-a list all containers stopped and running\nrm remove stopped containers\nkill kill the containers\n-P expose container on random host port"},"Daily-Notes/2024-12-24":{"title":"2024-12-24","links":[],"tags":[],"content":"\ndocker client -⇒ docker commands\ndocker machine -⇒ docker host\nubuntu package name docker.i\nin amazon-linux\nyum install docker -y\nenable daemon\nsystemctl enable --now docker\ndocker pull nginx\n\n\n                  \n                  Tip\n                  \n                \n\nImages can also be pulled from SHA Id.\n\n\ndocker run nginx\n-d detached mode\nps list running containers\n-a list all containers stopped and running\nrm remove stopped containers\nkill kill the containers\n-P expose container on random host port"},"Daily-Notes/2024-12-30":{"title":"2024-12-30","links":["DevOps/Tasks-Done/Task-4-Create-daemon-service-of-tomcat","DevOps/For-Reading/read-about-docker.sock-file"],"tags":[],"content":"Task 1: Task 4 Create daemon service of tomcat\nread about docker.sock file\npersistence storage\ndocker volume\ndocker volume ls\ncreating docker volume\ndocker volume create dev\nRoot directory of docker\ncd /var/lib/docker/\nall systemd service files lies in\n/etc/systemd/system/\nconfiguration files\n/etc/docker\nsocket file\n/var/run/docker.sock\ndocker run -d -P -v dev:/usr/share/nginx/html nginx\nTommorow\ndockerfile\nnetwork"},"Daily-Notes/2024-12-31-Dockerfile-Explained":{"title":"2024-12-31 Dockerfile Explained","links":["DevOps/Tasks-Done/Task-5-Create-dockerfiles"],"tags":[],"content":"Dockerfile Explained\nWhat is a dockerfile?\n\nStep by step instructions to build Image\nBlueprint of a container\nWritten in go\nHas Instructions\n\nSome Basic Instructions are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructionsUse CaseFROMSpecifies the base image to start with. example: nginx or ubuntuCOPYCopies files or directories from your computer into the image.RUNExecutes commands within the image during the build processADDSimilar to COPY, but can also extract compressed files and download from URLs.EXPOSEInforms Docker that the container listens on a specific network port at runtime.CMDSpecifies the command to run when the container starts.ENTRYPOINTMain command that always executes when the container starts.\nThere are two ways to run command in container.\n\nUsing CMD instruction\nUsing Entrypoint\n\nCMD\n\nPurpose: Defines the default command to execute when the container starts.\nFlexibility: Easily overridden by using the docker run command with a different command.\nExample: CMD [&quot;python&quot;, &quot;app.py&quot;]\n\nENTRYPOINT\n\nPurpose: Configures the container to run as an executable. It sets the main command that always executes.\nLess Flexible: While arguments can be appended, the core command defined by ENTRYPOINT remains.\nExample: ENTRYPOINT [&quot;/usr/bin/nginx&quot;]\n\nKey Differences and Use Cases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureCMDENTRYPOINTOverride BehaviorEasily overridden by docker runNot easily overridden; arguments can be appendedExecutionRuns after ENTRYPOINT if both are definedRuns first, effectively becoming the container’s primary executableTypical Use CasesProviding a default command, which can be changed if neededSetting a fixed executable for the container, like a web server\nEntrypoint file can be single command or shell script.\nSimple example of entrypoint script is this:\nEntrypoint.sh\n#!/bin/bash\n \n# Check if a configuration file exists\nif [ ! -f /app/config.yml ]; then\n  echo &quot;Error: config.yml not found!&quot;\n  exit 1\nfi\n \n# Start the application\nexec python /app/app.py\nIn your Dockerfile, you would use:\nDockerfile\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\nENTRYPOINT [&quot;/entrypoint.sh&quot;]\nExample of Dockerfile using command for entrypoint\nDockerfile\nFROM ubuntu:latest\n \n# Install Apache\nRUN apt-get update &amp;&amp; apt-get install -y apache2\n \n# Set the entrypoint to the Apache server\nENTRYPOINT [&quot;apache2ctl&quot;]\n \n# Default command to start Apache in the foreground\nCMD [&quot;-D&quot;, &quot;FOREGROUND&quot;] \nIn this example, ENTRYPOINT [&quot;apache2ctl&quot;] configures the container to run the Apache web server as its main process. When you start a container from this image, it will always execute apache2ctl.\n\nHomework\n\nFollow on Task 5 Create dockerfiles\n"},"Daily-Notes/2025-01-02-Multistage-Dockerfile":{"title":"2025-01-02 Multistage Dockerfile","links":[],"tags":[],"content":"Multistage Dockerfile\nTraditional Method\nFROM amazonlinux:latest\nRUN yum update &amp;&amp; yum install java-17 unzip -y\ncopy tomcat /tomcat/.\nRUN chmod +x /opt/apache-tomcat-9.0.98/bin/catalina.sh \nEXPOSE 8080\nCOPY studentui studentui/.\nWORKDIR /studentui\nRUN mvn clean package\nRUN mv target/*.war /tomcat/webapps/student.war\nCMD [&quot;/tomcat/bin/catalina.sh&quot;, &quot;run&quot;]\nMultistage Method\nFROM maven:3.9.9-amazoncorretto-8-debian-bookworm AS builder\nCOPY student-ui /.\nRUN mvn clean package     # build in target/*.war\n \nFROM tomcat:jre8:alpine\nCOPY --from=builder /target/*.war webapps/student.war"},"Daily-Notes/2025-01-06":{"title":"2025-01-06","links":["DevOps/Notes/Kubernetes-Architecture","DevOps/For-Reading/Read-about-each-and-every-component-of-kubernetes"],"tags":[],"content":"Kubernetes Architecture\n\ncluster — group of identical instance\nresources — pods\nnode — instance\nEKS\ncontrol plane / master\netcd = cluster database   name of the master, cluster etc\napi server = communication among cluster components, recieves and forwards information\nscheduler = scheduling pods\n pod = smallest unit of kubernetes, jacket like structure to container\n kubeproxy = for networking\n kubelet = sends information of node to api server also sends heartbeat\n controller = many controllers, manages pods and containers.\n\nbare minimum criteria for\n0.25m 6.25core\nGB     Gi\n1024   1000\ncontainerd\nkubeadm\nRead about each and every component of kubernetes"},"Daily-Notes/2025-01-07-kubernetes":{"title":"2025-01-07 kubernetes","links":["DevOps/Notes/Kubernetes-Architecture","DevOps/Tasks-Done/Task-6-How-to-set-permanent-alias","Task-7-Create-LAMP-Server"],"tags":[],"content":"Kubernetes Architecture\n\nCreating cluster on aws\nPrerequisites\n\nkubectl\neksctl\naws\n\nSteps\n\ncreate ec2 instance\nadd admin role on it\ninstall Prerequisites on it\n\nInstall kubectl binary from official website\n# for x86_64\ncurl -LO &quot;dl.k8s.io/release/$(curl -L -s dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;\n \n# for arm64\ncurl -LO &quot;dl.k8s.io/release/$(curl -L -s dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl&quot;\nInstall eksctl binary from its official website\n# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`\nARCH=amd64\nPLATFORM=$(uname -s)_$ARCH\n \ncurl -sLO &quot;github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz&quot;\n \n# (Optional) Verify checksum\ncurl -sL &quot;github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt&quot; | grep $PLATFORM | sha256sum --check\n \ntar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz\n \nsudo mv /tmp/eksctl /usr/local/bin\nWrite this script into script.sh file and run it with bash script.sh\nCheck if commands working properly:\n# see kubectl version\nkubectl version\n \n# see eksctl version\neksctl version\nand run Create Cluster command\neksctl create cluster --name clustername --node-type t2.medium --nodes 2 --region=us-east-1\nTo See nodes\nkubectl get nodes\nTo See pods\nkubectl get pods -A\nwrite pod.yaml manifest and apply\nkubectl apply -f pod.yaml\nget info about pods\nkubectl describe pod nginx\nRead about:\nManifest\nYAML\n\nPOD\nRepl set\nDeployments\nServices\n\tLoad balancer\n\tNode pod\n\tcluster ip\nconfig\n\tsecrets\n\tconfigMap\nNamespace\nResources\nResource Quota\nVolume\nIngress\nHPA\n\nPod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - image:\n    name:\n    ports:\n      container-port: 80\ns comes - comes\n- means array\ncluster version updates comes in every 4 month\nminimum n-2 version compatible for client to support with latest version of kubernetes\n\nTasks\n\nTask 6 How to set permanent alias\nTask 7 Create LAMP Server\n\nSome Useful Ai for Research\nwww.perplexity.ai/\nclaude.ai/\nchatgpt.com"},"Daily-Notes/2025-01-08":{"title":"2025-01-08","links":[],"tags":[],"content":"🔦Lights out"},"Daily-Notes/2025-01-09-k8s-manifest-files":{"title":"2025-01-09 k8s manifest files","links":[],"tags":[],"content":"ReplicaSets\nThe ReplicaSet configuration defines a number of identical pods required, and if a pod is evicted or fails, creates more pods to compensate for the loss.\nReplicaSets are usually not used directly—they are typically created as part of a deployment.\nHere is a simple example using matchLabels as the selector.\nrs.yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontendn\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\nApply Manifest kubectl apply -f rs.yaml\nNote: use either one file naming yml or yaml\nDeployment\nDeployments are one level up from ReplicaSets. They allow you to use a declarative method to deploy ReplicaSets and pods. You use YAML configuration to define what your group of pods should look like, and the deployment manipulates Kubernetes objects to create pods exactly according to the YAML specification.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontendn\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\ndeployment ----&gt; ReplicaSet -------&gt; Pods\n\nServices\nIn a Kubernetes cluster, each Pod has an internal IP address. But the Pods in a Deployment come and go, and their IP addresses change. So it doesn’t make sense to use Pod IP addresses directly. With a Service, you get a stable IP address that lasts for the life of the Service, even as the IP addresses of the member Pods change.\nA key aim of Services in Kubernetes is that you don’t need to modify your existing application to use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you’ve containerized. You use a Service to make that set of Pods available on the network so that clients can interact with it.\nTypes of Kubernetes Services\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypesShort CodesDescriptionLoadBalancerlbClients send requests to the IP address of a network load balancer.NodePortnpClients send requests to the IP address of a node on one or more nodePort values that are specified by the Service.ClusterIpcipInternal clients send requests to a stable internal IP address.ExternalNamexnInternal clients use the DNS name of a Service as an alias for an external DNS name.Headless-You can use a headless service when you want a Pod grouping, but don’t need a stable IP address.\n\n\n                  \n                  Hot Tip \n                  \n                \n\nIts not possible to remember all shortcodes for k8’s Resources\nThats why we have this command to Lists all the resources in cluster\nkubectl api-resources\n\n\nTwo ways to use Kubernetes\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperative WayDeclarative WaycommandsManifest"},"Daily-Notes/2025-01-10-Service-Types":{"title":"2025-01-10 Service Types","links":[],"tags":[],"content":"max surge / max unavailable\nrolling updates\naws s3 ls —profile dev\naws configure —profile\nService\n\nClusterIP\n\ndefault service\n\n\n\nService.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app.kubernetes.io/name: proxy\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    ports:\n      - containerPort: 80\n        name: http-web-svc\n \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app.kubernetes.io/name: proxy\n  ports:\n  - name: name-of-service-port\n    protocol: TCP\n    port: 80\n    targetPort: http-web-svc\nspec:\ntype: LoadBalancer\nannotations to change service type\ncan service lb me classic use kr skte ho kya?\nspec:\ntype: NodePort\nkubectl get -o wide\nget more information\n✅ Tasks To read\n\nminimum 5 points in services types\nsecrets and ConfigMap\n"},"Daily-Notes/2025-01-13":{"title":"2025-01-13","links":["DevOps/For-Reading/k8s-Namespace","DevOps/For-Reading/k8s-Volume"],"tags":[],"content":"ConfigMap - non confidential information\nSecrets - confidential information\nConfigMap\nCreation through Imperative way\nCreate configMap Command\nkubectl create cm nginx-cm --from-file=index.html\ngenerate configmap definition file\nkubectl create cm nginx-cm --from-file=index.html --dry-run=client -o yaml &gt; configmap.yaml\n\nEnter into pod shell\nkubectl exec -it mypod --bash\nPort forward\nkubectl port-forward service/myservice 80:80\nSecrets\nThree types of secret\n\ndocker-registry : Create a secret for use with a Docker registry\ngeneric : Create a secret from a local file, directory, or literal value\ntls : Create a TLS secret\n\nGenerate Secret.yaml file\nkubectl create secret generic --from-literal=idli=chutney --dry-run=client -o yaml &gt; secret.yaml\nDecode secret\necho &quot;Y2hhdG55&quot; | base64 --decode\nExpose pod external port\nkubectl expose pod nginx-secret --port=80 -o yaml --dry-run=client &gt; svc.yaml\n\nTommorrow:\n\nk8s Namespace\nk8s Volume\n"},"Daily-Notes/2025-01-14":{"title":"2025-01-14","links":[],"tags":[],"content":"Volumes\nenv\nNamespace (Logical division)\nCreate namespace\nkubectl create ns prod --dry-run=client -o yaml &gt; ns.yaml\n\nnamespace creates on across the cluster\nPersistence Volume\n\nhost\naddons (realtime use) ⇒ CSI Drivers EBS volume\n\nCSI ebs drivers\npod\npersistence volume claim\nservice account\nstorage account\nkubectl get sa\n"},"Daily-Notes/2025-01-20":{"title":"2025-01-20","links":[],"tags":[],"content":"Not from class\nDifference between POD, Deployment and container\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContainerPodDeploymentHas docker filehas yaml fileyamlcommand docker run nginxkubectl create -f pod.yamlkubectl create -f deployment.yaml\nController: - Ensures desired state is always present in the cluster.\nPod:- A wrapper around container that maintain desired state.\nReplicaSet - A controller that carries autohealing behaviour.\nLogin to kubernetes cluster with minkube\nminikube ssh\nDeplyoyment - Is an abstraction.\nController is golang application which ensures specific behaviour is implemented.\neg. - ReplicaSet behaviour ensures desired replicas are always present.\nAutohealing - spawns the containers with different ip addresses\nhence we create service\nsvc ---- 1. load balancer\n|-------- 2. Service Discovery\n|                    |------- labels and selector\n|-------- 3. expsoing to the world\nautohealing without service ----\ndeployment\npod ---- 172.16.3.4     —⇒ user 1\nrs1----     172.16.3.5  —⇒ user 2\nrs2  ---    172.16.3.6 —⇒ user 3\nbut if one pod gets deleted auto healing will create another pod with different ip address and user not knowing new ip address unable to access it\nhence service comes in handy\nService (eg. load balancer) will have single ip and all users will get ip of service and service will distribute traffic among pods and rs.\nbut when pods gets respawn with new ip how does service knows its ip\nfor that service has this feature called service discovery\nhow service does service discovery\nunlike manually keeping track of ip addresses which can change everytime service use labels and selectors mechanism.\nservice maintains service discovery through labels and selectors.\nlabel will be common for all the app and service will only be watching for the labels.\nService is also use for exposing your application to outside world.\nservice is allow application to access outiside k8s cluster.\nYou can create service of 3 types\nService.YAML\n|_ _ _ _ _ 1. Cluster IP (inside accessible) [ you get discovery and lb]\n|_ _ _ _ _ 2. Node Port (inside organisation or network\\ access worker node )\n|_ _ _ _ _ 3. Load balancer ( service is accessible to external world)\nload balancer only work for cloud based k8s like EKS\nload balancer —⇒ amazon.com\nNode Port -⇒ VPC and nodes\nCLUSTER IP -⇒ Cluster Network"},"Daily-Notes/2025-01-21-Creating-PVC":{"title":"2025-01-21 Creating PVC","links":[],"tags":[],"content":"Persistence Volume Claim\nStep 1: Create ec2 instance\n\nAdd admin role (administrator access)\n\nStep 2: Login to ec2 instance\nInstall kubectl package\ncurl -LO &quot;dl.k8s.io/release/$(curl -L -s dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;\nand eksctl\nARCH=amd64\nPLATFORM=$(uname -s)_$ARCH\n \ncurl -sLO &quot;github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz&quot;\n \n# (Optional) Verify checksum\ncurl -sL &quot;github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt&quot; | grep $PLATFORM | sha256sum --check\n \ntar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz\n \nsudo mv /tmp/eksctl /usr/local/bin\nWrite this script into script.sh file and run it with bash script.sh\nCheck if commands working properly:\n# see kubectl version\nkubectl version\n \n# see eksctl version\neksctl version\nStep 3: Create cluster\neksctl create cluster --name clustername --node-type t2.medium --nodes 2 --region=us-east-1\nStep 4: Set up AWS EBS storage integration with your EKS Cluster\nStep 1: Get OIDC ID from cluster\noidc_id=$(aws eks describe-cluster --name irondome --region=us-west-2 --query &quot;cluster.identity.oidc.issuer&quot; --output text | cut -d &#039;/&#039; -f 5)\necho $oidc_id\n\nYou will get something like this: 04B9B8B86AAB8AF93259FBD40DFF0D4C\n☠️Dont copy it it just for illustration.☠️\nStep 2: Find and extract the OpenID Connect (OIDC) provider ID for an EKS cluster\naws iam list-open-id-connect-providers --region=us-west-2 | grep $oidc_id | cut -d &quot;/&quot; -f4 \n\nStep 3: Creates an IAM OIDC (OpenID Connect) identity provider for your EKS cluster\neksctl utils associate-iam-oidc-provider --cluster irondome --approve --region=us-west-2\n\nStep 4: Create an IAM role that allows the EBS CSI Driver to manage EBS volumes for your EKS cluster\neksctl create iamserviceaccount \\\n        --name ebs-csi-controller-sa \\\n        --namespace kube-system \\\n        --cluster irondome \\\n        --role-name AmazonEKS_EBS_CSI_DriverRole \\\n        --role-only \\\n        --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n        --approve  --region=us-west-2\nStep 5: Create CSI Driver addon\nCheck what versions of the AWS EBS CSI Driver addon are available and compatible with Kubernetes version 1.30 in your region\neksctl utils describe-addon-versions --kubernetes-version 1.30 --region=us-west-2 --name aws-ebs-csi-driver\n\nInstall the EBS CSI Driver addon to your EKS cluster:\neksctl create addon --cluster irondome --name aws-ebs-csi-driver --version latest \\\n      --service-account-role-arn arn:aws:iam::970547378605:role/AmazonEKS_EBS_CSI_DriverRole --force --region=us-west-2\n\nStep 6: Create pods\nCreate a Pod definition file that:\n\nStart an nginx container\nMount an EBS volume (claimed through ebs-claim) at /data\nStore data persistently in the EBS volume even if the pod is deleted\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: ebs-claim\ncreate claim.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ebs-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: gp2\n  resources:\n    requests:\n      storage: 4Gi\nThis definition requests a 4GB persistent volume of type “gp2” (likely an EBS volume) that can be mounted by only one node at a time.\nPractical Walkthrough Video\ndrive.google.com/file/d/1lSwdeRuENgKLNIaqMpFCM9SosavuoSDb/view"},"Daily-Notes/2025-01-22-3-Microservice-Tasks":{"title":"2025-01-22 3-Microservice Tasks","links":[],"tags":[],"content":"Tasks:\n3 microservices in 3 different namespace\nnginx: dev-nginx\nhttpd: dev-httpd\ntomcat: dev-tomcat\nMicroservice 1\n\nCreate Docker Image nginx free css template\nBuild it and deploy to k8s\n\nMicroservice 2\n\nHttpd container\nMake deployment\nSend index.html via configMap\n\nMicroservice 3\n\nDeploy tomcat container\nMake deployment\n\nNginx ingress controller deploy on nginx-ingress namespace\nand 3 end points for 3 microservices\n&lt;subdomain.learndevops.live&gt;\nReading Task\n\nNode affinity\nAnti Affinity\nNode Selectors\nPod Affinity\nTaint and Tolerance\n\nPrevious task twist\n\n3 node\none pod on each node\nnode 1 ⇒ httpd\nnode 2 ⇒ tomcat\nnode 3 -⇒ nginx\n\nenvs.sh/gA-.yaml"},"Daily-Notes/2025-01-23-Tomcat-on-Single-Node-Task":{"title":"2025-01-23 Tomcat on Single Node Task","links":[],"tags":[],"content":"po\ndep\nrs\nclus\nip nodpo\nLB\nsecre\nconfigmap\nvolume\nnamespace\nhpa\npvc\nrolling upd\napplic lifecycle\nstateful set\ndaemon set\ntenant\npod affinity\nnode affinity\nnode selector\nanti afinity\ntaint and tolerance\nwhat is probe?\nTask\n\n1 Microservice Tomcat\nRS =1\nNode selector\npod affinity\nnode affinity\nImplement probe to check if service is running\nDatabase tomcat in Cluster with persistent volume\ningress\ntomcat app resource alloaction:\n\nRequest\nCPU: 500m\nMem: 1GB\nLimit\ncpu: 1\nmemory: 2gb\n\n\nhard coded values can be sent no secret will be used\n"},"Daily-Notes/2025-01-29":{"title":"2025-01-29","links":[],"tags":[],"content":"sudo yum install -y java-17-amazon-corretto\ngit code ⇒ build ⇒ push ⇒ deploy\ndevsecops ⇒ git checkout ⇒ git leaks scan ⇒ sonar scan ⇒ build ⇒ image scan / trivy ⇒ push ⇒ deploy\necr policy\nRe-run Jenkins with Docker Socket Mounted: When you run Jenkins in Docker, you’ll need to mount the host’s Docker socket into the Jenkins container. This allows the Jenkins container to communicate with the Docker daemon running on your local system.\nRun the following command to start the Jenkins container with Docker socket mounted:\ndocker run -d \\\n  -p 8080:8080 \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v jenkins_home:/var/jenkins_home \\\n  --name jenkins \\\n  jenkins/jenkins:latest\n\nHere’s what this does:\n-v /var/run/docker.sock:/var/run/docker.sock: This mounts the Docker socket from your host (/var/run/docker.sock) into the Jenkins container, allowing Jenkins inside the container to access Docker on the host.\n-v jenkins_home:/var/jenkins_home: This persists your Jenkins configuration and jobs, even if the container is removed.\n-p 8080:8080: Exposes the Jenkins UI on port 8080 of your host."},"Daily-Notes/2025-01-30":{"title":"2025-01-30","links":[],"tags":[],"content":"Jenkinsfile to push image into aws ecr\nagent any\n\n    stages {\n        stage(&#039;git checkout&#039;) {\n            steps {\n                checkout scmGit(branches: [[name: &#039;*/${BRANCH}&#039;]], extensions: [], userRemoteConfigs: [[credentialsId: &#039;git-user&#039;, url: &#039;github.com/cholekulche/BE-application-student.git&#039;]])\n            }\n        }\n        stage(&#039;build&#039;) {\n            steps {\n                sh &#039;sudo docker build -t student-ui .&#039;\n            }\n        }\n        stage(&#039;ecr push&#039;) {\n            steps {\n                sh &#039;&#039;&#039;\n                aws ecr get-login-password --region us-east-2 | sudo docker login --username AWS --password-stdin 970653867674.dkr.ecr.us-east-2.amazonaws.com\n                sudo docker tag student-ui:latest 970653867674.dkr.ecr.us-east-2.amazonaws.com/student-ui:${BRANCH}-${BUILD_TIMESTAMP}\n                sudo docker push 970653867674.dkr.ecr.us-east-2.amazonaws.com/student-ui:${BRANCH}-${BUILD_TIMESTAMP}\n                &#039;&#039;&#039;\n            }\n        }\n        stage(&#039;docker cleanup&#039;) {\n            steps {\n                sh &#039;sudo docker system prune -a -f&#039;\n            }\n        }\n    }\n}\n"},"Daily-Notes/2025-02-01":{"title":"2025-02-01","links":[],"tags":[],"content":"Docker in docker\nSonarcube\njenkins upgrade\nsonar cube = code coverage\n\nnot testing tool\ncode analysis and improvement\n\nsudo alternative —config java\nInstalling sonarcube\n\ninstall java-17\ninstall postgres\n\n\n maven plugin\n\n    - surefire\n\npom.xml file for sonarqube plugin"},"Daily-Notes/2025-02-02":{"title":"2025-02-02","links":[],"tags":[],"content":"🔥 Adding Docker-Based Jenkins Agents\nTo add Jenkins agents running as Docker containers, you need to:\n1️⃣ Install Docker in the Jenkins controller container.\n2️⃣ Install and configure the “Docker Plugin” in Jenkins.\n3️⃣ Configure Jenkins to launch Docker-based agents dynamically.\n🚀 Step 1: Run Jenkins with Docker Access\nFirst, ensure your Jenkins container can communicate with the Docker daemon. Run Jenkins with the Docker socket mounted:\ndocker run -d \\\n  -p 8080:8080 \\\n  -p 50000:50000 \\\n  --name jenkins \\\n  --user root \\\n  -v jenkins_home:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  jenkins/jenkins:lts\nNow, install Docker inside the Jenkins container:\ndocker exec -it jenkins bash\napt-get update &amp;&amp; apt-get install -y docker.io\nexit\n🔧 Step 2: Install the Docker Plugin in Jenkins\nOpen Jenkins at http://your-server-ip:8080/\n\nGo to Manage Jenkins → Manage Plugins\nIn the “Available” tab, search for “Docker Plugin” and install it.\nRestart Jenkins after installation.\n\n⚙️ Step 3: Configure Docker Cloud in Jenkins\nGo to Manage Jenkins → Manage Nodes and Clouds → Configure Clouds\nClick Add a new cloud → Select Docker\nIn &quot;Docker Host URI&quot;, enter:\n\nunix:///var/run/docker.sock\n\nClick Test Connection → It should be successful.\n\n🏗 Step 4: Create a Docker Agent Template\nUnder &quot;Docker Agent templates,&quot; click Add Docker Template\nSet:\n    Label: docker-agent\n    Docker Image: Use jenkins/inbound-agent (official Jenkins agent)\n    Remote File System Root: /home/jenkins\n    Usage: &quot;Use this node as much as possible&quot;\n    Launch Method: Select &quot;Attach Docker container&quot;\nClick Save.\n\n✅ Step 5: Run a Test Pipeline Using Docker Agents\nNow create a simple Jenkins Pipeline:\nGo to Jenkins Dashboard → New Item → Pipeline\nUse the following script:\npipeline {\n    agent { label &#039;docker-agent&#039; }\n    stages {\n        stage(&#039;Run on Docker Agent&#039;) {\n            steps {\n                sh &#039;echo &quot;Running on a Docker Agent!&quot;&#039;\n                sh &#039;docker --version&#039;\n            }\n        }\n    }\n}\n\nClick Build Now → Jenkins will automatically start a Docker agent and run the job inside it.\n🎯 Final Notes\n\nDynamic Agents: Jenkins will only spin up agents when needed and remove them after execution.\nIf agents don’t start: Check Manage Nodes and Clouds → Check agent logs.\nUsing Custom Images: You can specify a different agent image, e.g., python:3.10, maven:latest, etc.\n\n\nLet me know if you need more details! 🚀\nIt looks like Jenkins inside your container does not have the Docker CLI installed. Mounting the Docker socket (/var/run/docker.sock) allows the container to communicate with the Docker daemon running on your host, but the Jenkins container itself still needs the docker CLI to execute commands.\nSolution: Use a Docker-in-Docker (DinD) Approach\nInstead of using the basic jenkins/jenkins image, use jenkins/jenkins:lts with Docker installed or use a dedicated image like jenkins/jenkins:lts-jdk17 that comes with Docker.\nSteps to Fix\n1️⃣ Run Jenkins with Docker Installed\nInstead of jenkins/jenkins:latest, use jenkins/jenkins:lts and manually install Docker inside it.\ndocker run -d \\\n  -p 8080:8080 \\\n  -p 50000:50000 \\\n  --name jenkins \\\n  --user root \\\n  -v jenkins_home:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  jenkins/jenkins:lts\n2️⃣  Install Docker in the Jenkins Container\nOnce the container is running, enter it:\ndocker exec -it jenkins bash\nThen, install Docker inside the Jenkins container:\napt-get update &amp;&amp; apt-get install -y docker.io\n3️⃣ Verify Docker Inside Jenkins\nRun inside the container:\ndocker --version\ndocker ps\nIf these commands work, your Jenkins pipelines should now be able to execute docker commands successfully.\nAlternative: Use the jenkinsci/blueocean Image\nThe jenkinsci/blueocean image comes pre-installed with Docker CLI:\ndocker run -d \\\n  -p 8080:8080 \\\n  -p 50000:50000 \\\n  --name jenkins \\\n  --user root \\\n  -v jenkins_home:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  jenkinsci/blueocean\n \nThis might be an easier way if you don’t want to manually install Docker inside Jenkins.\nLet me know if you need more help! 🚀\nYou said:\nwhy   -p 50000:50000 \\\nChatGPT said:\nChatGPT\nThe -p 50000:50000 option in your Docker run command is used to expose the Jenkins agent (slave) communication port.\n🔹 Why is Port 50000 Used?\nJenkins uses port 50000 for JNLP-based (Java Network Launch Protocol) agent communication. This is needed when Jenkins connects to remote build agents (worker nodes) that run jobs.\n\nIf you plan to run Jenkins agents on different machines and connect them to your Jenkins controller, you should expose port 50000.\nIf you are running everything on a single container without agents, you don’t need -p 50000:50000.\n\n✅ When to Use -p 50000:50000\n✔️ If you want to add remote Jenkins agents.\n✔️ If you use JNLP (Java Web Start) agents to connect workers to Jenkins.\n❌ When You Can Skip It\n❌ If you are running all jobs inside the Jenkins container itself.\n❌ If you are using Docker-based agents instead of traditional JNLP agents.\n🛠 Example Without Agents (If You Don’t Need Port 50000)\ndocker run -d \\\n  -p 8080:8080 \\\n  --name jenkins \\\n  --user root \\\n  -v jenkins_home:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  jenkins/jenkins:lts"},"Daily-Notes/2025-02-03":{"title":"2025-02-03","links":[],"tags":[],"content":"Run using docker\n$ docker run -d --name sonarqube -p 9000:9000 sonarqube\nmvn clean verify sonar:sonar \\\n  -Dsonar.projectKey=student \\\n  -Dsonar.projectName=&#039;student&#039; \\\n  -Dsonar.host.url=http://3.137.177.165:32768 \\\n  -Dsonar.token=sqp_3584391716cf8fbc01eeea4e17d34b35b07d5a2f\n\ndocker run -d —privileged -P —user root  -v /mnt:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkins/jenkins\nnexus\njfrog\n→ s3"},"Daily-Notes/2025-02-04":{"title":"2025-02-04","links":[],"tags":[],"content":"Run sonarqube using docker\nartifactory to store artifact\n\nnexus\njfrog\ns3\n\naws-cli container\nstoring artifact in s3\ngithub pages - How to create free portfolio website using github pages\ncluster creates infra in its own way\nterraform gives complete control\nIAC = Infrastructure as a Code\nmain.tf\nprovider.tf\ncloud provider information\nvariable.tf\nterraform init\nprovider download\nterraform.lock.hcl created\nterraform plan\nwill show infra plan\nterraform apply\nterraform.tfstate created\ntf destroy\ndeletes all resources mentioned in tfstate file\nTask: create 3 ec2 instances with terraform"},"Daily-Notes/2025-02-17":{"title":"2025-02-17","links":[],"tags":[],"content":""},"DevOps/DockerTasks/Task-1-Creating-Tomcat-student-ui-container":{"title":"Task 1 Creating Tomcat student-ui container","links":[],"tags":["docker","devops","devops_tasks","containerisation","student-ui"],"content":"Task 1 - Creating Tomcat student-ui container\nInstructions\n\n Get the docker image of Amazon linux\n Add tomcat package\n Add student ui\n Then commit the image\n Store on ecr as well as on docker hub\n\nSteps Performed\nStep 1: Getting the the Amazon Linux Image\nUpon searching the for amazonlinux image in dockerhub I found its official image\nI pulled it into the system with docker pull amazonlinux\nsudo docker run -it -d -p 32768:8080 --name tomcat-student-ui amazonlinux`\nand I run the image interactively and in detached mode so i can execute shell command later on\n\nLets dive into the container\nsudo docker exec -it &lt;containerid&gt; &lt;shell-command&gt;`\n\nStep 2: Installing tomcat application\nOur App student-ui required specific version of tomcat thats why we gonna install it from source\nMake sure to install dependancies and unzip package before hand\nStep 3: Install tomcat from source\nInstall this specific version from source\ncd /opt # we can use this directory for temporary space\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip\nunzip apache-tomcat-9.0.98.zip # unzip the archive\nyum install java-17 -y # tomcat 9 requires java-17 to work properly\ncd apache-tomcat-9.0.98/bin/\nbash ./catalina.sh start \n\n\nIt looks like our tomcat server is up and running on desired port\nStep 4: Installing Student-ui\nbut first we need these package building tools git and maven for building our student-ui app\nInstalling git and maven\nsudo yum install git maven -y\n\nClone the student-ui repo\ngit clone github.com/Pritam-Khergade/student-ui\nbuild student-ui app using maven\ncd student-ui\nmvn clean package\n\nthis creates .war file in ./target folder\nrename it to suitable short name and move to /opt/apache-tomcat-9.0.98/webapps directory\nmv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war\nmv target/studentapp.war /opt/apache-tomcat-9.0.97/webapps/\nand the app should be accessible on tomcat server on http://instance-ip:32768/studentapp\n\nCreate the image out of this running container\nBefore creating the image its better to remove the unnecessary packages that we no longer need to make the size of the image minimal as possible.\nCleanup the no-longer needed packages to reduce size of image\nyum remove maven git unzip -y\nLets exit from container shell and build the image\nsudo docker commit &lt;container-id&gt;\n\nYou see the created image doesnt have any name so lets give it a tag\nsudo docker tag &lt;image-id&gt; &lt;newtag&gt;\n\nnow push it to docker hub and ECR\nUploading image to docker hub\nFirst create a repository at docker hub\n\nLogin to docker hub\nClick on repositories\nCreate new repository\nGive it proper name and click create\n\nHere is my repo looks llike\n\nLets push our image into this repo\nFirst rename add new tag to image appropriate according to docker hub repo name\nsudo docker tag &lt;old-tag-name&gt; &lt;newtag-name&gt;\n\nand now push it to docker hub\nsudo docker push archsarangx/tomcat-student-ui:latest\n\nand its successfully uploaded on docker hub at\n\nand anyone can pull it with\ndocker pull archsarangx/tomcat-student-ui:latest\nUploading image to ECR\nStep 1: Creating the repository at ECR\n\ngoto amazon ECR service and create repositoy\n\n\nthen click on blue repo name and click on view push commands\nStep 2: Authenticate with ECR\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 970547378605.dkr.ecr.us-west-2.amazonaws.com\nStep 2: Add Tag name to image\n“\ndocker tag archsarangx/tomcat-student-ui:latest 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest\ndocker push 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest\n\n\n🎉 And our Image is successfully uploaded on both ECR and docker hub.\nThank you for reading\nHave a good day!"},"DevOps/DockerTasks/Task-2-Create-the-Free-css-template-container":{"title":"Task 2 Create the Free-css template container","links":[],"tags":["devops_tasks","practical","devops","docker","containerisation"],"content":"Task 2: Create the Free-css template container\nInstructions\n\n get the docker image of Amazon linux\n install nginx\n add free-css template\n\nSteps Performed:\nGetting the Amazon Linux docker image\n\n\nGoto docker hub ⇒ hub.docker.com\n\n\nClick on Search and search for -⇒ amazonlinux\n\n\nHere I found this official image\n\n\n\nI simply pulled the image into my ec2 instance\ndocker pull amazonlinux\nhere is successful pull looks like\n\n\n\nWe can further confirm if the image is really present in the system with docker images\n\nLooks like amazonlinux is successfully pulled in our system\nlets start the container using this image\n\n\nRunning the container from image\nsudo docker run -d -p 32768:80 --name free-css-template amazonlinux\nWe used -d to run the container in detached mode meaning its output wont occupy the terminal screen and quietly run in the background after running this command it will simply give us container-id and gives us prompt.\n--name flag will assign the name for our container instead of their random funny names and with -p we are forwarding the network traffic of port 80 of the container to 32768 port of our host machine in this case ec2-instance.\n\nWe can check for the further detail of the container with\ndocker ps\nUpon inspection we can see there is no container running\nlets check again with docker ps -a and for our surprise container is exited just after it run. Thats because its an Operating system container which doesnt really do anything itself unless we assign it a task or a process that will run in background constantly so we can enter into the container to perform our tasks.\nWe can achieve this by running the container interactively\nbut first clean up the exited container with docker rm &lt;container-id&gt;\n\n\nGetting the shell into container\n\nRun a container interactively.\n\ndocker run -it -d -p 32768:80 --name free-css-template amazonlinux\nI run the container with -it and -d so it opened the interactive stdin shell session for me to interact with it and the container itself is in detached mode allowing us to enter into it anytime with exec command\ndocker exec -it &lt;container-id&gt; &lt;shell-command&gt;\n\nwe got bash shell session inside container\nNow we need to update the packages inside the container\nInstalling packages inside container\nyum update\n\n\nInstall nginx package start it\n\nyum install nginx -y\nsince this amazonlinux container image doesn’t come with systemd preinstalled we cannot start nginx daemon so we need to run this command manually in background\nnginx &amp;\nand our nginx server is successfully started at port 32768 on our host ec2-instance\n\nInstalling Free-CSS template\nLets add free css template in it\nHead on to www.free-css.com/ and download any free css template you want by right clicking on the download button and copying link address\n\nand go to your containers terminal session and download this file in nginx server directory with curl or wget\ncd /usr/share/nginx/html\ncurl -O www.free-css.com/assets/files/free-css-templates/download/page296/carvilla.zip\n \n# Extract the zip file\nunzip carvilla.zip\n \n# If unzip is not found install unzip package\nsudo yum install unzip\nextracting the zip has extracted all contents into folder named carvilla-v1.0\nI’ll rename it for my convenient.\nmv carvilla-v1.0 mywebsite\n\nand the website is running at port 32768/mywebsite\nnow we can get out of this container with exit command.\n\nWe have our container successfully hosting our free-css template on port http://instance-ip:32768/mywebsite\nLets create the image out of this container so we can spin many containers as we want and save the hustle to do all these steps all over again.\nCreating and Storing the Image on Dockerhub\n\nCreating the image\n\nNow that we have our container running and our site is visible I want to make image out of it and push it to docker hub\n\nfor that we use docker commit &lt;container-id&gt;\nit returns with the sha-id\n\nwith docker images we can confirm the image creation\n\nour image shown in first row doesnt have name so lets give it a tag and then push it to repo\ndocker tag &lt;image-id&gt; &lt;tagname&gt;\ndocker tag local-image:tagname new-repo:tagname\n\nHere I have already created repository in docker hub so I gave its name here\nand run docker hub login command before running push command\ndocker login\nand push it to our docker hub\ndocker push new-repo:tagname\n\nAnd our project is successful pushed over docker hub and can be access from here\nhub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags\nStoring the Image on EKS\n\nTo store the image into Amazon’s Elastic Container Registry which is similar to docker hub but their own we need to create a repository here too.\nI gave the repo same name\n\nand Upon clicking create button my repo is created\n\nClick on the Repo name in blue and click on view push commands to get instruction for pushing our image into this repo\n\nAuthenticating the docker client with EKS Registry\nTo push our image we need to first Authenticate with registry for that carefully use first command also we need to have aws-cli installed and configured for that luckily for me I am using amazonlinux ec2 instance so its already installed i can just run aws configure to configure aws-cli\nFor those havent can check this documentation to configure their aws-cli within instance.\n\nand then run this command provided in the instructions but remember to add sudo as we are using standard user\n\nNow that We have successfully login to ECR lets tag the image accordingly and push it to ECR\nsudo docker tag &lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt; &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;newname&gt;:&lt;newtag&gt;\n\nand push to ECR\nsudo docker push &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt;\n\nand refresh the amazon EKS web site to see the latest image listed in there\n\nThank you for Reading\nHave a good day!"},"DevOps/DockerTasks/Task-3-Create-Mysql-Container":{"title":"Task 3 Create Mysql Container","links":["tags/"],"tags":["devops_tasks","docker","containerisation",""],"content":"Create the MYSQL Container\nInstructions\n\n Host mysql container\n Use -e  environment variables\n use MYSQL_USER, MYSQL_PASSWORD\n\nSteps Performed\nStep 1: Login to Ec2-instance\nStep 2: Pull docker image of mysql\nsudo docker pull mysql\nStep 3: Run the container with environment variables\nsudo docker run -e MYSQL_USER=sarang -e MYSQL_PASSWORD=mysupersecretpassword -e MYSQL_RANDOM_ROOT_PASSWORD=yes -p 3306:3306 --name mysql_container -v mysql_data:/var/lib/mysql mysql:latest\nHere we used -e flag to specify each environment variables for mysql\nMYSQL_USER=sarang  This sets the username for our mysql database\nMYSQL_PASSWORD=mysupersecretpassword This sets the password for our database\nMYSQL_RANDOM_ROOT_PASSWORD=yes    # This tells the mysql to generate a random root password (it can be seen in docker logs)\n-v mysql_data:/var/lib/mysql mysql:latest This flag sets the persistence volume for our container and it mounts /var/lib/mysql directory of the container to the mysql_data directory of our host."},"DevOps/For-Reading/Agile-Development":{"title":"Agile Development","links":[],"tags":[],"content":"Agile topics\n\n\n                  \n                  Agile Manifesto \n                  \n                \n\nThe agile manifesto outlines 4 values and 12 principles for teams, but—decades later—is it still relevant? Find out\n\n\n\n\n                  \n                  Scrum \n                  \n                \n\nIn scrum, a product is built in a series of fixed-length iterations called sprints, giving agile teams a framework for shipping software on a regular cadence. Learn how the scrum methodology impacts traditional project management.\n\n\n\n\n                  \n                  Kanban \n                  \n                \n\nKanban is a popular agile framework that requires real-time communication of team’s capacity and full transparency of work. Learn how the kanban methodology for agile software development can benefit for your team.\n\n\nWhat is the Agile methodology?\nThe Agile methodology is a project management approach that involves breaking the project into phases and emphasizes continuous collaboration and improvement. Teams follow a cycle of planning, executing, and evaluating.\nWhereas the traditional “waterfall” approach has one discipline contribute to the project, then “throw it over the wall” to the next contributor, agile calls for collaborative cross-functional teams. Open communication, collaboration, adaptation, and trust amongst team members are at the heart of agile. Although the project lead or product owner typically prioritizes the work to be delivered, the team takes the lead on deciding how the work will get done, self-organizing around granular tasks and assignments.\nAgile isn’t defined by a set of ceremonies or specific development techniques. Rather, agile is a group of methodologies that demonstrate a commitment to tight feedback cycles and continuous improvement.\n\nThe original Agile Manifesto didn’t prescribe two-week iterations or an ideal team size. It simply laid out a set of core values that put people first. The way you and your team live those values today – whether you do scrum by the book, or blend elements of kanban and XP – is entirely up to you.\nWhy choose agile?\nTeams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year’s worth of plans. “Just enough” planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.\nBut it’s not just a numbers game—first and foremost, it’s about people. As described by the Agile Manifesto, authentic human interactions are more important than rigid processes. Collaborating with customers and teammates is more important than predefined arrangements. And delivering a working solution to the customer’s problem is more important than hyper-detailed documentation.\nAn agile team unites under a shared vision, then brings it to life the way they know is best. Each team sets their own standards for quality, usability, and completeness. Their “definition of done” then informs how fast they’ll churn the work out. Although it can be scary at first, company leaders find that when they put their trust in an agile team, that team feels a greater sense of ownership and rises to meet (or exceed) management’s expectations.\nAgile yesterday, today, and tomorrow\nThe publication of the Agile Manifesto in 2001 marks the birth of agile as a methodology. Since then, many agile frameworks have emerged such as scrum, kanban, lean, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way. Scrum and XP are favored by software development teams, while kanban is a darling among service-oriented teams like IT or human resources.\nToday, many agile teams combine practices from a few different frameworks, spiced up with practices unique to the team. Some teams adopt some agile rituals (like regular stand-ups, retros, backlogs, etc.), while others created a new agile practice (agile marketing teams who adhere to the Agile Marketing Manifesto).\n\nThe agile teams of tomorrow will value their own effectiveness over adherence to doctrine. Openness, trust, and autonomy are emerging as the cultural currency for companies who want to attract the best people and get the most out of them. Such companies are already proving that practices can vary across teams, as long as they’re guided by the right principles.\nAtlassian on agile\nThe way each team practices agile should be unique to their needs and culture. Indeed, no two teams inside Atlassian have identical agile practices.\nAlthough many of our teams organize their work in sprints, estimate in story points, and prioritize their backlogs, we’re not die-hard practitioners of scrum. Or kanban. Or any other trademarked methodology. Instead, we give each team the autonomy to cherry-pick the practices that will make them most effective. And we encourage you to take a similar approach.\nFor example, if you’re on a queue-oriented team like IT, kanban provides a solid foundation for your agile practice. But nothing should stop you from sprinkling in a few scrum practices like demo sessions with stakeholders or regular retrospectives.\nThe key to doing agile right is embracing a mindset of continuous improvement. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don’t.\n"},"DevOps/For-Reading/Agile-Vs-DevOps":{"title":"Agile Vs DevOps","links":[],"tags":[],"content":"Agile and DevOps have shaped the way software is developed today. They’ve become so widely adapted and revered as to permeate beyond the world of software development into shaping project management and org charts in businesses of all stripes.\nDevOps and agile can be tricky to define, and the lines between the two often seem to blur.\n\nAt a basic level, DevOps is the combination of two teams (software development and IT operations) to create a more powerful, efficient software development process.\nAgile is a series of methodologies around iterative development designed to make tasks smaller and more manageable and increase collaboration.\n\nBut what are the deeper differences between DevOps and agile? Is DevOps better than agile? Or can DevOps and agile be implemented together? In this post, we’ll dive into some common agile and DevOps FAQs.\nDevOps vs Agile\n\nWhat is the difference between DevOps and agile?\nWhat is agile?\nWhat are the benefits of agile?\nHow can a company be agile?\nWhat is DevOps?\nWhy combine software development and IT operations?\nWhat are some DevOps concepts and key terms?\nWhat are the benefits of DevOps?\nIs DevOps better than agile? Is DevOps a replacement for agile?\nWhat is CI/CD?\nWhat are the benefits of CI/CD?\nWhat are some common DevOps tools?\nHow can I learn more about DevOps?\n\nWhat is the difference between DevOps and agile?\nAgile and DevOps are both used in the development of software. While they’re both designed to improve the software development process, they seek to do so by taking different approaches. But they’re not mutually exclusive. (More on that below.)\nAgile is essentially about giving software developers a shared set of principles to drive decision-making and allow for more responsiveness to change.\nDevOps is about a culture change intended to drive collaboration between software developers and IT operations.\nIt’s often said that DevOps is agile applied beyond the software development team.\n\n\nPost-COVID DevOps: Accelerating the Future\nHow has COVID affected — or even accelerated — DevOps best practices for engineering teams? Watch this free, on-demand webinar panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.\n\nWhat is agile?\nAgile is an iterative software development approach that focuses on collaboration and quick, rapid releases. It’s a set of values and principles that can be used to help drive decisions in software development. \nWhen it comes to agile, it’s tough to define it more concisely than the original micro manifesto itself, which was written back in 2001. (No TL;DR version needed. It’s only 68 words.) It states:\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\n\nIndividuals and interactions over processes and tools\nWorking software over comprehensive documentation\nCustomer collaboration over contract negotiation\nResponding to change over following a plan\n\nThat is, while there is value in the items on the right, we value the items on the left more\nThe manifest is paired with 12 agile principles to help make better decisions. \nCopying agile approaches and investing in agile tools doesn’t make your team agile. It’s not just about doing two-week iterations or building out smaller teams. In agile, the what a team does is less important than the why they do it.\nAgile (as the name implies) is about the flexibility and adaptability to build software with ever-changing needs and toss set-in-stone plans out the window.\nThis includes better connecting the dev team with end-users. (You could sort of think of that a bit like the “What would you say you do here?” scene from Office Space.)\nWhat are the benefits of agile?\n\nAgility -  More quickly respond to market changes or customer feedback.\nQuality - A focus on testing and sustained customer involvement means the chances of a product’s overall quality being high are greater.\nCollaboration - Agile is about people. By placing value on human interactions over processes and “that’s just the way it’s done,” organizations are able to let employees act guided by their experience and a shared set of values rather than being micro-managed or shackled to detailed documentation.\nOwnership - The trust required from leadership to have agile teams can create an increased feeling of ownership.\nCustomer satisfaction - With a focus on finding and fixing problems quickly and a direct line between customers and developers, customers are more likely to be satisfied and come back for more.\nPredictability - By doing away with big plans that are set in stone and often detached from reality, organizations can get a better picture of what’s going well and what’s not working right away rather than months down the road when it’s too late to do anything to correct it.\nIncrease productivity - Regularly used planning tools like Kanban and Scrum help teams keep tabs of work and progress toward goals.\n\nHow can a company be agile?\nMany organizations are used to working in a waterfall model. A waterfall model is a linear approach is a sequence of events that starts with a heavy upfront investment of time and resources in scoping out requirements and budgeting before moving into development, testing, and production.\nMoving this paradigm to agile and running agile at scale can be difficult and complex. This isn’t helped by the fact that “agile” is less of a thing you do and more of a basis for decision-making. For organizations guided by process above all else, this can require a major shift and definitely takes some backing from management.\nWhat is DevOps?\nWith a name that sounds like a covert military team and the kind of goofy capitalization you’ve come to expect from tech terminology, DevOps combines software development and IT operations. Take the “dev” from software development and the “ops” from IT operations and you get this portmanteau, which is the fun-to-say term that describes mashing together of multiple words, like spork, phablet, brunch, jorts, and bromance.\nDevOps isn’t just a process. It’s a shift in workplace culture. It’s a collaboration between teams. Doubling down on automation and installing all of the right software won’t get you there alone. Like agile, people are the key component.\nInterested in upscaling or beginning your journey with DevOps? A Cloud Guru’s DevOps Learning Paths offers custom courses fit for beginners and advanced gurus!\nAWS offers the following definition of the DevOps model:\nDevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.\n\n\nAutomating AWS Cost Optimization\nAWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this free, on-demand webinar, you’ll get an overview of AWS cost-optimization tools and strategies.\nWatch Now\n\nWhy combine software development and IT operations?\nHow did these two groups get combined into one? And why would you want to combine development and IT operations? \nWay back when (around 2007 or so), IT operations teams were separate from development teams. Then, people in the development and operations communities realized there was some issues in the way these teams were structured and how they interacted (or didn’t interact).\n\nDevelopment and IT operations teams had different objectives\nDevelopment and operations teams had different performance indicators\nDevelopment and operations teams were siloed physically in different rooms, buildings, or offices\n\nSo they started talking. This gave birth to DevOps and the DevOps engineer. \nPatrick Debois, often called the Godfather of DevOps, brought these groups together at devopsdays in 2009 where they discussed ways to bridge the gap between the two fields. Now, thousands of enterprises have adapted or are working toward adapting these practices.\nThese new approaches basically make ops everyone’s job to a degree, which makes the future of ops jobs very interesting. \nDevOps concepts and key terms\n\nContainer - A lightweight, standalone, executable piece of software. It includes everything needed to run that software.\nContinuous delivery (CD) - The ongoing and manual or automatic release of software to production. It’s aimed around new cycles. Some orgs release a new version with any changes.\nContinuous integration (CI) - The ongoing programming, building, and testing of code. This is done before turning it over to the continuous delivery (CD) system for deployment. With CD, it makes up CI/CD.\nInfrastructure as Code (IaC) - Defining infrastructure you want to use with programming code that can be understood by cloud services. These cloud services then create the infrastructure for you based on this code. This allows you to define standards and reuse code, which saves time.\nMicroservices - Application architecture that is broken into multiple small pieces. Containers are often used to deploy microservices.\nOpen source - Computer software code released under a license for free, like Linux or Kubernetes.\nPipeline - A set of connected processes where the output of one process is the input for the next.\nServerless - Running a service or microservice on cloud-based infrastructure without worrying about the servers running your code. You simply provide the code, and the cloud provider runs the code and gives you the results. See more on the Function as a Service (FaaS) services of AWS, Azure, and GCP.\nSource code repository - A place to upload and track the history of your code, like GitHub or AWS CodeCommit.\nUnit testing - Breaking your application down into small parts to test that each features works.\n\nWhat are the benefits of DevOps?\nDevOps is all about producing higher-quality software faster and saving a lot of time and money. Here’s a more detailed breakdown of the benefits.\n\nSpeed - Release updates and new features faster, adapt to the changing market, and become more efficient.\nRapid delivery - Increase deployment frequency and the pace of releases. Respond to customers’ needs faster and build a competitive advantage. \nReliability - Automatic testing is built-in. Ensures rollouts are of the highest quality and that you have less downtime because you build for stability and test before deploying.\nScale - Implements automation. With the use of cloud and container technology, you can scale usage up and down and save money while you do so.\nCollaboration - Allow teams who used to be apart to work together. Workflows can be combined, inefficiency is reduced, and time is saved.\nSecurity - Allows infrastructure to be created by code, just like software. By doing this (instead of doing it manually), you can define policies to stay compliant no matter how many servers will be deployed.\n\nWant an overview of the benefits with a heavy dash of sarcasm? Check out our post 5 reasons to NOT move to DevOps.\nIs DevOps a replacement for agile?\nDevOps and agile can speed up the delivery of and increase the quality of software. Agile replaced the old-school waterfall model, but DevOps isn’t a replacement for agile.\nDevOps exists because of agile, and the two can coexist and be used together. You don’t have to choose between DevOps and agile. Ideally, your organization will practice both. \nSee the top DevOps skills people are learning at ACG right now\nWhat is CI/CD?\nContinuous integration and continuous development (or CI/CD) is a DevOps tactic — a way to deliver apps to customers with the introduction of automation into the agile development process. \nThe CI/CD pipeline (as it’s called when these practices are combined) has become an integral part of DevOps culture. A variety of tools and techniques are used for implementing such a pipeline. (More on those tools below.)\nThe CI/CD pipeline is supported by teams working in an agile way with either DevOps or a site reliability engineering (SRE) approach. \nRead more about why you need SRE skills to thrive in a serverless world and check out our Site Reliability Engineering Concepts or Google Cloud DevOps and SREs courses.\nWhat are the benefits of CI/CD?\nCI/CD pipelines smooth and speed up the flow of code from development through operations and QA into production by automating manual processes and increasing confidence in your releases. \nThis is the exact opposite of the waterfall release approach still practiced by many large organizations, where developers throw code “over the wall” to ops, devs don’t get access to production, and ops doesn’t have much inside knowledge of how the code works.\nCI/CD allows organizations to:\n\nBuild faster\nDecrease code review time\nAutomatic\nFaster fault isolation\nAdditional deployment features\n\nAs mentioned before, you can be doing the right things and using the right tools and still not be agile or properly implementing DevOps. A broken and messy team can result in broken and messy CI/CD pipelines. You can almost predict how your CI/CD pipeline looks based on how your dev and ops teams work together.\n\n\nGet the Cloud Dictionary of Pain\nSpeaking cloud doesn’t have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this cloud guide for succinct definitions of some of the most painful cloud terms.\nGet the Goods\n\nWhat are some common DevOps tools?\nHere are some of the most popular DevOps CI/CD tools you might use if working in an AWS cloud environment.\n\n\nGit - Free open-source version control system. It stores the entire history of your code that your developers will continue to push new code to It has a tiny footprint and fast performance. It also supports branching, allowing others to work on features separately without having developers trip over code.\n\n\nGitHub - Online service to host Git repositories. GitHub is owned by Microsoft, which offers a similar DevOps tool called Azure DevOps. Whether you are a developer or not, working alone or in a team, Azure DevOps training can help you organize the way you plan, create and deliver software. Get the lowdown on Azure DevOps vs GitHub in this comparison of Microsoft DevOps tools.\n\n\nAWS CodeCommit - Fully managed server for hosting Git repositories. It’s secure and encrypted, highly available, and easily integrated with other AWS services.\n\n\nAWS CodeBuild - Fully managed continuous integration code that complies code, runs tests, and produces software packages. When developers write code and push it into Git, CodeBuild compiles that code and produces a software package. It scales automatically and can process multiple builds concurrently.\n\n\nAWS CodeDeploy - Fully managed deployment service that takes the software package (or files you updated and pushed into your Git repository) and deploys it to AWS or on-premises servers. It integrates well with AWS services and your own servers. It completely automates software deployment, eliminating error-prone manual operations.\n\n\nAWS CodePipeline - Fully management continuous delivery service that helps you completely automate your release pipeline. It can automate the build, test, and deploy phases of software development and it can integrate with CodeCommit and GitHub, giving you the flexibility to use the source-control system of your choice. Read more about automating CI/CD with AWS CodePipeline.\n\n\nOther common DevOps tools include Docker, Jira, Jenkins, Puppet, Chef, Splunk, Ansible, Kubernetes, Bamboo, and Nagios."},"DevOps/For-Reading/Git-Fetch-vs-Git-pull":{"title":"Git Fetch vs Git pull","links":[],"tags":[],"content":"Git fetch and git pull basics\nGit fetch and git pull are both Git commands used to retrieve update information from a remote repository. So, how do they differ? Git fetch downloads the changes from the remote repository to the local repository but does not make any changes to the current working directory. Since the changes are not merged into the local branch, you can check the changes from the remote repository without interrupting your current work. On the other hand, git pull retrieves the latest changes from the remote repository like git fetch, but it also automatically merges those changes into the current branch. In contrast to git fetch, git pull directly applies the changes from the remote repository to the local working directory.\nWhat is git fetch?\nThe git fetch command retrieves the latest commit history from the remote repository, but it does not affect the local working directory. Even after fetching remote changes, they are not reflected in the local branch. It is primarily used when you want to retrieve the latest status from the remote repository and review the changes before they are reflected in the local repository. To apply the retrieved changes to the local branch, you need to manually run git merge or git rebase.\nWhat is git pull?\nThe git pull command combines git fetch and git merge (or git rebase) into a single command. This allows you to fetch changes from the remote repository and automatically integrate them into the current local branch.\nWhile git fetch retrieves changes from the remote repository without applying them to the local branch, running git pull automatically integrates the changes from the remote repository into the local branch.\nGit pull is suitable for quickly reflecting remote changes in the local branch, but it can lead to conflicts, so caution is needed, especially when working with multiple people.\nWhen to use git fetch\nGit fetch is a command used to retrieve the latest information from a remote repository. The retrieved information is not directly reflected in the local branch. Using git pull will reflect all remote branches, including incorrect or problematic ones, in the local branch.\nWhen changes are made simultaneously on both remote and local branches, or when there are new users on the team, it is safer to use git fetch to retrieve the remote branch contents first and then perform merge or rebase.\nWhen to use git pull\nGit pull is a command that performs more processes compared to git fetch. Git pull can perform both git fetch and additionally execute git merge or git rebase. For this reason, git pull is recommended when you want to quickly reflect changes from the remote repository in the local branch.\nGit fetch and git pull FAQs\nWhat is the difference between git pull and git fetch?\nGit pull is a command that performs git fetch followed by git merge or git rebase. While git fetch does not affect the local repository, git pull automatically synchronizes changes from the remote repository with the local repository.\nWhat precautions should be taken when using git pull?\nWhen executing git pull, there may be conflicts between remote and local changes. Merge conflicts are particularly likely to occur, so if conflicts arise, they need to be resolved manually. Additionally, using git pull —rebase allows you to incorporate the latest changes while performing a rebase.\nWhat is git fetch used for?\nGit fetch is useful for checking and retrieving the latest status of the remote repository. However, the changes retrieved are not automatically reflected in the local branch; git fetch is used to synchronize the local and remote repositories."},"DevOps/For-Reading/Git-Theory":{"title":"Git Theory","links":[],"tags":[],"content":"CVCS Centralized Versioning System\nA centralized version control system (VCS) uses a single, central repository to store all file versions and their change history. Team members have their own working copies, but all modifications are ultimately committed to this central server. This facilitates collaboration by providing a single source of truth, but it also creates a single point of failure. Examples include Subversion (SVN) and CVS.\n\nDVCS - Decentralized Version Control System\nIn a Decentralized Version Control System (DVCS), every user has a complete copy of the repository, including its entire history. This eliminates the reliance on a central server, allowing for offline work and greater flexibility. Changes are shared between repositories as needed. Popular examples include Git and Mercurial.\n\nWhat is Git?\nGit is like a super-powered tracking system for your files and the changes you make to them over time. Imagine it as a special folder that remembers every version of your work, allowing you to go back to any previous stage if needed.  \nHere’s what makes Git special:\n\nKeeps a detailed history: Git meticulously records every change you make to your files, who made it, and when. This history helps you understand how your project evolved.  \nBranching and merging: Git allows you to create separate branches, like alternate timelines, to experiment with new features or try different ideas without affecting the main project. You can then merge these branches back into the main project when you’re ready.  \nCollaboration made easy: Git is designed for teamwork. Multiple people can work on the same project simultaneously, and Git helps manage and integrate everyone’s contributions smoothly.  \nOffline access: You have the entire project history on your computer, so you can work even without an internet connection.  \nPopular and widely used: Git is the most popular version control system in the world, used by countless developers and companies.  \n\nHistory of Git\nGit was created by Linus Torvalds, the famous creator of the Linux operating system, in 2005. He needed a better tool to manage the Linux kernel development, as the existing version control systems were not efficient enough for such a large and complex project.\nHere’s a glimpse of Git’s history:\n\nEarly Days (2002-2005): Linux kernel development relied on a proprietary DVCS called BitKeeper. When its free-of-charge use was revoked, Torvalds decided to create his own version control system, with the goal of being faster, simpler, and more robust.\nBirth of Git (April 2005): Torvalds began work on Git and within a remarkably short period, had a functional system ready to manage the Linux kernel.\nCommunity Takes Over (July 2005): Junio Hamano took over the maintenance of Git, guiding its development and shaping it into the mature system it is today.\nWidespread Adoption: Git’s speed, flexibility, and powerful features quickly gained popularity among developers. It became the preferred choice for open-source projects and eventually spread to commercial software development.\n\nToday, Git is the most widely used version control system worldwide, powering the development of countless software projects, from small personal projects to massive corporate endeavors."},"DevOps/For-Reading/K8s-Service-Types":{"title":"K8s Service Types","links":[],"tags":["devops","kubernetes","reading_tasks"],"content":"What is a Kubernetes Service?\nThe idea of a Service is to group a set of Pod endpoints into a single resource. You can configure various ways to access the grouping. By default, you get a stable cluster IP address that clients inside the cluster can use to contact Pods in the Service. A client sends a request to the stable IP address, and the request is routed to one of the Pods in the Service.\nA Service identifies its member Pods with a selector. For a Pod to be a member of the Service, the Pod must have all of the labels specified in the selector. A label is an arbitrary key/value pair that is attached to an object.\nThe following Service manifest has a selector that specifies two labels. The selector field says any Pod that has both the app: metrics label and the department:engineering label is a member of this Service.\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: metrics\n    department: engineering\n  ports:\n  ...\n\nWhy use a Kubernetes Service?\nIn a Kubernetes cluster, each Pod has an internal IP address. But the Pods in a Deployment come and go, and their IP addresses change. So it doesn’t make sense to use Pod IP addresses directly. With a Service, you get a stable IP address that lasts for the life of the Service, even as the IP addresses of the member Pods change.\nA Service also provides load balancing. Clients call a single, stable IP address, and their requests are balanced across the Pods that are members of the Service.\nTypes of Kubernetes Services\nThere are five types of Services:\n\n\nClusterIP (default): Internal clients send requests to a stable internal IP address.\n\n\nNodePort: Clients send requests to the IP address of a node on one or more nodePort values that are specified by the Service.\n\n\nLoadBalancer: Clients send requests to the IP address of a network load balancer.\n\n\nExternalName: Internal clients use the DNS name of a Service as an alias for an external DNS name.\n\n\nHeadless: You can use a headless service when you want a Pod grouping, but don’t need a stable IP address.\n\n\n\n\n                  \n                  NOTE\n                  \n                \n\nThe NodePort type is an extension of the ClusterIP type. So a Service of type NodePort has a cluster IP address.\nThe LoadBalancer type is an extension of the NodePort type. So a Service of type LoadBalancer has a cluster IP address and one or more nodePort values.\n\n\nServices of type ClusterIP\nClusterIP is the default service type in Kubernetes, and it provides internal connectivity between different components of our application. \nKubernetes assigns a virtual IP address to a ClusterIP service that can solely be accessed from within the cluster during its creation. This IP address is stable and doesn’t change even if the pods behind the service are rescheduled or replaced.\nClusterIP services are an excellent choice for internal communication between different components of our application that don’t need to be exposed to the outside world. For example, if we have a microservice that processes data and sends it to another microservice for further processing, we can use a ClusterIP service to connect them.\nWhen you create a Service of type ClusterIP, Kubernetes creates a stable IP address that is accessible from nodes in the cluster.\nHere is a manifest for a Service of type ClusterIP:\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\nspec:\n  selector:\n    app: backend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n\nIn this example, we define a service named backend with a selector that targets pods labeled with app: backend. The service exposes port 80, which is the port used by clients to access the service, and forwards the traffic to the pods’ port 8080, which is where the backend application is running.\nYou can create the Service by using kubectl apply -f [MANIFEST_FILE]. After you create the Service, you can use kubectl get service to see the stable IP address:\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)\nmy-cip-service   ClusterIP   10.11.247.213   none          80/TCP\n\nClients in the cluster call the Service by using the cluster IP address and the TCP port specified in the port field of the Service manifest. The request is forwarded to one of the member Pods on the TCP port specified in the targetPort field. For the preceding example, a client calls the Service at 10.11.247.213 on TCP port 80. The request is forwarded to one of the member Pods on TCP port 8080. The member Pod must have a container that is listening on TCP port 8080. If there is no container listening on port 8080, clients will see a message like “Failed to connect” or “This site can’t be reached”.\nService of type NodePort\nNodePort services extend the functionality of ClusterIP services by enabling external connectivity to our application. When we create a NodePort service on any node within the cluster that meets the defined criteria, Kubernetes opens up a designated port that forwards traffic to the corresponding ClusterIP service running on the node.\nThese services are ideal for applications that need to be accessible from outside the cluster, such as web applications or APIs. With NodePort services, we can access our application using the node’s IP address and the port number assigned to the service.\nWhen you create a Service of type NodePort, Kubernetes gives you a nodePort value. Then the Service is accessible by using the IP address of any node along with the nodePort value.\nHere is a manifest for a Service of type NodePort:\napiVersion: v1 \nkind: Service \nmetadata: \n  name: frontend \nspec: \n  selector: \n    app: frontend \n  type: NodePort \n  ports: \n    - name: http \n      port: 80 \n      targetPort: 8080\n\nWe define a service named frontend that targets pods labeled with app: frontend by setting a selector. The service exposes port 80 and forwards the traffic to the pods’ port 8080. We set the service type to NodePort, and Kubernetes exposes the service on a specific port on a qualifying node within the cluster.\nWhen we create a NodePort service, Kubernetes assigns a port number from a predefined range of 30000-32767. Additionally, we can specify a custom port number by adding the nodePort field to the service definition:\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\nspec:\n  selector:\n    app: frontend\n  type: NodePort\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    nodePort: 30080\nThe nodePort field is specified as 30080, which tells Kubernetes to expose the service on port 30080 on every node in the cluster.\nAfter you create the Service, you can use kubectl get service -o yaml to view its specification and see the nodePort value.\nspec:\n  clusterIP: 10.11.254.114\n  externalTrafficPolicy: Cluster\n  ports:\n  - nodePort: 32675\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n\nExternal clients call the Service by using the external IP address of a node along with the TCP port specified by nodePort. The request is forwarded to one of the member Pods on the TCP port specified by the targetPort field.\nFor example, suppose the external IP address of one of the cluster nodes is 203.0.113.2. Then for the preceding example, the external client calls the Service at 203.0.113.2 on TCP port 32675. The request is forwarded to one of the member Pods on TCP port 8080. The member Pod must have a container listening on TCP port 8080.\nThe NodePort Service type is an extension of the ClusterIP Service type. So internal clients have two ways to call the Service:\n\nUse clusterIP and port.\nUse a node’s IP address and nodePort.\n\nFor some cluster configurations, the external Application Load Balancer uses a Service of type NodePort.\nAn external Application Load Balancer is a proxy server, and is fundamentally different from the external passthrough Network Load Balancer described in this topic under Service of type LoadBalancer.\n\n\n                  \n                  NOTE\n                  \n                \n\nYou can specify your own nodePort value in the 30000—32767 range. However, it’s best to omit the field and let Kubernetes allocate a nodePort for you. This avoids collisions between Services.\n\n\nServices of type LoadBalancer\nLoadBalancer services connect our applications externally, and production environments use them where high availability and scalability are critical. When we create a LoadBalancer service, Kubernetes provisions a load balancer in our cloud environment and forwards the traffic to the nodes running the service.\nLoadBalancer services are ideal for applications that need to handle high traffic volumes, such as web applications or APIs. With LoadBalancer services, we can access our application using a single IP address assigned to the load balancer.\nHere’s an example of a simple LoadBalancer service definition:\napiVersion: v1\nkind: Service\nmetadata:\n  name: web\nspec:\n  selector:\n    app: web\n  type: LoadBalancer\n  ports:\n    - name: http\n      port: 80\n      targetPort: 8080\nWe set the service type to LoadBalancer to instruct Kubernetes to provision a load balancer. Here, we define a service named web and specify a selector that targets pods labeled with app: web. Additionally, we expose port 80 and forward traffic to the pods’ port 8080.\nAfter creating the LoadBalancer service, Kubernetes provisions a load balancer in the cloud environment with a public IP address. We can use this IP address to access our application from outside the cluster.\nService of type ExternalName\nA Service of type ExternalName provides an internal alias for an external DNS name. Internal clients make requests using the internal DNS name, and the requests are redirected to the external name.\nHere is a manifest for a Service of type ExternalName:\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-xn-service\nspec:\n  type: ExternalName\n  externalName: example.com\n\nWhen you create a Service, Kubernetes creates a DNS name that internal clients can use to call the Service. For the preceding example, the DNS name is my-xn-service.default.svc.cluster.local. When an internal client makes a request to my-xn-service.default.svc.cluster.local, the request gets redirected to example.com.\nThe ExternalName Service type is fundamentally different from the other Service types. In fact, a Service of type ExternalName does not fit the definition of Service given at the beginning of this topic. A Service of type ExternalName is not associated with a set of Pods, and it does not have a stable IP address. Instead, a Service of type ExternalName is a mapping from an internal DNS name to an external DNS name.\nHeadless Service\nA headless Service is a type of Kubernetes Service that does not allocate a cluster IP address. Instead, a headless Service uses DNS to expose the IP addresses of the Pods that are associated with the Service. This allows you to connect directly to the Pods, instead of going through a proxy.\nHeadless Services are useful for a variety of scenarios, including:\n\n\nLoad balancing across pods: You can use headless Services to load balance across Pods. To implement this, create a Service with a selector that matches the Pods that you want to load balance. The Service will then distribute traffic evenly across all of the Pods that match the selector.\n\n\nService discovery: You can use a headless Service to implement Service discovery. To implement this, create a Service with a name and a selector. DNS record for the headless service contains all the IPs of the Pods behind the Service that match the selector. Clients can use these DNS records to find the IP addresses of the Pods that are associated with the Service.\n\n\nDirect Pod access: Clients can connect directly to the Pods that are associated with a headless Service, which can be useful for Services that require direct access to the underlying Pods, such as load balancers and DNS servers.\n\n\nFlexibility: Headless services can be used to create a variety of different topologies, such as load balancers, DNS servers, and distributed databases.\n\n\nIf you have special network requirements for your workloads that can not be solved using headless Services with selectors, there is also the possibility of using headless Services without selectors. Headless Services are a useful tool for accessing Services that are not located within the Kubernetes cluster itself, as the control plane does not create EndpointSlice objects, you can read more about it in Service without selectors\nThe following example is a manifest for a Headless Service:\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  clusterIP: None\n  selector:\n    app: nginx\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n\nOnce you have created a headless Service, you can find the IP addresses of the Pods that are associated with the Service by querying the DNS. For example, the following command lists the IP addresses of the Pods that are associated with the nginx Service:\nNote: This example assumes that the Pods created are tagged with the nginx label.\ndig +short nginx.default.svc.cluster.local\n\nAnother example which uses Kubernetes query expansion::\ndig +short +search nginx\n\nYou can create a headless Service with a single command, and headless Services are easy to update and scale.\nkubectl create service clusterip my-svc --clusterip=&quot;None&quot; --dry-run=client -o yaml &gt; [file.yaml]\n\nService abstraction\nA Service is an abstraction in the sense that it is not a process that listens on some network interface. Part of the abstraction is implemented in the iptables rules of the cluster nodes. Depending on the type of the Service, other parts of the abstraction are implemented by either an external passthrough Network Load Balancer or an external Application Load Balancer.\nArbitrary Service ports\nThe value of the port field in a Service manifest is arbitrary. However, the value of targetPort is not arbitrary. Each member Pod must have a container listening on targetPort.\nHere’s a Service, of type LoadBalancer, that has a port value of 50000:\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-ap-service\nspec:\n  clusterIP: 10.11.241.93\n  externalTrafficPolicy: Cluster\n  ports:\n  - nodePort: 30641\n    port: 50000\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: parts\n    department: engineering\n  sessionAffinity: None\n  type: LoadBalancer\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 203.0.113.200\n\nA client calls the Service at 203.0.113.200 on TCP port 50000. The request is forwarded to one of the member Pods on TCP port 8080.\nMultiple ports\nThe ports field of a Service is an array of ServicePort objects. The ServicePort object has these fields:\n\nname\nprotocol\nport\ntargetPort\nnodePort\n\nIf you have more than one ServicePort, each ServicePort must have a unique name.\nHere is a Service, of type LoadBalancer, that has two ServicePort objects:\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-tp-service\nspec:\n  clusterIP: 10.11.242.196\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: my-first-service-port\n    nodePort: 31233\n    port: 60000\n    protocol: TCP\n    targetPort: 50000\n  - name: my-second-service-port\n    nodePort: 31081\n    port: 60001\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: tests\n    department: engineering\n  sessionAffinity: None\n  type: LoadBalancer\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 203.0.113.201\n\nNote: You can specify a maximum of five ports for a LoadBalancer service.\nIn the preceding example, if a client calls the Service at 203.0.113.201 on TCP port 60000, the request is forwarded to a member Pod on TCP port 50000. But if a client calls the Service at 203.0.113.201 on TCP port 60001, the request is forwarded to a member Pod on TCP port 8080.\nEach member Pod must have a container listening on TCP port 50000 and a container listening on TCP port 8080. This could be a single container with two threads, or two containers running in the same Pod.\nService endpoints\nWhen you create a Service, Kubernetes creates an Endpoints object that has the same name as your Service. Kubernetes uses the Endpoints object to keep track of which Pods are members of the Service.\nSingle-stack and dual-stack Services\nYou can create an IPv6 Service of type ClusterIP or NodePort. GKE supports dual-stack Services of type LoadBalancer during Preview which carries no SLA or technical support.\nFor each of these Service types, you can define ipFamilies and ipFamilyPolicy fields as either IPv4, IPv6, or a dual-stack Service.\nChoosing the Right Service Type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService TypeUse CaseAccessibilityResource AllocationClusterIPInternal communication between application componentsWithin the cluster onlyMinimal resources neededNodePortExternal accessibility for web applications or APIsAccessible from outside the cluster via a high-numbered port on the nodeAdditional resources neededLoadBalancerProduction environments with high traffic volumesAccessible from outside the cluster via a load balancerSignificant resources neededCloud Provider’s Load BalancerUsing a cloud provider for KubernetesAccessible from outside the cluster via the cloud provider’s load balancerMay result in cost savings and better performance\nReferences\n\nGKE Documentation\nwww.baeldung.com/ops/kubernetes-service-types\n"},"DevOps/For-Reading/Monolithic-vs-Microservice-Architecture":{"title":"Monolithic vs Microservice Architecture","links":[],"tags":[],"content":"Monolithic applications are built as a single, unified unit. Think of it like a large container holding all the components of your application—user interface, business logic, database access—all bundled together.  \nMicroservices, on the other hand, break down the application into a collection of small, independent services. Each service focuses on a specific business function and communicates with other services through APIs. Imagine it as a set of interconnected Lego blocks, each serving a distinct purpose.  \nHere’s a table highlighting the key differences:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureMonolithicMicroservicesArchitectureSingle unitCollection of independent servicesDeploymentDeployed as a single packageEach service deployed independentlyScalabilityScaling requires scaling the entire applicationIndividual services can be scaled independentlyTechnology StackTypically uses a single technology stackServices can use different technology stacksDevelopmentEasier to develop initiallyMore complex development and deploymentMaintenanceCan be harder to maintain as the application growsEasier to maintain individual servicesFault IsolationA bug in one part can affect the entire applicationA bug in one service is less likely to affect others\nIn a nutshell:\n\nMonolithic apps are simpler to start with but can become complex to manage as they grow.  \nMicroservices offer greater flexibility, scalability, and fault isolation, but they come with increased architectural complexity.  \n\nThe choice between monolithic and microservices depends on the specific needs and complexity of your application.\nReferences:\n\nrtslabs.com/data-integration-in-a-microservices-architecture\n"},"DevOps/For-Reading/Read-about-each-and-every-component-of-kubernetes":{"title":"Read about each and every component of kubernetes","links":[],"tags":[],"content":"Kubernetes Components and Architecture\nKubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It follows a master-worker architecture consisting of multiple components that work together to maintain the desired state of a cluster. Below is an explanation of each component in Kubernetes, along with an architecture diagram and manifest files.\nKey Kubernetes Components:\n1. Master Node (Control Plane)\nThe master node is the brain of the Kubernetes cluster. It manages and controls the cluster, making global decisions (e.g., scheduling), detecting and responding to cluster events (e.g., starting up pods), and managing the overall state of the cluster.\nThe master node consists of the following components:\n\n\nAPI Server (kube-apiserver):\nThe API server is the central management point for Kubernetes. It exposes the Kubernetes API, which is used for communicating with other components of the system. All requests (whether for interacting with pods, nodes, deployments, etc.) are handled by the API server.\n\n\nScheduler (kube-scheduler):\nThe scheduler is responsible for selecting which node a newly created pod will run on based on resource availability and constraints defined by the user. It makes the decision to place pods on specific worker nodes.\n\n\nController Manager (kube-controller-manager):\nThe controller manager ensures that the desired state of the cluster is maintained. It manages controllers like the ReplicaSet, Deployment, and Node controllers. The controllers constantly check the state of the cluster and take corrective actions to meet the desired state.\n\n\netcd:\netcd is a distributed key-value store that holds the entire configuration and state of the Kubernetes cluster. It is the source of truth for all cluster data, such as the definitions of nodes, pods, deployments, services, and more. It’s highly available and fault-tolerant.\n\n\n2. Worker Node\nWorker nodes (also called minions) are the machines responsible for running the application workloads in the form of containers. Each node in a Kubernetes cluster contains the following components:\n\n\nKubelet:\nThe kubelet is an agent that runs on each worker node. It ensures that containers are running in a pod by communicating with the API server and monitoring the health of the containers.\n\n\nKube Proxy:\nKube Proxy manages the networking and load balancing between pods. It ensures that each pod has network connectivity and routes traffic to the correct backend service.\n\n\nContainer Runtime:\nThis is the software responsible for running containers. Examples of container runtimes are Docker, containerd, and CRI-O.\n\n\n3. Pods\nA pod is the smallest and simplest Kubernetes object. It is a single instance of a running process in the cluster, and it can contain one or more containers. All containers in a pod share the same network namespace and storage.\n4. Namespaces\nNamespaces provide a way to divide cluster resources between multiple users. Namespaces are used for organizing resources within a cluster into logical groups. They are primarily useful in large clusters with many users.\n5. Services\nA service is an abstraction that defines a set of pods and a policy by which to access them. Services ensure that applications can communicate with each other reliably, regardless of pod lifecycles or IP changes.\n6. Deployments\nA deployment provides declarative updates for pods and ReplicaSets. It allows you to manage and scale applications running in pods, ensuring that the desired number of replicas is always running.\n7. ReplicaSets\nReplicaSets ensure that a specified number of identical pods are running at any given time. A ReplicaSet can be used by a Deployment to maintain the number of replicas.\n8. StatefulSets\nStatefulSets are similar to ReplicaSets but designed for applications that require persistent storage and stable network identities. These are often used for stateful applications like databases.\n9. ConfigMaps and Secrets\n\nConfigMap: A way to inject configuration data into pods, which can be used as environment variables, command-line arguments, or configuration files.\nSecret: Stores sensitive data like passwords, OAuth tokens, or ssh keys in an encrypted form.\n\n10. Ingress\nIngress is a collection of rules that allow inbound connections to reach the cluster services. It manages HTTP and HTTPS routes to services.\n\nKubernetes Architecture Diagram:\nHere’s a high-level diagram of the Kubernetes architecture:\nlua\nCopy code\n                +--------------------------------------+\n                |               Kubernetes             |\n                |             Master Node              |\n                |                                      |\n                |   +----------------------------+     |\n                |   |    kube-apiserver           |     |\n                |   +----------------------------+     |\n                |   |    kube-scheduler           |     |\n                |   +----------------------------+     |\n                |   |    kube-controller-manager  |     |\n                |   +----------------------------+     |\n                |   |    etcd                     |     |\n                +--------------------------------------+\n                           /              \\\n                          /                \\\n                         /                  \\\n                        /                    \\\n       +------------------+           +------------------+  \n       |  Worker Node 1   |           |  Worker Node 2   |\n       |                  |           |                  |\n       |  +-----------+   |           |  +-----------+   |\n       |  | Kubelet   |   |           |  | Kubelet   |   |\n       |  +-----------+   |           |  +-----------+   |\n       |  | Kube Proxy|   |           |  | Kube Proxy|   |\n       |  +-----------+   |           |  +-----------+   |\n       |  | Container  |   |           |  | Container  |   |\n       |  | Runtime    |   |           |  | Runtime    |   |\n       |  +-----------+   |           |  +-----------+   |\n       +------------------+           +------------------+\n \n \nKubernetes Manifest Files\nKubernetes uses manifest files (usually written in YAML or JSON) to declare the desired state of various Kubernetes resources. Below are some key examples of manifest files.\n1. Pod Manifest\nA simple pod manifest that defines a pod with a single container running Nginx:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n2. Deployment Manifest\nA deployment manifest that creates a Deployment for Nginx with 3 replicas:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n \n3. Service Manifest\nA service manifest to expose the Nginx deployment:\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n \n4. ConfigMap Manifest\nA ConfigMap for providing configuration data to a pod:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-config\ndata:\n  nginx.conf: |\n    server {\n      listen       80;\n      server_name  localhost;\n      location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n      }\n    }\n \n5. Secret Manifest\nA Secret to store sensitive information like a password:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-password\ntype: Opaque\ndata:\n  password: cGFzc3dvcmQ=  # This is &#039;password&#039; encoded in base64\n \n6. Ingress Manifest\nAn Ingress resource to manage HTTP routing:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\nspec:\n  rules:\n  - host: nginx.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n\nSummary\nKubernetes is a complex system made up of many components that work together to provide a platform for managing containerized applications. The Master Node is responsible for controlling the cluster, while the Worker Nodes run the actual workloads in containers. Resources such as Pods, Services, Deployments, ConfigMaps, and Secrets are defined through YAML manifests that allow for declarative management of the desired state.\nBy using Kubernetes’ powerful abstractions, you can efficiently deploy, scale, and manage applications in a fault-tolerant and automated manner."},"DevOps/For-Reading/Reading-Docker-Inspect-output":{"title":"Reading Docker Inspect output","links":["Task-3-Create-Mysql-Container"],"tags":["reading_tasks","docker"],"content":"Output of Mysql container from Task 3 Create Mysql Container\nInspecting a container\ndocker inspect &lt;container-id&gt;\n[\n  {\n    &quot;Id&quot;: &quot;3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3&quot;,\n    &quot;Created&quot;: &quot;2024-12-29T12:46:23.52003502Z&quot;,\n    &quot;Path&quot;: &quot;docker-entrypoint.sh&quot;,\n    &quot;Args&quot;: [\n      &quot;mysqld&quot;\n    ],\n    &quot;State&quot;: {\n      &quot;Status&quot;: &quot;running&quot;,\n      &quot;Running&quot;: true,\n      &quot;Paused&quot;: false,\n      &quot;Restarting&quot;: false,\n      &quot;OOMKilled&quot;: false,\n      &quot;Dead&quot;: false,\n      &quot;Pid&quot;: 13864,\n      &quot;ExitCode&quot;: 0,\n      &quot;Error&quot;: &quot;&quot;,\n      &quot;StartedAt&quot;: &quot;2024-12-29T12:46:23.928523535Z&quot;,\n      &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;\n    },\n    &quot;Image&quot;: &quot;sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb&quot;,\n    &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/resolv.conf&quot;,\n    &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hostname&quot;,\n    &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/hosts&quot;,\n    &quot;LogPath&quot;: &quot;/var/lib/docker/containers/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3/3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3-json.log&quot;,\n    &quot;Name&quot;: &quot;/mysql_container&quot;,\n    &quot;RestartCount&quot;: 0,\n    &quot;Driver&quot;: &quot;overlay2&quot;,\n    &quot;Platform&quot;: &quot;linux&quot;,\n    &quot;MountLabel&quot;: &quot;&quot;,\n    &quot;ProcessLabel&quot;: &quot;&quot;,\n    &quot;AppArmorProfile&quot;: &quot;&quot;,\n    &quot;ExecIDs&quot;: null,\n    &quot;HostConfig&quot;: {\n      &quot;Binds&quot;: [\n        &quot;mysql_data:/var/lib/mysql&quot;\n      ],\n      &quot;ContainerIDFile&quot;: &quot;&quot;,\n      &quot;LogConfig&quot;: {\n        &quot;Type&quot;: &quot;json-file&quot;,\n        &quot;Config&quot;: {}\n      },\n      &quot;NetworkMode&quot;: &quot;default&quot;,\n      &quot;PortBindings&quot;: {\n        &quot;3306/tcp&quot;: [\n          {\n            &quot;HostIp&quot;: &quot;&quot;,\n            &quot;HostPort&quot;: &quot;3306&quot;\n          }\n        ]\n      },\n      &quot;RestartPolicy&quot;: {\n        &quot;Name&quot;: &quot;no&quot;,\n        &quot;MaximumRetryCount&quot;: 0\n      },\n      &quot;AutoRemove&quot;: false,\n      &quot;VolumeDriver&quot;: &quot;&quot;,\n      &quot;VolumesFrom&quot;: null,\n      &quot;ConsoleSize&quot;: [\n        30,\n        120\n      ],\n      &quot;CapAdd&quot;: null,\n      &quot;CapDrop&quot;: null,\n      &quot;CgroupnsMode&quot;: &quot;private&quot;,\n      &quot;Dns&quot;: [],\n      &quot;DnsOptions&quot;: [],\n      &quot;DnsSearch&quot;: [],\n      &quot;ExtraHosts&quot;: null,\n      &quot;GroupAdd&quot;: null,\n      &quot;IpcMode&quot;: &quot;private&quot;,\n      &quot;Cgroup&quot;: &quot;&quot;,\n      &quot;Links&quot;: null,\n      &quot;OomScoreAdj&quot;: 0,\n      &quot;PidMode&quot;: &quot;&quot;,\n      &quot;Privileged&quot;: false,\n      &quot;PublishAllPorts&quot;: false,\n      &quot;ReadonlyRootfs&quot;: false,\n      &quot;SecurityOpt&quot;: null,\n      &quot;UTSMode&quot;: &quot;&quot;,\n      &quot;UsernsMode&quot;: &quot;&quot;,\n      &quot;ShmSize&quot;: 67108864,\n      &quot;Runtime&quot;: &quot;runc&quot;,\n      &quot;Isolation&quot;: &quot;&quot;,\n      &quot;CpuShares&quot;: 0,\n      &quot;Memory&quot;: 0,\n      &quot;NanoCpus&quot;: 0,\n      &quot;CgroupParent&quot;: &quot;&quot;,\n      &quot;BlkioWeight&quot;: 0,\n      &quot;BlkioWeightDevice&quot;: [],\n      &quot;BlkioDeviceReadBps&quot;: [],\n      &quot;BlkioDeviceWriteBps&quot;: [],\n      &quot;BlkioDeviceReadIOps&quot;: [],\n      &quot;BlkioDeviceWriteIOps&quot;: [],\n      &quot;CpuPeriod&quot;: 0,\n      &quot;CpuQuota&quot;: 0,\n      &quot;CpuRealtimePeriod&quot;: 0,\n      &quot;CpuRealtimeRuntime&quot;: 0,\n      &quot;CpusetCpus&quot;: &quot;&quot;,\n      &quot;CpusetMems&quot;: &quot;&quot;,\n      &quot;Devices&quot;: [],\n      &quot;DeviceCgroupRules&quot;: null,\n      &quot;DeviceRequests&quot;: null,\n      &quot;MemoryReservation&quot;: 0,\n      &quot;MemorySwap&quot;: 0,\n      &quot;MemorySwappiness&quot;: null,\n      &quot;OomKillDisable&quot;: null,\n      &quot;PidsLimit&quot;: null,\n      &quot;Ulimits&quot;: [\n        {\n          &quot;Name&quot;: &quot;nofile&quot;,\n          &quot;Hard&quot;: 65536,\n          &quot;Soft&quot;: 32768\n        }\n      ],\n      &quot;CpuCount&quot;: 0,\n      &quot;CpuPercent&quot;: 0,\n      &quot;IOMaximumIOps&quot;: 0,\n      &quot;IOMaximumBandwidth&quot;: 0,\n      &quot;MaskedPaths&quot;: [\n        &quot;/proc/asound&quot;,\n        &quot;/proc/acpi&quot;,\n        &quot;/proc/kcore&quot;,\n        &quot;/proc/keys&quot;,\n        &quot;/proc/latency_stats&quot;,\n        &quot;/proc/timer_list&quot;,\n        &quot;/proc/timer_stats&quot;,\n        &quot;/proc/sched_debug&quot;,\n        &quot;/proc/scsi&quot;,\n        &quot;/sys/firmware&quot;,\n        &quot;/sys/devices/virtual/powercap&quot;\n      ],\n      &quot;ReadonlyPaths&quot;: [\n        &quot;/proc/bus&quot;,\n        &quot;/proc/fs&quot;,\n        &quot;/proc/irq&quot;,\n        &quot;/proc/sys&quot;,\n        &quot;/proc/sysrq-trigger&quot;\n      ]\n    },\n    &quot;GraphDriver&quot;: {\n      &quot;Data&quot;: {\n        &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f-init/diff:/var/lib/docker/overlay2/96c958faea5ea8ce734cd4db5008bb2188c285dce8190a8c7d35528a9e2ad055/diff:/var/lib/docker/overlay2/643ab6a924a7f65786e4033a2c82e8815cbca24318f429d9f62600d0e7e955a1/diff:/var/lib/docker/overlay2/a9e3b51cd314aa5cad0258b1330d9ca358ff9e4f063ed45f34948ff834b39c0a/diff:/var/lib/docker/overlay2/37efbaa9117f660d35ed59ca9683c1c22a545a8be5621032781bd59467451a29/diff:/var/lib/docker/overlay2/ea28ca7d3354a82731cf3e4947821e814414353c8b431189aec28a2e758946cd/diff:/var/lib/docker/overlay2/729ec68eb4d4f5273a13a068bb24d70d34dcb160fceb795f3058054552b28174/diff:/var/lib/docker/overlay2/6876ecc19a1ae8a61be41a8f298e9e0a012c0ab952944a4923ac46559c32ed3c/diff:/var/lib/docker/overlay2/8b4ab94e45bbe1ab2068b4488f60434de5788dcd4626f2eda8ccd367e2c513ac/diff:/var/lib/docker/overlay2/06533c7033b5c8d5069139b433e42842ebadf121567da334c956001cd942a139/diff:/var/lib/docker/overlay2/d01a0a333a0ac1376c37feec0c1cb771b28631b213fb399ed12ff685085c4b71/diff&quot;,\n        &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/merged&quot;,\n        &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/diff&quot;,\n        &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/12d00f10d37b859750c2db62bb19bd6f3cde4bdca742ecf8a77a7c00fe912f4f/work&quot;\n      },\n      &quot;Name&quot;: &quot;overlay2&quot;\n    },\n    &quot;Mounts&quot;: [\n      {\n        &quot;Type&quot;: &quot;volume&quot;,\n        &quot;Name&quot;: &quot;mysql_data&quot;,\n        &quot;Source&quot;: &quot;/var/lib/docker/volumes/mysql_data/_data&quot;,\n        &quot;Destination&quot;: &quot;/var/lib/mysql&quot;,\n        &quot;Driver&quot;: &quot;local&quot;,\n        &quot;Mode&quot;: &quot;z&quot;,\n        &quot;RW&quot;: true,\n        &quot;Propagation&quot;: &quot;&quot;\n      }\n    ],\n    &quot;Config&quot;: {\n      &quot;Hostname&quot;: &quot;3ce6ffd3357d&quot;,\n      &quot;Domainname&quot;: &quot;&quot;,\n      &quot;User&quot;: &quot;&quot;,\n      &quot;AttachStdin&quot;: false,\n      &quot;AttachStdout&quot;: true,\n      &quot;AttachStderr&quot;: true,\n      &quot;ExposedPorts&quot;: {\n        &quot;3306/tcp&quot;: {},\n        &quot;33060/tcp&quot;: {}\n      },\n      &quot;Tty&quot;: false,\n      &quot;OpenStdin&quot;: false,\n      &quot;StdinOnce&quot;: false,\n      &quot;Env&quot;: [\n        &quot;MYSQL_USER=name&quot;,\n        &quot;MYSQL_PASSWORD=passwd&quot;,\n        &quot;MYSQL_RANDOM_ROOT_PASSWORD=yes&quot;,\n        &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,\n        &quot;GOSU_VERSION=1.17&quot;,\n        &quot;MYSQL_MAJOR=innovation&quot;,\n        &quot;MYSQL_VERSION=9.1.0-1.el9&quot;,\n        &quot;MYSQL_SHELL_VERSION=9.1.0-1.el9&quot;\n      ],\n      &quot;Cmd&quot;: [\n        &quot;mysqld&quot;\n      ],\n      &quot;Image&quot;: &quot;mysql:latest&quot;,\n      &quot;Volumes&quot;: {\n        &quot;/var/lib/mysql&quot;: {}\n      },\n      &quot;WorkingDir&quot;: &quot;/&quot;,\n      &quot;Entrypoint&quot;: [\n        &quot;docker-entrypoint.sh&quot;\n      ],\n      &quot;OnBuild&quot;: null,\n      &quot;Labels&quot;: {}\n    },\n    &quot;NetworkSettings&quot;: {\n      &quot;Bridge&quot;: &quot;&quot;,\n      &quot;SandboxID&quot;: &quot;86cf8ccab46e8354b1949fbc9c3c96452864b775418ee65163a356bb7de1eaac&quot;,\n      &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/86cf8ccab46e&quot;,\n      &quot;Ports&quot;: {\n        &quot;3306/tcp&quot;: [\n          {\n            &quot;HostIp&quot;: &quot;0.0.0.0&quot;,\n            &quot;HostPort&quot;: &quot;3306&quot;\n          },\n          {\n            &quot;HostIp&quot;: &quot;::&quot;,\n            &quot;HostPort&quot;: &quot;3306&quot;\n          }\n        ],\n        &quot;33060/tcp&quot;: null\n      },\n      &quot;HairpinMode&quot;: false,\n      &quot;LinkLocalIPv6Address&quot;: &quot;&quot;,\n      &quot;LinkLocalIPv6PrefixLen&quot;: 0,\n      &quot;SecondaryIPAddresses&quot;: null,\n      &quot;SecondaryIPv6Addresses&quot;: null,\n      &quot;EndpointID&quot;: &quot;b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2&quot;,\n      &quot;Gateway&quot;: &quot;172.17.0.1&quot;,\n      &quot;GlobalIPv6Address&quot;: &quot;&quot;,\n      &quot;GlobalIPv6PrefixLen&quot;: 0,\n      &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,\n      &quot;IPPrefixLen&quot;: 16,\n      &quot;IPv6Gateway&quot;: &quot;&quot;,\n      &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;,\n      &quot;Networks&quot;: {\n        &quot;bridge&quot;: {\n          &quot;IPAMConfig&quot;: null,\n          &quot;Links&quot;: null,\n          &quot;Aliases&quot;: null,\n          &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;,\n          &quot;NetworkID&quot;: &quot;1306a6d500d7b8f9a7da615852fab3d6ee12750f0d8550e6292d8ed5b5850930&quot;,\n          &quot;EndpointID&quot;: &quot;b28accc98581fb0504cd3e243023616b47126553a7b2c69ef1b443c351ad7fe2&quot;,\n          &quot;Gateway&quot;: &quot;172.17.0.1&quot;,\n          &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,\n          &quot;IPPrefixLen&quot;: 16,\n          &quot;IPv6Gateway&quot;: &quot;&quot;,\n          &quot;GlobalIPv6Address&quot;: &quot;&quot;,\n          &quot;GlobalIPv6PrefixLen&quot;: 0,\n          &quot;DriverOpts&quot;: null,\n          &quot;DNSNames&quot;: null\n        }\n      }\n    }\n  }\n]\nExplanation\nThe docker inspect command provides a detailed view of a Docker object’s configuration. This could be a container, image, network, volume, etc. You’ve provided the output for a container. Let’s break down the key components:\n1. Id: 3ce6ffd3357d7d78a4fff68218ea477ef0bcc3a747153cb93143e128bb21e4a3\n\nThis is the unique identifier for your container.\n\n2. Created: 2024-12-29T12:46:23.52003502Z\n\nTimestamp indicating when the container was created (in UTC).\n\n3. Path: docker-entrypoint.sh\n\nThis is the default command that will be executed within the container when it starts. Entrypoint scripts are useful for setting up the container environment, running services, or executing other tasks.\n\n4. Args: [&quot;mysqld&quot;]\n\nThese are the arguments passed to the Path (entrypoint). In this case, “mysqld” is likely the command to start the MySQL server.\n\n5. State:\n\nStatus: running - Current state of the container.\nRunning: true - Indicates if the container is currently running.\nPaused: false - Indicates if the container is paused.\nRestarting: false - Indicates if the container is in the process of restarting.\nOOMKilled: false - Indicates if the container was killed due to running out of memory (Out Of Memory).\nDead: false - Indicates if the container has exited.\nPid: 13864 - Process ID of the main process running inside the container.\nExitCode: 0 - Exit code of the container (0 usually means success).\nError: &quot;&quot; - Any error message associated with the container’s state.\nStartedAt: 2024-12-29T12:46:23.928523535Z - Timestamp when the container was started.\nFinishedAt: 0001-01-01T00:00:00Z - Timestamp when the container finished (if applicable).\n\n6. Image: sha256:56a8c14e14044b8ec7ffb4dd165c8dbe10d4c6ba3d9e754f0c906f52a0b5b4fb\n\nThe ID (SHA256 hash) of the image that was used to create this container.\n\n7. ResolvConfPath, HostnamePath, HostsPath, LogPath:\n\nPaths to files within the container’s filesystem that handle DNS resolution, hostname, host mappings, and container logs.\n\n8. Name: /mysql_container\n\nThe name you assigned to the container when you created it.\n\n9. RestartCount: 0\n\nNumber of times the container has been restarted.\n\n10. Driver: overlay2\n\nThe storage driver used for the container’s filesystem layers.\n\n11. Platform: linux\n\nThe platform (OS) for which the container was built.\n\n12. MountLabel, ProcessLabel, AppArmorProfile: - Security-related labels and profiles.\n13. ExecIDs: null - List of IDs of exec instances that have been run in this container (if any).\n14. HostConfig: This section contains container settings that are specific to the host machine where the container is running:\n- **Binds:** `[&quot;mysql_data:/var/lib/mysql&quot;]` -  Shows volumes mounted inside the container. Here, a volume named &quot;mysql_data&quot; is mounted to `/var/lib/mysql` inside the container. This is often used for persistent storage.\n- **LogConfig:** Configuration for container logging.\n- **NetworkMode:** `default` - Networking mode for the container.\n- **PortBindings:** `{&quot;3306/tcp&quot;: [{&quot;HostIp&quot;: &quot;&quot;, &quot;HostPort&quot;: &quot;3306&quot;}]}` -  Port mappings between the host and the container. This exposes the container&#039;s port 3306 on the host&#039;s port 3306.\n- **RestartPolicy:** `{&quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0}` - Container restart policy (in this case, it won&#039;t restart automatically).\n- **AutoRemove:** `false` - Whether the container should be automatically removed when it exits.\n- **[Many other settings]:**  Resource limits (memory, CPU), security options, and other low-level configurations.\n\n15. GraphDriver: - Data: Information about the graph driver (storage driver) used by the container.\n16. Mounts: - Detailed information about the mounts configured for the container, including the source, destination, and options.\n17. Config: This section describes the container’s configuration, which is defined by the image and any modifications you made:\n- **Hostname:** `3ce6ffd3357d` - Hostname of the container.\n- **Domainname:**  Domain name of the container.\n- **User:** User that the container runs as.\n- **AttachStdin, AttachStdout, AttachStderr:**  Whether to attach to standard input/output/error streams.\n- **ExposedPorts:**  Ports that the container exposes (may or may not be published to the host).\n- **Tty:**  Whether the container has a pseudo-TTY allocated.\n- **OpenStdin:** Whether to keep STDIN open even if not attached.\n- **Env:** `[&quot;MYSQL_USER=name&quot;, &quot;MYSQL_PASSWORD=passwd&quot;, ...]` - Environment variables set within the container.\n- **Cmd:** `[&quot;mysqld&quot;]` -  The command to be executed when the container starts.\n- **Image:** `mysql:latest` - The image the container was created from.\n- **Volumes:** Volumes defined in the container configuration.\n- **WorkingDir:** `/` - The working directory inside the container.\n- **Entrypoint:** `[&quot;docker-entrypoint.sh&quot;]` - The entrypoint script for the container.\n- **OnBuild:**  Triggers that execute in the context of the image build.\n- **Labels:**  Metadata labels associated with the container.\n\n18. NetworkSettings: This section provides details about the container’s network configuration:\n- **Bridge:**  Name of the bridge network the container is attached to.\n- **SandboxID, SandboxKey:** Information related to the container&#039;s network namespace.\n- **Ports:**  Details about the ports that are published to the host.\n- **[Other network settings]:** IP address, MAC address, gateway, and other network-related information.\n\nIf you have a specific question about any of these elements, feel free to ask! I’m here to help you understand your Docker container’s configuration."},"DevOps/For-Reading/k8s-Namespace":{"title":"k8s Namespace","links":[],"tags":[],"content":"In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.).\nWhen to Use Multiple Namespaces\nNamespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.\nNamespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.\nNamespaces are a way to divide cluster resources between multiple users (via resource quota).\nIt is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.\nNote:\nFor a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.\nInitial namespaces\nKubernetes starts with four initial namespaces:\ndefault\nKubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.\nkube-node-lease\nThis namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.\nkube-public\nThis namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.\nkube-system\nThe namespace for objects created by the Kubernetes system.\nWorking with Namespaces\nCreation and deletion of namespaces are described in the Admin Guide documentation for namespaces.\nNote:\nAvoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.\nViewing namespaces\nYou can list the current namespaces in a cluster using:\nkubectl get namespace\nNAME              STATUS   AGE\ndefault           Active   1d\nkube-node-lease   Active   1d\nkube-public       Active   1d\nkube-system       Active   1d\n\nSetting the namespace for a request\nTo set the namespace for a current request, use the --namespace flag.\nFor example:\nkubectl run nginx --image=nginx --namespace=&lt;insert-namespace-name-here&gt;\nkubectl get pods --namespace=&lt;insert-namespace-name-here&gt;\nSetting the namespace preference\nYou can permanently save the namespace for all subsequent kubectl commands in that context.\nkubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;\n# Validate it\nkubectl config view --minify | grep namespace:\nNamespaces and DNS\nWhen you create a Service, it creates a corresponding DNS entry. This entry is of the form &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local, which means that if a container only uses &lt;service-name&gt;, it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).\nAs a result, all namespace names must be valid RFC 1123 DNS labels.\nWarning:\nBy creating namespaces with the same name as public top-level domains, Services in these namespaces can have short DNS names that overlap with public DNS records. Workloads from any namespace performing a DNS lookup without a trailing dot will be redirected to those services, taking precedence over public DNS.\nTo mitigate this, limit privileges for creating namespaces to trusted users. If required, you could additionally configure third-party security controls, such as admission webhooks, to block creating any namespace with the name of public TLDs.\nNot all objects are in a namespace\nMost Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes, are not in any namespace.\nTo see which Kubernetes resources are and aren’t in a namespace:\n# In a namespace\nkubectl api-resources --namespaced=true\n \n# Not in a namespace\nkubectl api-resources --namespaced=false\nAutomatic labelling\nFEATURE STATE: Kubernetes 1.22 [stable]\nThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces. The value of the label is the namespace name."},"DevOps/For-Reading/k8s-Volume":{"title":"k8s Volume","links":[],"tags":[],"content":"On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. First, when a container crashes kubelet will restart it, but the files will be lost - the container starts with a clean state. Second, when running containers together in a Pod it is often necessary to share files between those containers. The Kubernetes Volume abstraction solves both of these problems.\nBackground\nDocker also has a concept of volumes, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another container. Lifetimes are not managed and until very recently there were only local-disk-backed volumes. Docker now provides volume drivers, but the functionality is very limited for now (e.g. as of Docker 1.7 only one volume driver is allowed per container and there is no way to pass parameters to volumes).\nA Kubernetes volume, on the other hand, has an explicit lifetime - the same as the pod that encloses it. Consequently, a volume outlives any containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many type of volumes, and a Pod can use any number of them simultaneously.\nAt its core, a volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.\nTo use a volume, a pod specifies what volumes to provide for the pod (the spec.volumes field) and where to mount those into containers(the spec.containers.volumeMounts field).\nA process in a container sees a filesystem view composed from their Docker image and volumes. The Docker image is at the root of the filesystem hierarchy, and any volumes are mounted at the specified paths within the image. Volumes can not mount onto other volumes or have hard links to other volumes. Each container in the Pod must independently specify where to mount each volume.\nTypes of Volumes\nKubernetes supports several types of Volumes:\n\nemptyDir\nhostPath\ngcePersistentDisk\nawsElasticBlockStore\nnfs\niscsi\nflocker\nglusterfs\nrbd\ncephfs\ngitRepo\nsecret\npersistentVolumeClaim\ndownwardAPI\nazureFileVolume\nazureDisk\nvsphereVolume\nQuobyte\nPortworxVolume\nScaleIO\n\nWe welcome additional contributions.\nemptyDir\nAn emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the pod can all read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. NOTE: a container crashing does NOT remove a pod from a node, so the data in an emptyDir volume is safe across container crashes.\nSome uses for an emptyDir are:\n\nscratch space, such as for a disk-based merge sort\ncheckpointing a long computation for recovery from crashes\nholding files that a content-manager container fetches while a webserver container serves the data\n\nBy default, emptyDir volumes are stored on whatever medium is backing the machine - that might be disk or SSD or network storage, depending on your environment. However, you can set the emptyDir.medium field to &quot;Memory&quot; to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on machine reboot and any files you write will count against your container’s memory limit.\nExample pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /cache\n      name: cache-volume\n  volumes:\n  - name: cache-volume\n    emptyDir: {}\n\nhostPath\nA hostPath volume mounts a file or directory from the host node’s filesystem into your pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.\nFor example, some uses for a hostPath are:\n\nrunning a container that needs access to Docker internals; use a hostPath of /var/lib/docker\nrunning cAdvisor in a container; use a hostPath of /dev/cgroups\n\nWatch out when using this type of volume, because:\n\npods with identical configuration (such as created from a podTemplate) may behave differently on different nodes due to different files on the nodes\nwhen Kubernetes adds resource-aware scheduling, as is planned, it will not be able to account for resources used by a hostPath\nthe directories created on the underlying hosts are only writable by root. You either need to run your process as root in a privileged container or modify the file permissions on the host to be able to write to a hostPath volume\n\nExample pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - name: test-volume\n    hostPath:\n      # directory location on host\n      path: /data\n\ngcePersistentDisk\nA gcePersistentDisk volume mounts a Google Compute Engine (GCE) Persistent Disk into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of a PD are preserved and the volume is merely unmounted. This means that a PD can be pre-populated with data, and that data can be “handed off” between pods.\nImportant: You must create a PD using gcloud or the GCE API or UI before you can use it\nThere are some restrictions when using a gcePersistentDisk:\n\nthe nodes on which pods are running must be GCE VMs\nthose VMs need to be in the same GCE project and zone as the PD\n\nA feature of PD is that they can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a PD with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, PDs can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.\nUsing a PD on a pod controlled by a ReplicationController will fail unless the PD is read-only or the replica count is 0 or 1.\nCreating a PD\nBefore you can use a GCE PD with a pod, you need to create it.\ngcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk\n\nExample pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - name: test-volume\n    # This GCE PD must already exist.\n    gcePersistentDisk:\n      pdName: my-data-disk\n      fsType: ext4\n\nawsElasticBlockStore\nAn awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be “handed off” between pods.\nImportant: You must create an EBS volume using aws ec2 create-volume or the AWS API before you can use it\nThere are some restrictions when using an awsElasticBlockStore volume:\n\nthe nodes on which pods are running must be AWS EC2 instances\nthose instances need to be in the same region and availability-zone as the EBS volume\nEBS only supports a single EC2 instance mounting a volume\n\nCreating an EBS volume\nBefore you can use an EBS volume with a pod, you need to create it.\naws ec2 create-volume --availability-zone eu-west-1a --size 10 --volume-type gp2\n\nMake sure the zone matches the zone you brought up your cluster in. (And also check that the size and EBS volume type are suitable for your use!)\nAWS EBS Example configuration\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-ebs\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-ebs\n      name: test-volume\n  volumes:\n  - name: test-volume\n    # This AWS EBS volume must already exist.\n    awsElasticBlockStore:\n      volumeID: &lt;volume-id&gt;\n      fsType: ext4\n\nnfs\nAn nfs volume allows an existing NFS (Network File System) share to be mounted into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be “handed off” between pods. NFS can be mounted by multiple writers simultaneously.\nImportant: You must have your own NFS server running with the share exported before you can use it\nSee the NFS example for more details.\niscsi\nAn iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an iscsi volume are preserved and the volume is merely unmounted. This means that an iscsi volume can be pre-populated with data, and that data can be “handed off” between pods.\nImportant: You must have your own iSCSI server running with the volume created before you can use it\nA feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.\nSee the iSCSI example for more details.\nflocker\nFlocker is an open-source clustered container data volume manager. It provides management and orchestration of data volumes backed by a variety of storage backends.\nA flocker volume allows a Flocker dataset to be mounted into a pod. If the dataset does not already exist in Flocker, it needs to be first created with the Flocker CLI or by using the Flocker API. If the dataset already exists it will be reattached by Flocker to the node that the pod is scheduled. This means data can be “handed off” between pods as required.\nImportant: You must have your own Flocker installation running before you can use it\nSee the Flocker example for more details.\nglusterfs\nA glusterfs volume allows a Glusterfs (an open source networked filesystem) volume to be mounted into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of a glusterfs volume are preserved and the volume is merely unmounted. This means that a glusterfs volume can be pre-populated with data, and that data can be “handed off” between pods. GlusterFS can be mounted by multiple writers simultaneously.\nImportant: You must have your own GlusterFS installation running before you can use it\nSee the GlusterFS example for more details.\nrbd\nAn rbd volume allows a Rados Block Device volume to be mounted into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of a rbd volume are preserved and the volume is merely unmounted. This means that a RBD volume can be pre-populated with data, and that data can be “handed off” between pods.\nImportant: You must have your own Ceph installation running before you can use RBD\nA feature of RBD is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many pods as you need. Unfortunately, RBD volumes can only be mounted by a single consumer in read-write mode - no simultaneous writers allowed.\nSee the RBD example for more details.\ncephfs\nA cephfs volume allows an existing CephFS volume to be mounted into your pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of a cephfs volume are preserved and the volume is merely unmounted. This means that a CephFS volume can be pre-populated with data, and that data can be “handed off” between pods. CephFS can be mounted by multiple writers simultaneously.\nImportant: You must have your own Ceph server running with the share exported before you can use it\nSee the CephFS example for more details.\ngitRepo\nA gitRepo volume is an example of what can be done as a volume plugin. It mounts an empty directory and clones a git repository into it for your pod to use. In the future, such volumes may be moved to an even more decoupled model, rather than extending the Kubernetes API for every such use case.\nHere is an example for gitRepo volume:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: server\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /mypath\n      name: git-volume\n  volumes:\n  - name: git-volume\n    gitRepo:\n      repository: &quot;git@somewhere:me/my-git-repository.git&quot;\n      revision: &quot;22f1d8406d464b0c0874075539c1f2e96c253775&quot;\n\nsecret\nA secret volume is used to pass sensitive information, such as passwords, to pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.\nImportant: You must create a secret in the Kubernetes API before you can use it\nSecrets are described in more detail here.\npersistentVolumeClaim\nA persistentVolumeClaim volume is used to mount a PersistentVolume into a pod. PersistentVolumes are a way for users to “claim” durable storage (such as a GCE PersistentDisk or an iSCSI volume) without knowing the details of the particular cloud environment.\nSee the PersistentVolumes example for more details.\ndownwardAPI\nA downwardAPI volume is used to make downward API data available to applications. It mounts a directory and writes the requested data in plain text files.\nSee the downwardAPI volume example for more details.\nFlexVolume\nA FlexVolume enables users to mount vendor volumes into a pod. It expects vendor drivers are installed in the volume plugin path on each kubelet node. This is an alpha feature and may change in future.\nMore details are in here\nAzureFileVolume\nA AzureFileVolume is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0) into a Pod.\nMore details can be found here\nAzureDiskVolume\nA AzureDiskVolume is used to mount a Microsoft Azure Data Disk into a Pod.\nMore details can be found here\nvsphereVolume\nPrerequisite: Kubernetes with vSphere Cloud Provider configured. For cloudprovider configuration please refer vSphere getting started guide.\nA vsphereVolume is used to mount a vSphere VMDK Volume into your Pod. The contents of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.\nImportant: You must create VMDK using one of the following method before using with POD.\nCreating a VMDK volume\n\nCreate using vmkfstools.\n\nFirst ssh into ESX and then use following command to create vmdk,\n    vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk\n\n\nCreate using vmware-vdiskmanager.\n\n  vmware-vdiskmanager -c -t 0 -s 40GB -a lsilogic myDisk.vmdk\n\nvSphere VMDK Example configuration\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-vmdk\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-vmdk\n      name: test-volume\n  volumes:\n  - name: test-volume\n    # This VMDK volume must already exist.\n    vsphereVolume:\n      volumePath: &quot;[DatastoreName] volumes/myDisk&quot;\n      fsType: ext4\n\nMore examples can be found here.\nQuobyte\nA Quobyte volume allows an existing Quobyte volume to be mounted into your pod.\nImportant: You must have your own Quobyte setup running with the volumes created before you can use it\nSee the Quobyte example for more details.\nPortworxVolume\nA PortworxVolume is an elastic block storage layer that runs hyperconverged with Kubernetes. Portworx fingerprints storage in a server, tiers based on capabilities, and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.\nA PortworxVolume can be dynamically created through Kubernetes or it can also be pre-provisioned and referenced inside a Kubernetes pod. Here is an example pod referencing a pre-provisioned PortworxVolume:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-portworx-volume-pod\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /mnt\n      name: pxvol\n  volumes:\n  - name: pxvol\n    # This Portworx volume must already exist.\n    portworxVolume:\n      volumeID: &quot;pxvol&quot;\n      fsType: &quot;&lt;fs-type&gt;&quot;\n\nImportant: Make sure you have an existing PortworxVolume with name pxvol before using it in the pod\nMore details and examples can be found here\nScaleIO\nScaleIO is a software-based storage platform that can use existing hardware to create clusters of scalable shared block networked storage. The ScaleIO volume plugin allows deployed pods to access existing ScaleIO volumes (or it can dynamically provision new volumes for persistent volume claims, see ScaleIO Persistent Volumes).\nImportant: You must have an existing ScaleIO cluster already setup and running with the volumes created before you can use them\nThe following is an example pod configuration with ScaleIO:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-0\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: pod-0\n    volumeMounts:\n    - mountPath: /test-pd\n      name: vol-0\n  volumes:\n  - name: vol-0\n    scaleIO:\n      gateway: https://localhost:443/api\n      system: scaleio\n      volumeName: vol-0\n      secretRef:\n        name: sio-secret\n      fsType: xfs\n\nFor further detail, plese the see the ScaleIO examples.\nUsing subPath\nSometimes, it is useful to share one volume for multiple uses in a single pod. The volumeMounts.subPath property can be used to specify a sub-path inside the referenced volume instead of its root.\nHere is an example of a pod with a LAMP stack (Linux Apache Mysql PHP) using a single, shared volume. The HTML contents are mapped to its html folder, and the databases will be stored in its mysql folder:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-lamp-site\nspec:\n    containers:\n    - name: mysql\n      image: mysql\n      volumeMounts:\n      - mountPath: /var/lib/mysql\n        name: site-data\n        subPath: mysql\n    - name: php\n      image: php\n      volumeMounts:\n      - mountPath: /var/www/html\n        name: site-data\n        subPath: html\n    volumes:\n    - name: site-data\n      persistentVolumeClaim:\n        claimName: my-lamp-site-data\n\nResources\nThe storage media (Disk, SSD, etc.) of an emptyDir volume is determined by the medium of the filesystem holding the kubelet root dir (typically /var/lib/kubelet). There is no limit on how much space an emptyDir or hostPath volume can consume, and no isolation between containers or between pods.\nIn the future, we expect that emptyDir and hostPath volumes will be able to request a certain amount of space using a resource specification, and to select the type of media to use, for clusters that have several media types."},"DevOps/For-Reading/read-about-docker.sock-file":{"title":"read about docker.sock file","links":[],"tags":["reading_tasks","docker","socket"],"content":"What is docker.sock file\nBasically this file act as a bridge between docker daemon and docker client.\nIts responsible for api communications between docker client and dockerd service that runs in the background.\nSo when you run docker run nginx\ndocker client send request to this file over at /var/run/docker.sock and then docker daemon which is constantly listening for this socket it receives the request and create container.\n\nFrom Gemini Ai\nThe docker.sock file is a Unix socket that allows communication between the Docker client and the Docker daemon (the background process that manages containers). It acts like a bridge, enabling you to manage containers, images, networks, and more.\nHere’s a breakdown:\n\nWhat is a socket? A socket is a way for programs to communicate with each other, either on the same machine or over a network. Think of it like a phone line specifically for applications.\nWhy a socket instead of a network connection? Using a Unix socket for communication between the client and daemon on the same host is more efficient and secure than using network-based protocols like HTTP or TCP.\nHow it works: When you run a Docker command (e.g., docker run), the client sends the command to the docker.sock file. The Docker daemon, listening on the other end of the socket, receives the command and executes it.\nLocation: The docker.sock file is typically located at /var/run/docker.sock on Linux systems.\nSecurity: The docker.sock file is usually owned by the root user and the docker group, which restricts access and enhances security.\n\nKey points to remember:\n\nIt’s the primary way to interact with the Docker daemon.\nIt enables efficient and secure communication.\nIt’s a critical component for managing Docker containers and images.\nMisusing or exposing the docker.sock file can lead to security risks.\n\nIf you’re interested in diving deeper, I recommend checking out these resources:\n\nUnderstanding /var/run/docker.sock: dev.to/piyushbagani15/understanding-varrundockersock-the-key-to-dockers-inner-workings-nm7\nWhat is Docker socket binding? www.geeksforgeeks.org/what-is-docker-socket-binding/\nDocker Tips: about /var/run/docker.sock: www.geeksforgeeks.org/docker-tips-about-varrundockersock/\n\nInteresting Read From a Reddit user”\nWhen you install docker in a machine. Two diffrent programs come in:\n\nDocker Client\nDocker Server\n\nDocker Server receives commands over a socket (either over a network or through a “file”)\nDocker Client communicates over a network and sends message to the Docker server to say make a container, start a container, stop a container etc.\nWhen the client and server are running on the same computer, they can connect through a special file called a socket. And since they can communicate through a file and Docker can efficiently share files between hosts and containers, it means you can run the client inside Docker itself.\nHere is a sample:\ndocker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker sh\nThis command creates a container that docker client installed within. And check the volume part: -v /var/run/docker.sock:/var/run/docker.sock\nWith -v flag it shares host docker.sock file so you can manipulate the containers within the host via a container.\ndocker run --rm -it ubuntu bash # Creates a new container via container \nRun docker ps on host terminal.\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\n0f9e333b59fe   ubuntu    &quot;bash&quot;                   5 seconds ago    Up 4 seconds              zealous_wilson\nb4a8af31416b   docker    &quot;docker-entrypoint.s…&quot;   16 minutes ago   Up 16 minutes             epic_elion\nFrom Brave Ai\nDocker.sock file\nThe Docker.sock file, also known as the Docker socket, is a Unix domain socket (UDS) file used by Docker to communicate between the Docker daemon (dockerd) and the Docker client. It allows the client to send commands to the daemon and receive responses.\nLocation\nBy default, the Docker.sock file is located at /var/run/docker.sock on Linux systems and C:\\ProgramData\\docker\\socket on Windows systems.\nFunctionality\nThe Docker.sock file enables the following:\n\nCommand execution: The Docker client sends commands (e.g., docker run, docker ps) to the daemon through the socket, which executes the requested action.\nResponse retrieval: The daemon sends responses (e.g., container output, error messages) back to the client through the socket.\n\nSecurity considerations\n\nAccess control: The Docker.sock file is owned by the docker group and has restrictive permissions by default, ensuring that only authorized users can access it.\nEncryption: The communication between the client and daemon is not encrypted by default. To secure the socket, you can enable TLS encryption using the --tls and --tlscert flags when starting the daemon.\n\nMounting the socket\nYou can mount the Docker.sock file inside a container using the -v flag when running the container. For example:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock my-image\nThis allows the container to access the Docker socket and interact with the Docker daemon on the host system.\nAlternatives\nIf you need to access the Docker daemon remotely or securely, consider using a TCP socket (e.g., dockerd -H tcp://0.0.0.0:2375) or a proxy like Docker Compose or Kubernetes.\n\nSome External Links for good reads on this subject\n\nlobster1234.github.io/2019/04/05/docker-socket-file-for-ipc/\nstackoverflow.com/questions/35110146/what-is-the-purpose-of-the-file-docker-sock\nwww.educative.io/answers/var-run-dockersock\n"},"DevOps/K8S-Tasks/3-Microservices-Task":{"title":"3 Microservices Task","links":[],"tags":[],"content":"1. Project Setup and Namespace Creation\nBooting up EC2 and Cluster\n\nLaunch Ec2 instance\nAdd IAM Role\nInstall Docker and Kubernetes Packages\nCreate Cluster\n\nDownload Free css template\ncurl -O www.free-css.com/assets/files/free-css-templates/download/page296/neogym.zip\n \nunzip neogym.zip\n \nmv neogym-html/* html/\n2. Microservice 1: Nginx with Free CSS Template\n\nDockerfile (Dockerfile_nginx):\n\nFROM nginx:stable-alpine3.20-perl \nCOPY ./html/ /usr/share/nginx/html/\nEXPOSE 80\n \nCMD [&quot;/usr/bin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n\nBuild and Push Docker Image:\n\n# Build Image\ndocker build -t nginx-free-css:latest -f Dockerfile_nginx . \n# Add tag to repo-name/image name for docker hub push\ndocker tag nginx-css &lt;repo-name&gt;/nginx-css\n# Push to docker hub repo\ndocker push nginx-free-css:latest\n\nCreate Kubernetes Namespaces:\nCreate 3 Namespaces for 3 Microservices\n\nkubectl create namespace dev-nginx\nkubectl create namespace dev-httpd\nkubectl create namespace dev-tomcat\nCreate Deployments\n\nKubernetes Deployment (dev-nginx namespace):\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: nginx-deployment\n      namespace: dev-nginx\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: nginx\n      template:\n        metadata:\n          labels:\n            app: nginx\n        spec:\n          containers:\n          - name: nginx\n            image: nginx-free-css:latest\n            ports:\n            - containerPort: 80\n\nApply Deployment:\n\nkubectl apply -f nginx-deployment.yaml -n dev-nginx\n3. Microservice 2: HTTP Server with ConfigMap\n\nConfigMap (dev-httpd namespace):\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: index-html\n  namespace: dev-httpd\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n      &lt;title&gt;HTTP Server&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n      &lt;h1&gt;Hello from HTTP Server!&lt;/h1&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n\nDeployment (dev-httpd namespace):\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: httpd-deployment\n  namespace: dev-httpd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: httpd\n  template:\n    metadata:\n      labels:\n        app: httpd\n    spec:\n      containers:\n      - name: httpd\n        image: httpd:latest \n        volumeMounts:\n        - name: config-volume\n          mountPath: /usr/local/apache2/htdocs/ \n      volumes:\n      - name: config-volume\n        configMap:\n          name: index-html\n\nApply ConfigMap and Deployment:\n\nkubectl apply -f index-html.yaml -n dev-httpd\nkubectl apply -f httpd-deployment.yaml -n dev-httpd\n4. Microservice 3: Tomcat\n\nDeployment (dev-tomcat namespace):\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev-tomcat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tomcat\n  template:\n    metadata:\n      labels:\n        app: tomcat\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:latest\n        ports:\n        - containerPort: 8080\n\nApply Deployment:\n\nkubectl apply -f tomcat-deployment.yaml -n dev-tomcat\n5. Nginx Ingress Controller (nginx-ingress namespace)\n\nInstall Ingress Controller:\n\nRefer to the official Nginx Ingress Controller documentation for installation instructions.\n\n\n\n6. Ingress Configuration (nginx-ingress namespace)\n\nCreate Ingress resource:\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: microservices-ingress\nspec:\n  rules:\n  - host: httpd.your-domain.com \n    http:\n      paths:\n      - path: /\n        pathType: Prefix \n        backend:\n          service:\n            name: httpd-service \n            port:\n              number: 80\n  - host: tomcat.your-domain.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: tomcat-service\n            port:\n              number: 8080\n  - host: nginx.your-domain.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 80\n\n\nCreate Services for each Deployment:\n\nYou’ll need to create Services of type LoadBalancer or NodePort for each Deployment to expose them.\n\n\n\nApply Ingress and Services:\n\n\nkubectl apply -f microservices-ingress.yaml -n nginx-ingress \n# Apply Service YAML files for each Deployment \n\n7. DNS Configuration\n\nConfigure your DNS records to point the following domains to the Ingress Controller’s IP address:\n\nhttpd.your-domain.com\ntomcat.your-domain.com\nnginx.your-domain.com\n\n\n\n8. Node Assignment (Manual)\n\nNode 1: Schedule the httpd-deployment to run on Node 1.\nNode 2: Schedule the tomcat-deployment to run on Node 2.\nNode 3: Schedule the nginx-deployment to run on Node 3.\n\nNote:\n\nReplace placeholders like your-domain.com with actual domain names.\nAdjust paths, ports, and other configurations as per your requirements.\nThis is a basic example. You may need to adjust the configurations based on your specific needs and security requirements.\nConsider using a more robust service discovery mechanism like Kubernetes Service.\nFor production environments, use a more advanced deployment strategy like rolling updates or blue/green deployments.\n"},"DevOps/K8S-Tasks/K8s-Task-1":{"title":"K8s Task 1","links":[],"tags":[],"content":"Deployment of Nginx Free CSS Template on K8s\nStep 1: Booting up EC2 and Cluster\n\nLaunch Ec2 instance\nAdd IAM Role\nInstall Docker and Kubernetes Packages\nCreate Cluster\n\nStep 2: Download Free css template\ncurl -O www.free-css.com/assets/files/free-css-templates/download/page296/neogym.zip\n \nunzip neogym.zip\n \nmv neogym-html/* html/\nStep 2: Docker Image Creation\n\nCreate Dockerfile\n\nFROM nginx:stable-alpine3.20-perl \nCOPY ./html/ /usr/share/nginx/html/\nEXPOSE 80\n \nCMD [&quot;/usr/bin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n\nBuild Image and Push it to Docker hub\n\ndocker build . -t nginx-css\n\ndocker tag nginx-css &lt;repo-name&gt;/nginx-css\n\ndocker push &lt;repo-name&gt;/nginx-css\n\nStep 3: Create Deployment file\n# Create namespace\nkubectl create ns dev-nginx\n\nDeployment File\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-css\n  namespace: dev-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-css\n  template:\n    metadata:\n      labels:\n        app: nginx-css\n    spec:\n      containers:\n      - name: nginx\n        image: archsarangx/nginx-css\n        ports:\n        - containerPort: 80\n \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev-nginx\nspec:\n  selector:\n    app: nginx-css\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer\nInstalling Helm\nhelm.sh"},"DevOps/K8S-Tasks/Tomcat-on-single-node":{"title":"Tomcat on single node","links":[],"tags":[],"content":"Task\n\n1 Microservice = Tomcat\nReplicaSet 1\nAdd Node selector\nadd pod affinity\nadd node affinity\nImplement probe to check to see if service is running\nAdd Database tomcat in Cluster with persistent volume\nUse ingress LB\ntomcat app resource alloaction:\n\nRequest\nCPU: 500m\nMem: 1GB\nLimit\ncpu: 1\nmemory: 2gb\n\n\nhard coded values can be sent so no secret will be used\nor\nUse online bookshop application where secrets and configMap can be used\n"},"DevOps/Notes/Docker-Theory-and-Practical":{"title":"Docker Theory and Practical","links":[],"tags":["docker"],"content":"\nBefore Learning about docker it is crucial to know about containers and even before that one should know what is Microservices and monolithic application architecture\nMonolithic vs Microservice Apps\nAt initial stages of the software development developers usually build software in monolithic way meaning everything that software has to offer is bundle in the single unit but as the development goes on the need to federate the services of the software arises and developers adapt Microservices method to divide software into chunks so all chunks communicate with each other creating illusion of one single software.\nMonolithic Application\n\nSingle unified application\n\nMicroservice app\n\none app divided in chunks\n\nWhat is docker?\n*Docker is a containerization technology that packages your application in special portable sandboxed format along with its dependencies and system libraries for better compatibility across devices.\n\nWhat is container?\n\nits like small virtual machines\ncontainers are free to use all the system resources they needs\nwhereas in vms softwares has limited preallocated resources to use\n\nDocker Commands\n#Creating a nginx container\ndocker run nginx\n \n# run as detached mode\ndocker run -d nginx\n \n# list running containers\ndocker ps \n \n# list all containers\ndocker ps -a\n \n# kill running container\ndocker kill &lt;container-id&gt;\n \n# expose container on random host port\ndocker run -P nginx\n \n# expose container on custom host port\ndocker run -d -p 1313:8080 nginx:latest\n \n# Stopping the container\ndocker stop &lt;container id&gt;\n \n# removing stopped containers\ndocker rm &lt;container id&gt;\n \nInstalling docker\nRhel Based Distro Installation\nInstalling docker on amazonlinux and rhel based distros are much easier than installling on ubuntu\nsudo yum install docker # rhel/amazonlinux/fedora etc\nsudo pacman -S docker   # archlinux based distros\nUbuntu/Debian Family based distro Installation\ncurl -fsSL get.docker.com -o get-docker.sh &amp;&amp; sudo sh get-docker.sh`\nRunning this script should install following package inside your ubuntu based distro docker-engine, docker.io, containerd, runc.\n\n\n                  \n                  Tip\n                  \n                \n\nHowever I still recommend to check this official installation guide\n\n\nEnabling docker daemon\nsystemctl start docker\nsystemctl enable docker\nTo run docker command its absolutely require to have docker daemon running in background otherwise it wont work so make sure you have docker daemon running.\nPulling the Image\ndocker pull nginx\nThis will pull latest version of nginx image from docker hub.\nBy default its always going to pull latest tag\nIn order to pull specific version you need to mention its tag name like this\ndocker pull nginx:1.23.1\n\nRunning the container from the image\ndocker run nginx\nThis command creates and starts a Docker container running the Nginx web serve. Although By default, Nginx runs on port 80 within the container. This command doesn’t map any ports, so you won’t be able to access the Nginx server from your host machine to specify port you can use-p flag.\nMapping specific port\ndocker run -p 8080:80 nginx\nHere 8080 is host machine port and 80 is container port\ndocker run -P nginx\nThis will map random port for the host machine.\nRunning the container in detached mode\ndocker run -d nginx`\n\nNormally running container will cause container to run in foreground and occupy the terminal screen. So its better choice to use -d to make it run in background.\nGiving the specific name to container\ndocker run --name my-container nginx\nBy default docker give funny names to containers. but you can specify your custom name and can use that name instead of ‘container-id’ in docker commands.\nStopping container\ndocker stop &lt;conainer-id&gt;\nStopped containers and be restarted with start command\nRemoving stopped container\ndocker rm &lt;container-id&gt;\nSometimes stopped or exited containers wont have any use and needs to be removed to save space.\nListing the running container\ndocker ps\nThis will list all the running containers only and not exited or stopped ones.\nTo list all containers including exited and stopped ones use ps -a\ndocker ps -a\nSeeing the logs of the container\ndocker logs &lt;container-id&gt;\nThis is important to monitor activities inside of containers.\nInspecting the running container\ndocker inspect &lt;container-id&gt;\nThis will list all the information of container\nCreating the image out of running container\ndocker commit &lt;container-id&gt; \nThis will come in handy when you done changes inside running container and want to incorporate these changes into your image.\n$ sudo docker images\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\n&lt;none&gt;       &lt;none&gt;    5c9d3a455e0f   10 seconds ago   214MB\n\nGiving tag to image\ndocker tag &lt;image-id&gt; new-name\nAdding tag is like giving Image a name this comes in handy for identifying images.\n$ sudo docker images\nREPOSITORY     TAG       IMAGE ID       CREATED          SIZE\ncustom-nginx   latest    5c9d3a455e0f   47 seconds ago   214MB\nPushing Image to docker hub and ECR\ndocker push &lt;image-id&gt;\nIf you have verified docker hub login through docker login command it will automatically push the image to your docker hub repo assuming your docker hub repo has same name as your image tag\nand same goes to ECR except you need to authenticate through slightly different method:\naws ecr get-login-password --region &lt;region-name&gt; | docker login --username AWS --password-stdin &lt;amazon-account-id&gt;.dkr.ecr.&lt;region-name&gt;.amazonaws.com\nReferences:\n\nwww.geeksforgeeks.org/monolithic-vs-microservices-architecture/\nadventofdocker.com\n"},"DevOps/Notes/Git-Notes-in-My-Own-words":{"title":"Git Notes","links":[],"tags":["blog","git"],"content":"What is Git?\nGit is a Distributed Version Control System that mainly used for Source code management by developers.\nGit basically divides your code into three part.\n\nWorking Area\nStaging Area\nHead or commit\n\nWorking Area is your state of code before running the git add command.\nThe files in here also known as “untracked files”.\nTo add the files into git you do git add and they moved to staging area.\nHere your code got the stage.\nBefore finalizing your code into production i.e. ‘commit’ you make required changes and after making sure\nyour code is ready to go into production you run git commit command.\nwhich is basically telling git this is a final changes with the special stamp of “commit hash”.\nThen If you want you can push your code into remote repository sitting onto the cloud waiting to be forked and contributed by peoples across the glob.\nBasic Git commands\n\ngit add     To add code into git\ngit commit  To commit changes\ngit push    To upload your code into remote repo\ngit pull    To download latest changes from remote to your local\ngit fetch   Fetching Metadata and logs from remote\n\nBranching Strategies\nGit Provide “Branches” which are nothing but a way to seperate your code into different version so you can later integrate your features and bugfixes into production once they finished peacefully in their own isolated places.\nThere are well known three branches people generally creates:\n\nMain  (which is your production branch)\nDev   (which is your ongoing development which never cease to stop.)\nTest  (Here your code is being tested against performance and security standards.)\n\nSome people also like to create their own custom branches like alpha, beta, stable etc i.e. according to release state.\nCommands Related to Branches\n\ngit branch -a   List all available branches\ngit checkout -b Create and switch to the new branch\ngit checkout    To switch to different branch\n"},"DevOps/Notes/Git-Theory-and-Practical":{"title":"Git Theory and Practical","links":[],"tags":["git"],"content":"Understand Git Concepts\nCVCS Centralized Versioning System\nA centralized version control system (VCS) uses a single, central repository to store all file versions and their change history. Team members have their own working copies, but all modifications are ultimately committed to this central server. This facilitates collaboration by providing a single source of truth, but it also creates a single point of failure. Examples include Subversion (SVN) and CVS.\n\nDVCS - Decentralized Version Control System\nIn a Decentralized Version Control System (DVCS), every user has a complete copy of the repository, including its entire history. This eliminates the reliance on a central server, allowing for offline work and greater flexibility. Changes are shared between repositories as needed. Popular examples include Git and Mercurial.\n\nWhat is Git?\nGit is like a super-powered tracking system for your files and the changes you make to them over time. Imagine it as a special folder that remembers every version of your work, allowing you to go back to any previous stage if needed.  \nHere’s what makes Git special:\n\nKeeps a detailed history: Git meticulously records every change you make to your files, who made it, and when. This history helps you understand how your project evolved.  \nBranching and merging: Git allows you to create separate branches, like alternate timelines, to experiment with new features or try different ideas without affecting the main project. You can then merge these branches back into the main project when you’re ready.  \nCollaboration made easy: Git is designed for teamwork. Multiple people can work on the same project simultaneously, and Git helps manage and integrate everyone’s contributions smoothly.  \nOffline access: You have the entire project history on your computer, so you can work even without an internet connection.  \nPopular and widely used: Git is the most popular version control system in the world, used by countless developers and companies.  \n\nHistory of Git\nGit was created by Linus Torvalds, the famous creator of the Linux operating system, in 2005. He needed a better tool to manage the Linux kernel development, as the existing version control systems were not efficient enough for such a large and complex project.\nHere’s a glimpse of Git’s history:\n\nEarly Days (2002-2005): Linux kernel development relied on a proprietary DVCS called BitKeeper. When its free-of-charge use was revoked, Torvalds decided to create his own version control system, with the goal of being faster, simpler, and more robust.\nBirth of Git (April 2005): Torvalds began work on Git and within a remarkably short period, had a functional system ready to manage the Linux kernel.\nCommunity Takes Over (July 2005): Junio Hamano took over the maintenance of Git, guiding its development and shaping it into the mature system it is today.\nWidespread Adoption: Git’s speed, flexibility, and powerful features quickly gained popularity among developers. It became the preferred choice for open-source projects and eventually spread to commercial software development.\n\nToday, Git is the most widely used version control system worldwide, powering the development of countless software projects, from small personal projects to massive corporate endeavors.\nInstallation of Git\n\nSignup to github.com\nInstall git with gitbash in windows\nor via package manager in linux\n\nfor ubuntu apt install git\nfor rhel based distros yum install git\nfor arch based distros pacman -S git\n\n\n\nInitializing empty repository as git\n\nCreate empty folder in local machine\nnavigate to it and run git init\nadd some files and run git add .\ncommit changes `git commit -m “git initialized and new files added”\nCreate repository in github and use its link to below step\nAdd remote repo git remote add origin git@github.com:&lt;username&gt;/&lt;repo-name&gt;.git\nupdate the remote repo with push git push origin main\n\n\nBasic Git Commands\nAdding Files to Staging Area\ngit add . \nThis command will staged all files and modifications done in current and subdirectories but wont wont stage deletions.\ngit add -A \nThis command will stage all types of changes across your entire working directory including newfiles, modifications and deletions\nThis command will commit changes into git\ngit commit -m &quot;new commit&quot;\nCreates a commit with the specified message. This is the most common way to commit changes.\ngit commit -A &quot;your commit message&quot;\nAutomatically stages all tracked files that have been modified and deleted, then creates a commit. It combines git add -A and git commit -m into a single command.\nDoes not stage any new files. If you have new files you want to include in the commit, you still need to stage them separately using git add.\nSyncing latest commit to remote repo\ngit push origin main\nUploads your local changes to the remote repository.\nAfter pushing, other developers can access your changes by pulling from the remote repository.\nPulling latest changes from remote to local\ngit pull origin main\nDownloads changes from the remote repository to your local repository. Used to stay up-to-date with the latest changes made by other develpers.*"},"DevOps/Notes/Kubernetes-Architecture":{"title":"Kubernetes Architecture","links":[],"tags":[],"content":"Kubernetes Cluster Architecture\n\n\nControl Plane:\n\nAPI Server: Central management point\netcd: Cluster state database\nController Manager: Maintains desired state\nScheduler: Places pods on nodes\n\n\nWorker Nodes:\n\nKubelet: Node agent\nContainer Runtime: Runs containers\nKube Proxy: Networking rules\nPods: Groups of containers\n\n\nExternal Access:\n\nkubectl CLI: Command-line interface\nLoad Balancer: External traffic distribution\nIngress: HTTP/HTTPS routing\n\n\nStorage:\n\nPersistent Volumes: Durable storage\n\n\nNetworking:\n\nCNI: Pod networking\nNetwork Policies: Traffic rules\n\n\n\nThe arrows show the communication flow between components. For example:\n\nAll components communicate through the API Server\nKubelets on worker nodes report to API Server\nPods can access Persistent Volumes\nNetwork policies control pod communication\n"},"DevOps/Notes/My-limited-diff-usecase-In-case-I-forgot":{"title":"My limited diff usecase In case I forgot","links":[],"tags":["blog","diff"],"content":"What is diff?\nShows difference/comparison between files\nWhy is it useful?\n\nTracking changes: See how a file has evolved over time.\nComparing versions: Find out what’s different between two versions of a document or code.\nMerging changes: Helps you combine different versions of a file.\nTroubleshooting: Identify what went wrong in a configuration file or program.\n\nHow to use it?\nThe basic command is super simple:\ndiff file1 file2\nJust replace file1 and file2 with the actual names of your files.\nMaking the output more helpful\ndiff has some options to make the output easier to understand:\n\n-u (or --unified): Shows the changes in a “unified” format, which is more compact and easier to read.\n-y (or --side-by-side): Displays the files side-by-side, so you can see the differences directly.\n-w (or --ignore-all-space): Ignores differences in whitespace (spaces and tabs), which is helpful if you only care about the content.\n-r (or --recursive): Compares entire directories, not just individual files.\n-q (or --brief): Only tells you if the files are different, not what the differences are.\n\nExample:\nLet’s say you have two files, original.txt and revised.txt. To see the differences in a unified format:\ndiff -u original.txt revised.txt\nUnderstanding the output\ndiff uses some symbols to show you the changes:\n\n+: Lines added in the second file.\n-: Lines removed from the first file.\n@@: Indicates a block of changed lines.\n\n\nSee difference between two folders\nBasic Usage\ndiff -qr folder1 folder2\n\n-q: Only reports when files differ.\n-r: Recursively compares subdirectories.\n\nMore detailed output\ndiff -ruN dir1 dir2\n\n-r: Recursively compares subdirectories.\n-u: Produces a unified diff format, which is easier to read.\n-N: Treats absent files as empty.\n\nFor files\ndiff -wy file1.txt file2.txt\n\n-w : for ignoring whitespaces\n-y : side by side comparison\n\nSome Very Important Flags\n\n--color=always\n"},"DevOps/Notes/Understanding-Manifest-File":{"title":"Understanding Manifest File","links":[],"tags":[],"content":"Each configuration file has 3 part\n\nMetadata\nSpecification\nStatus (often autogenerated by kubernetes)\n\nAttributes of “spec” are specific to the “kind”\nMetadata part contains Labels and\nSpecification part contains Selectors"},"DevOps/Tasks-Done/Task-1-Creating-Tomcat-student-ui-container":{"title":"Task 1 Creating Tomcat student-ui container","links":[],"tags":["docker","devops","devops_tasks","containerisation","student-ui"],"content":"Task 1 - Creating Tomcat student-ui container\nInstructions\n\n Get the docker image of Amazon linux\n Add tomcat package\n Add student ui\n Then commit the image\n Store on ecr as well as on docker hub\n\nSteps Performed\nStep 1: Getting the the Amazon Linux Image\nUpon searching the for amazonlinux image in dockerhub I found its official image\nI pulled it into the system with docker pull amazonlinux\nsudo docker run -it -d -p 32768:8080 --name tomcat-student-ui amazonlinux`\nand I run the image interactively and in detached mode so i can execute shell command later on\n\nLets dive into the container\nsudo docker exec -it &lt;containerid&gt; &lt;shell-command&gt;`\n\nStep 2: Installing tomcat application\nOur App student-ui required specific version of tomcat thats why we gonna install it from source\nMake sure to install dependancies and unzip package before hand\nStep 3: Install tomcat from source\nInstall this specific version from source\ncd /opt # we can use this directory for temporary space\ncurl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip\nunzip apache-tomcat-9.0.98.zip # unzip the archive\nyum install java-17 -y # tomcat 9 requires java-17 to work properly\ncd apache-tomcat-9.0.98/bin/\nbash ./catalina.sh start \n\n\nIt looks like our tomcat server is up and running on desired port\nStep 4: Installing Student-ui\nbut first we need these package building tools git and maven for building our student-ui app\nInstalling git and maven\nsudo yum install git maven -y\n\nClone the student-ui repo\ngit clone github.com/Pritam-Khergade/student-ui\nbuild student-ui app using maven\ncd student-ui\nmvn clean package\n\nthis creates .war file in ./target folder\nrename it to suitable short name and move to /opt/apache-tomcat-9.0.98/webapps directory\nmv target/studentapp-2.2-SNAPSHOT.war target/studentapp.war\nmv target/studentapp.war /opt/apache-tomcat-9.0.97/webapps/\nand the app should be accessible on tomcat server on http://instance-ip:32768/studentapp\n\nCreate the image out of this running container\nBefore creating the image its better to remove the unnecessary packages that we no longer need to make the size of the image minimal as possible.\nCleanup the no-longer needed packages to reduce size of image\nyum remove maven git unzip -y\nLets exit from container shell and build the image\nsudo docker commit &lt;container-id&gt;\n\nYou see the created image doesnt have any name so lets give it a tag\nsudo docker tag &lt;image-id&gt; &lt;newtag&gt;\n\nnow push it to docker hub and ECR\nUploading image to docker hub\nFirst create a repository at docker hub\n\nLogin to docker hub\nClick on repositories\nCreate new repository\nGive it proper name and click create\n\nHere is my repo looks llike\n\nLets push our image into this repo\nFirst rename add new tag to image appropriate according to docker hub repo name\nsudo docker tag &lt;old-tag-name&gt; &lt;newtag-name&gt;\n\nand now push it to docker hub\nsudo docker push archsarangx/tomcat-student-ui:latest\n\nand its successfully uploaded on docker hub at\n\nand anyone can pull it with\ndocker pull archsarangx/tomcat-student-ui:latest\nUploading image to ECR\nStep 1: Creating the repository at ECR\n\ngoto amazon ECR service and create repositoy\n\n\nthen click on blue repo name and click on view push commands\nStep 2: Authenticate with ECR\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 970547378605.dkr.ecr.us-west-2.amazonaws.com\nStep 2: Add Tag name to image\n“\ndocker tag archsarangx/tomcat-student-ui:latest 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest\ndocker push 970547378605.dkr.ecr.us-west-2.amazonaws.com/archsarangx/tomcat-student-ui:latest\n\n\n🎉 And our Image is successfully uploaded on both ECR and docker hub.\nThank you for reading\nHave a good day!"},"DevOps/Tasks-Done/Task-2-Create-the-Free-css-template-container":{"title":"Task 2 Create the Free-css template container","links":[],"tags":["devops_tasks","practical","devops","docker","containerisation"],"content":"Task 2: Create the Free-css template container\nInstructions\n\n get the docker image of Amazon linux\n install nginx\n add free-css template\n\nSteps Performed:\nGetting the Amazon Linux docker image\n\n\nGoto docker hub ⇒ hub.docker.com\n\n\nClick on Search and search for -⇒ amazonlinux\n\n\nHere I found this official image\n\n\n\nI simply pulled the image into my ec2 instance\ndocker pull amazonlinux\nhere is successful pull looks like\n\n\n\nWe can further confirm if the image is really present in the system with docker images\n\nLooks like amazonlinux is successfully pulled in our system\nlets start the container using this image\n\n\nRunning the container from image\nsudo docker run -d -p 32768:80 --name free-css-template amazonlinux\nWe used -d to run the container in detached mode meaning its output wont occupy the terminal screen and quietly run in the background after running this command it will simply give us container-id and gives us prompt.\n--name flag will assign the name for our container instead of their random funny names and with -p we are forwarding the network traffic of port 80 of the container to 32768 port of our host machine in this case ec2-instance.\n\nWe can check for the further detail of the container with\ndocker ps\nUpon inspection we can see there is no container running\nlets check again with docker ps -a and for our surprise container is exited just after it run. Thats because its an Operating system container which doesnt really do anything itself unless we assign it a task or a process that will run in background constantly so we can enter into the container to perform our tasks.\nWe can achieve this by running the container interactively\nbut first clean up the exited container with docker rm &lt;container-id&gt;\n\n\nGetting the shell into container\n\nRun a container interactively.\n\ndocker run -it -d -p 32768:80 --name free-css-template amazonlinux\nI run the container with -it and -d so it opened the interactive stdin shell session for me to interact with it and the container itself is in detached mode allowing us to enter into it anytime with exec command\ndocker exec -it &lt;container-id&gt; &lt;shell-command&gt;\n\nwe got bash shell session inside container\nNow we need to update the packages inside the container\nInstalling packages inside container\nyum update\n\n\nInstall nginx package start it\n\nyum install nginx -y\nsince this amazonlinux container image doesn’t come with systemd preinstalled we cannot start nginx daemon so we need to run this command manually in background\nnginx &amp;\nand our nginx server is successfully started at port 32768 on our host ec2-instance\n\nInstalling Free-CSS template\nLets add free css template in it\nHead on to www.free-css.com/ and download any free css template you want by right clicking on the download button and copying link address\n\nand go to your containers terminal session and download this file in nginx server directory with curl or wget\ncd /usr/share/nginx/html\ncurl -O www.free-css.com/assets/files/free-css-templates/download/page296/carvilla.zip\n \n# Extract the zip file\nunzip carvilla.zip\n \n# If unzip is not found install unzip package\nsudo yum install unzip\nextracting the zip has extracted all contents into folder named carvilla-v1.0\nI’ll rename it for my convenient.\nmv carvilla-v1.0 mywebsite\n\nand the website is running at port 32768/mywebsite\nnow we can get out of this container with exit command.\n\nWe have our container successfully hosting our free-css template on port http://instance-ip:32768/mywebsite\nLets create the image out of this container so we can spin many containers as we want and save the hustle to do all these steps all over again.\nCreating and Storing the Image on Dockerhub\n\nCreating the image\n\nNow that we have our container running and our site is visible I want to make image out of it and push it to docker hub\n\nfor that we use docker commit &lt;container-id&gt;\nit returns with the sha-id\n\nwith docker images we can confirm the image creation\n\nour image shown in first row doesnt have name so lets give it a tag and then push it to repo\ndocker tag &lt;image-id&gt; &lt;tagname&gt;\ndocker tag local-image:tagname new-repo:tagname\n\nHere I have already created repository in docker hub so I gave its name here\nand run docker hub login command before running push command\ndocker login\nand push it to our docker hub\ndocker push new-repo:tagname\n\nAnd our project is successful pushed over docker hub and can be access from here\nhub.docker.com/r/archsarangx/amazonlinux-free-css-demo/tags\nStoring the Image on EKS\n\nTo store the image into Amazon’s Elastic Container Registry which is similar to docker hub but their own we need to create a repository here too.\nI gave the repo same name\n\nand Upon clicking create button my repo is created\n\nClick on the Repo name in blue and click on view push commands to get instruction for pushing our image into this repo\n\nAuthenticating the docker client with EKS Registry\nTo push our image we need to first Authenticate with registry for that carefully use first command also we need to have aws-cli installed and configured for that luckily for me I am using amazonlinux ec2 instance so its already installed i can just run aws configure to configure aws-cli\nFor those havent can check this documentation to configure their aws-cli within instance.\n\nand then run this command provided in the instructions but remember to add sudo as we are using standard user\n\nNow that We have successfully login to ECR lets tag the image accordingly and push it to ECR\nsudo docker tag &lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt; &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;newname&gt;:&lt;newtag&gt;\n\nand push to ECR\nsudo docker push &lt;amazon-account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;username&gt;/&lt;repo-name&gt;:&lt;tag-name&gt;\n\nand refresh the amazon EKS web site to see the latest image listed in there\n\nThank you for Reading\nHave a good day!"},"DevOps/Tasks-Done/Task-3-Create-Mysql-Container":{"title":"Task 3 Create Mysql Container","links":["tags/"],"tags":["devops_tasks","docker","containerisation",""],"content":"Create the MYSQL Container\nInstructions\n\n Host mysql container\n Use -e  environment variables\n use MYSQL_USER, MYSQL_PASSWORD\n\nSteps Performed\nStep 1: Login to Ec2-instance\nStep 2: Pull docker image of mysql\nsudo docker pull mysql\nStep 3: Run the container with environment variables\nsudo docker run -e MYSQL_USER=sarang -e MYSQL_PASSWORD=mysupersecretpassword -e MYSQL_RANDOM_ROOT_PASSWORD=yes -p 3306:3306 --name mysql_container -v mysql_data:/var/lib/mysql mysql:latest\nHere we used -e flag to specify each environment variables for mysql\nMYSQL_USER=sarang  This sets the username for our mysql database\nMYSQL_PASSWORD=mysupersecretpassword This sets the password for our database\nMYSQL_RANDOM_ROOT_PASSWORD=yes    # This tells the mysql to generate a random root password (it can be seen in docker logs)\n-v mysql_data:/var/lib/mysql mysql:latest This flag sets the persistence volume for our container and it mounts /var/lib/mysql directory of the container to the mysql_data directory of our host."},"DevOps/Tasks-Done/Task-4-Create-daemon-service-of-tomcat":{"title":"Task 4 Create daemon service of tomcat","links":[],"tags":["systemd","devops_tasks","linux"],"content":"The simplest possible script I could make for tomcat is this one\n# make catalina.sh executable\nchmod +x ~/apache-tomcat-9.0.98/bin/catalina.sh \n \n# navigate to systemd services directory\ncd /etc/systemd/system\n \n# Create new tomcat service file\nvi tomcat.service\nAdd this script\n[Unit]\nDescription=Tomcat Service Daemon\n \n[Service]\nType=forking\nExecStart=home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start\nExecReload=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh start\nExecStop=/home/ec2-user/apache-tomcat-9.0.98/bin/catalina.sh stop\n \n[Install]\nWantedBy=multi-user.target\nRun these commands afterwards\n# Reloading the systemd daemon\nsystemctl daemon-reload\n \n# Enabling and starting tomcat.service\nsystemctl enable --now tomcat.service\n \n# Checking Status of our service\nsystemctl status tomcat.service\nHowever this isnt sufficient for tomcats full usecase\nfor that you need to add Environment Variables\n# Set environment variables (optional but recommended) \n \n# Update with your Java installation path\nEnvironment=JAVA_HOME=/usr/lib/jvm/jre \n \n# Update with your Tomcat PID file path \nEnvironment=CATALINA_PID=/home/ec2-user/apache-tomcat-9.0.98/temp/tomcat.pid \n \n# Update with your Tomcat installation path \nEnvironment=CATALINA_HOME=/home/ec2-user/apache-tomcat-9.0.98 \n \n# Update with your Tomcat base directory \nEnvironment=CATALINA_BASE=/home/ec2-user/apache-tomcat-9.0.98 \n \n# Customize JVM options \nEnvironment=&#039;CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC&#039;\nEnvironment=&#039;JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom&#039;\n \nalso for security purposes its recommended to use tomcat service as a dedicated tomcat user\nafter making changes in the file its important to reload systemd daemon with systemctl daemon-reload and restarting the tomat service and check the status.\nIf everything is working and your tomcat page is displaying on port 8080 it means your tomcat service is successfully working."},"DevOps/Tasks-Done/Task-5-Create-dockerfiles":{"title":"Task 5 Create dockerfiles","links":["Daily-Notes/2024-12-31-Dockerfile-Explained"],"tags":[],"content":"1. Create a dockerfile for nginx container\n\nCreate a Seperate folder and move to it\nmkdir my_nginx_image &amp;&amp; cd my_nginx_image\nCreate a docker file with vi Dockerfile\n\nFROM nginx:latest\nCOPY oxer-html /usr/share/nginx/html/.\n\nBuild the Image docker build -t . my_nginx_image\nRun the container docker run -dP my_nginx_image\n\n2. Write dockerfile for amazonlinux running ngnix\nFROM amazonlinux:latest\nRUN yun update &amp;&amp; yum install nginx\nEXPOSE 80\nCMD [&quot;/usr/sbin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n3. Write dockerfile for tomcat application\nFROM amazonlinux:latest\nWORKDIR /opt\nRUN yum update &amp;&amp; yum install java-17 unzip -y\nRUN curl -O dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.zip\nRUN unzip apache-tomcat-9.0.98.zip\nRUN chmod +x /opt/apache-tomcat-9.0.98/bin/catalina.sh \nEXPOSE 8080\nCMD [&quot;/opt/apache-tomcat-9.0.98/bin/catalina.sh&quot;, &quot;run&quot;]\nBacklinks\n\n2024-12-31 Dockerfile Explained\n"},"DevOps/Tasks-Done/Task-6-How-to-set-permanent-alias":{"title":"Task 6 How to set permanent alias","links":[],"tags":["ShellScripting","linux","bash"],"content":"aliases are shortnames or nick names for big forgottable commands user sets by themselves the use for their own convenience and comfort over big and tedius commands.\nExample. If you set alias for ls -la --color=auto command to be ll it will automagically trigger that command everytime you use ll command.\nThere are two ways to set Alias in linux terminal:\nMethod 1 (Temporary Alias):\nalias k=&quot;kubectl&quot;\nHere you only set alias for your current shell meaning as long you have that shell session on you can use it if you kill it or spawn in new shell or restart terminal it wont be available again. To use permanent Alias use Method 2.\nMethod 2 (Permanent Alias)\nAdding this line in ~/.bashrc or /.bash-profile file\nYou can add this line into your shell configuration whether its bash or zsh. it will work.\nalias k=&quot;kubectl&quot;\nLimitation: For simple purposes this is fine but it wont work in your shell scripts cuz its limited to your shell and not its child processes.\nUsing export will add aditional functionality to your alias and expands its availability to the child processess of shell also which makes them usable in your custom shell scripts too.\nexport alias k=&quot;kubectl&quot;\nYou can add your alias in this way either in ~/.bashrc or ~/.profile or ~/.bash-profile file\nImmediate result may not be seen you need to restart shell or source the file again\nsource ~/.bashrc\nor \n. ~/.bashrc"},"Development/Mysql-APP-From-Scratch":{"title":"Mysql APP From Scratch","links":[],"tags":[],"content":"1. EC2 Instance Setup and MySQL Installation\n# Update system packages\nsudo yum update -y\n \n# Install Apache web server\nsudo yum install -y httpd\n \n# Install PHP and MySQL\nsudo amazon-linux-extras enable php7.4\nsudo yum clean metadata\nsudo yum install -y php php-devel php-mysqlnd httpd mysql-server\n \n# Start and enable services\nsudo systemctl start httpd\nsudo systemctl enable httpd\nsudo systemctl start mysqld\nsudo systemctl enable mysqld\n \n# Secure MySQL installation\nsudo mysql_secure_installation\n2. Database and Table Creation\n-- Create database\nCREATE DATABASE user_registration;\nUSE user_registration;\n \n-- Create users table\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    student_name VARCHAR(100),\n    address VARCHAR(255),\n    email VARCHAR(100) UNIQUE,\n    username VARCHAR(50) UNIQUE,\n    password VARCHAR(255)\n);\n3. User signin page\nsignin.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n    &lt;meta charset=&quot;UTF-8&quot;&gt;\n    &lt;title&gt;Sign In&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            display: flex;\n            justify-content: center;\n            align-items: center;\n            height: 100vh;\n            margin: 0;\n            background-color: #f4f4f4;\n        }\n        .container {\n            background-color: white;\n            padding: 30px;\n            border-radius: 5px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n            width: 300px;\n        }\n        input {\n            width: 100%;\n            padding: 10px;\n            margin: 10px 0;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n        }\n        button {\n            width: 100%;\n            padding: 10px;\n            background-color: #007bff;\n            color: white;\n            border: none;\n            border-radius: 4px;\n            cursor: pointer;\n        }\n        .signup-link {\n            text-align: center;\n            margin-top: 15px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=&quot;container&quot;&gt;\n        &lt;h2&gt;Sign In&lt;/h2&gt;\n        &lt;form action=&quot;login.php&quot; method=&quot;post&quot;&gt;\n            &lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;Username&quot; required&gt;\n            &lt;input type=&quot;password&quot; name=&quot;password&quot; placeholder=&quot;Password&quot; required&gt;\n            &lt;button type=&quot;submit&quot;&gt;Sign In&lt;/button&gt;\n        &lt;/form&gt;\n        &lt;div class=&quot;signup-link&quot;&gt;\n            &lt;p&gt;Don&#039;t have an account? &lt;a href=&quot;signup.html&quot;&gt;Sign Up&lt;/a&gt;&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n4. User Registration page\nsignup.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n    &lt;meta charset=&quot;UTF-8&quot;&gt;\n    &lt;title&gt;Sign Up&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            display: flex;\n            justify-content: center;\n            align-items: center;\n            height: 100vh;\n            margin: 0;\n            background-color: #f4f4f4;\n        }\n        .container {\n            background-color: white;\n            padding: 30px;\n            border-radius: 5px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n            width: 300px;\n        }\n        input {\n            width: 100%;\n            padding: 10px;\n            margin: 10px 0;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n        }\n        button {\n            width: 100%;\n            padding: 10px;\n            background-color: #28a745;\n            color: white;\n            border: none;\n            border-radius: 4px;\n            cursor: pointer;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=&quot;container&quot;&gt;\n        &lt;h2&gt;Sign Up&lt;/h2&gt;\n        &lt;form action=&quot;register.php&quot; method=&quot;post&quot;&gt;\n            &lt;input type=&quot;text&quot; name=&quot;student_name&quot; placeholder=&quot;Student Name&quot; required&gt;\n            &lt;input type=&quot;text&quot; name=&quot;address&quot; placeholder=&quot;Address&quot; required&gt;\n            &lt;input type=&quot;email&quot; name=&quot;email&quot; placeholder=&quot;Email&quot; required&gt;\n            &lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;Username&quot; required&gt;\n            &lt;input type=&quot;password&quot; name=&quot;password&quot; placeholder=&quot;Password&quot; required&gt;\n            &lt;button type=&quot;submit&quot;&gt;Register&lt;/button&gt;\n        &lt;/form&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n5. Register Script\nregister.php\n&lt;?php\nsession_start();\n$host = &#039;localhost&#039;;\n$db_username = &#039;root&#039;;  // Change this to your MySQL username\n$db_password = &#039;your_mysql_password&#039;;  // Change to your MySQL password\n$database = &#039;user_registration&#039;;\n \n// Create connection\n$conn = new mysqli($host, $db_username, $db_password, $database);\n \n// Check connection\nif ($conn-&gt;connect_error) {\n    die(&quot;Connection failed: &quot; . $conn-&gt;connect_error);\n}\n \nif ($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) {\n    $student_name = $conn-&gt;real_escape_string($_POST[&#039;student_name&#039;]);\n    $address = $conn-&gt;real_escape_string($_POST[&#039;address&#039;]);\n    $email = $conn-&gt;real_escape_string($_POST[&#039;email&#039;]);\n    $username = $conn-&gt;real_escape_string($_POST[&#039;username&#039;]);\n    $password = password_hash($_POST[&#039;password&#039;], PASSWORD_BCRYPT);\n \n    // Check if username or email already exists\n    $check_query = &quot;SELECT * FROM users WHERE username = ? OR email = ?&quot;;\n    $stmt = $conn-&gt;prepare($check_query);\n    $stmt-&gt;bind_param(&quot;ss&quot;, $username, $email);\n    $stmt-&gt;execute();\n    $result = $stmt-&gt;get_result();\n \n    if ($result-&gt;num_rows &gt; 0) {\n        echo &quot;Username or email already exists!&quot;;\n    } else {\n        // Insert new user\n        $sql = &quot;INSERT INTO users (student_name, address, email, username, password) VALUES (?, ?, ?, ?, ?)&quot;;\n        $stmt = $conn-&gt;prepare($sql);\n        $stmt-&gt;bind_param(&quot;sssss&quot;, $student_name, $address, $email, $username, $password);\n \n        if ($stmt-&gt;execute()) {\n            $_SESSION[&#039;username&#039;] = $username;\n            header(&quot;Location: profile.php&quot;);\n        } else {\n            echo &quot;Error: &quot; . $stmt-&gt;error;\n        }\n    }\n    $stmt-&gt;close();\n}\n$conn-&gt;close();\n?&gt;\n6. Login Script\nLogin.php\n&lt;?php\nsession_start();\n$host = &#039;localhost&#039;;\n$db_username = &#039;root&#039;;\n$db_password = &#039;your_mysql_password&#039;;\n$database = &#039;user_registration&#039;;\n \n$conn = new mysqli($host, $db_username, $db_password, $database);\n \nif ($conn-&gt;connect_error) {\n    die(&quot;Connection failed: &quot; . $conn-&gt;connect_error);\n}\n \nif ($_SERVER[&quot;REQUEST_METHOD&quot;] == &quot;POST&quot;) {\n    $username = $conn-&gt;real_escape_string($_POST[&#039;username&#039;]);\n    $password = $_POST[&#039;password&#039;];\n \n    $sql = &quot;SELECT * FROM users WHERE username = ?&quot;;\n    $stmt = $conn-&gt;prepare($sql);\n    $stmt-&gt;bind_param(&quot;s&quot;, $username);\n    $stmt-&gt;execute();\n    $result = $stmt-&gt;get_result();\n \n    if ($result-&gt;num_rows == 1) {\n        $user = $result-&gt;fetch_assoc();\n        if (password_verify($password, $user[&#039;password&#039;])) {\n            $_SESSION[&#039;username&#039;] = $username;\n            header(&quot;Location: profile.php&quot;);\n        } else {\n            echo &quot;Incorrect password!&quot;;\n        }\n    } else {\n        echo &quot;Account not registered!&quot;;\n    }\n    $stmt-&gt;close();\n}\n$conn-&gt;close();\n?&gt;\n7. Profile Page\nProfile.php\n&lt;?php\nsession_start();\nif (!isset($_SESSION[&#039;username&#039;])) {\n    header(&quot;Location: signin.html&quot;);\n    exit();\n}\n \n$host = &#039;localhost&#039;;\n$db_username = &#039;root&#039;;\n$db_password = &#039;your_mysql_password&#039;;\n$database = &#039;user_registration&#039;;\n \n$conn = new mysqli($host, $db_username, $db_password, $database);\n \n$username = $_SESSION[&#039;username&#039;];\n$sql = &quot;SELECT * FROM users WHERE username = ?&quot;;\n$stmt = $conn-&gt;prepare($sql);\n$stmt-&gt;bind_param(&quot;s&quot;, $username);\n$stmt-&gt;execute();\n$result = $stmt-&gt;get_result();\n$user = $result-&gt;fetch_assoc();\n?&gt;\n \n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Profile&lt;/title&gt;\n    &lt;style&gt;\n        body { font-family: Arial, sans-serif; max-width: 400px; margin: 0 auto; padding: 20px; }\n        .profile-section { background: #f4f4f4; padding: 20px; border-radius: 5px; }\n        input { width: 100%; margin: 10px 0; padding: 5px; }\n        button { margin-top: 10px; padding: 10px; }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=&quot;profile-section&quot;&gt;\n        &lt;h2&gt;Profile&lt;/h2&gt;\n        &lt;form action=&quot;update_profile.php&quot; method=&quot;post&quot;&gt;\n            &lt;label&gt;Name: &lt;input type=&quot;text&quot; name=&quot;student_name&quot; value=&quot;&lt;?php echo $user[&#039;student_name&#039;]; ?&gt;&quot;&gt;&lt;/label&gt;\n            &lt;label&gt;Email: &lt;input type=&quot;email&quot; name=&quot;email&quot; value=&quot;&lt;?php echo $user[&#039;email&#039;]; ?&gt;&quot;&gt;&lt;/label&gt;\n            &lt;label&gt;New Password: &lt;input type=&quot;password&quot; name=&quot;new_password&quot;&gt;&lt;/label&gt;\n            &lt;button type=&quot;submit&quot;&gt;Update Profile&lt;/button&gt;\n        &lt;/form&gt;\n        &lt;a href=&quot;logout.php&quot;&gt;Logout&lt;/a&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n8. Update profile script\nupdate_profile.php\n&lt;?php\nsession_start();\nif (!isset($_SESSION[&#039;username&#039;])) {\n    header(&quot;Location: signin.html&quot;);\n    exit();\n}\n \n$host = &#039;localhost&#039;;\n$db_username = &#039;root&#039;;\n$db_password = &#039;your_mysql_password&#039;;\n$database = &#039;user_registration&#039;;\n \n$conn = new mysqli($host, $db_username, $db_password, $database);\n \n$username = $_SESSION[&#039;username&#039;];\n$student_name = $conn-&gt;real_escape_string($_POST[&#039;student_name&#039;]);\n$email = $conn-&gt;real_escape_string($_POST[&#039;email&#039;]);\n \n$update_sql = &quot;UPDATE users SET student_name = ?, email = ?&quot;;\n$params = [$student_name, $email];\n$types = &quot;ss&quot;;\n \nif (!empty($_POST[&#039;new_password&#039;])) {\n    $new_password = password_hash($_POST[&#039;new_password&#039;], PASSWORD_BCRYPT);\n    $update_sql .= &quot;, password = ?&quot;;\n    $params[] = $new_password;\n    $types .= &quot;s&quot;;\n}\n \n$update_sql .= &quot; WHERE username = ?&quot;;\n$params[] = $username;\n$types .= &quot;s&quot;;\n \n$stmt = $conn-&gt;prepare($update_sql);\n$stmt-&gt;bind_param($types, ...$params);\n \nif ($stmt-&gt;execute()) {\n    header(&quot;Location: profile.php&quot;);\n} else {\n    echo &quot;Error updating profile: &quot; . $stmt-&gt;error;\n}\n \n$stmt-&gt;close();\n$conn-&gt;close();\n?&gt;\nLogout script\nlogout.php\n&lt;?php\nsession_start();\nsession_destroy();\nheader(&quot;Location: signin.html&quot;);\nexit();\n?&gt;\nDeployment Steps:\n\nInstall all these files in /var/www/html/ directory\nSet proper file permissions: sudo chmod 755 /var/www/html/*\nRestart Apache: sudo systemctl restart httpd\n\nSecurity Notes:\n\nReplace your_mysql_password with a strong password\nConfigure firewall to allow HTTP/HTTPS traffic\nUse HTTPS in production\nImplement additional input validation\n\nRequirements Covered:\n\nMySQL installed on EC2\nSimple registration and login system\nProfile update functionality\nBasic error handling\nPassword hashing for security\n\nKubernetifying this app\nStep 1: Create dockerimage\nFROM php:7.4-apache\n \n# Install system dependencies and PHP extensions\nRUN docker-php-ext-install mysqli pdo pdo_mysql\n \n# Copy application files\nCOPY . /var/www/html/\n \n# Set proper permissions\nRUN chown -R www-data:www-data /var/www/html \\\n    &amp;&amp; chmod -R 755 /var/www/html\n \n# Enable Apache mod_rewrite\nRUN a2enmod rewrite\n \n# Expose port 80\nEXPOSE 80\n \ndocker build -t your-dockerhub-username/php-user-registration:v1 .\ndocker push your-dockerhub-username/php-user-registration:v1"},"Drawings/CVCS-Diagram-Excalidraw":{"title":"CVCS-Diagram-Excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nRemote Repo\nServer \nWorking Copy \nWorking Copy \npush \npull \nPull \nPush "},"Drawings/DVCS-Diagram-Excalidraw":{"title":"DVCS-Diagram-Excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nDVCS\nDecentralized Version Control System \nServers \nWorking Copy \nWorking Copy \nRemote Servers \nWorking Copy \nRemote Servers \nfetch \nPush \npush \nfetch \npull \npull "},"Drawings/Docker-Architecture-diagram_1.excalidraw":{"title":"Docker-Architecture-diagram_1.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nHW \nOS (windows) \nVirtulization (VW/VB) \nwhatever comes above the os layer is application or software \nLinux \nVirtualization in our PCs \nLinux \nHW \nVirtualizer/ Hypervisor \nType 1\nHypervisor \nType 2\nHypervisor \nLinux \nLinux \nLinux \nHW \nOS \nDocker \ncontainer \ncontainer \ncontainer \nDocker \nVirtualization in Company "},"Drawings/Drawing-2025-01-10-09.07.03.excalidraw":{"title":"Drawing 2025-01-10 09.07.03.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements"},"Drawings/Manifest_structure.excalidraw":{"title":"Manifest_structure.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nPods \nPods \nPods \nReplicaSet \nDeployment \nDeployment \nReplicaSet \nPods "},"Drawings/cluster-diagram.excalidraw":{"title":"cluster-diagram.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\ncontrol plane \ncloud-controller-manager \netcd \nkube-api-server \nscheduler \ncontroller-manager \nkubelet \nkube-proxy \npod \npod \npod \nNode 1 \nCRI \nkubelet \nkube-proxy \npod \npod \npod \nNode 1 \nCRI "},"Drawings/docker-sock-file.excalidraw":{"title":"docker-sock-file.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\ndocker.sock \ncontainer \nDocker Daemon "},"Drawings/kubernetes-cluster.excalidraw":{"title":"kubernetes-cluster.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nMaster Node \nkube-api-server \nkube-scheduler \nkube-controller-manager \netcd \nkubelet \nkube-proxy \ncontainer-runtime \nkubelet \nkube-proxy \ncontainer-runtime \nworker node \nworker node "},"Jenkins/Jenkins-First-Look":{"title":"Jenkins First Look","links":[],"tags":[],"content":"What Is Jenkins?\n\nJenkins is an open-source server that is written entirely in Java. It lets you execute a series of actions to achieve the continuous integration process, that too in an automated fashion.\nThis CI server runs in servlet containers such as Apache Tomcat. Jenkins facilitates continuous integration and continuous delivery in software projects by automating parts related to build, test, and deployment. This makes it easy for developers to continuously work on the betterment of the product by integrating changes to the project.\nJenkins automates the software builds in a continuous manner and lets the developers know about the errors at an early stage. A strong Jenkins community is one of the prime reasons for its popularity. Jenkins is not only extensible but also has a thriving plugin ecosystem.\nSome of the possible steps that can be performed using Jenkins are:\n\nSoftware build using build systems such as Gradle, Maven, and more.\nAutomation testing using test frameworks such as Nose2, PyTest, Robot, Selenium, and more.\nExecute test scripts (using Windows terminal, Linux shell, etc.\nAchieve test results and perform post actions such as printing test reports, and more.\nExecute test scenarios against different input combinations for obtaining improved test coverage.\nContinuous Integration (CI) where the artifacts are automatically created and tested. This aids in identification of issues in the product at an early stage of development.\n\nAt the time of what is Jenkins blog, it had close to 1500+ plugins contributed by the community. Plugins help in customizing the experience with Jenkins, along with providing support for accelerating activities related to building, deploying, and automating a project.\nHistory Of Jenkins\nJenkins has an early mover advantage since it has been in development since 2011. Kohsuke Kawaguchi created Jenkins (then called ‘Hudson’) while working at Sun Microsystems. Hudson was created in the summer of 2004 and the first release was in February 2005.\nAfter the acquisition of Sun Microsystems by Oracle, a proposal was approved by the Hudson community for creating the Jenkins project. In February 2011, Oracle intended that the development of Hudson should continue hence, Hudson was forked instead of renaming it to Jenkins.\nThough Hudson and Jenkins were being developed independently, Jenkins acquired significantly more projects &amp; contributors than Hudson. Consequently, Hudson is no longer maintained by the community.\nJenkins Release Cycle\nLike other open-source projects, Jenkins also produces two release lines – LTS (Long-Term Support) and Weekly (regular) releases. Jenkins is very good with releases, as stable releases happen every four weeks.\n1. Easy Installation &amp; Configuration\nJenkins is a self-contained Java program that is agnostic of the platform on which it is installed. It is available for almost all the popular operating systems such as Windows, different flavors of Unix, and Mac OS.\nIt is available as a normal installer, as well as a .war file. Once installed, it is easy to configure using its web interface.\n2. Open-Source\nAs it is open-source, it is free for use. There is a strong involvement of the community which makes it a powerful CI/CD tool. You can take support from the Jenkins community, whether it is for extensibility, support, documentation, or any other feature related to Jenkins.\n3. Thriving Plugin Ecosystem\nThe backbone of Jenkins is the community and the community members have been instrumental in the development (and testing) of close to 1500+ plugins available in the Update Center.\n4. Easy Distribution\nJenkins is designed in such a manner that makes it relatively easy to distribute work across multiple machines and platforms for the accelerated build, testing, and deployment.\nHow Does Jenkins Work?\nIn this section of the What is Jenkins blog, we look at the internal functioning of Jenkins i.e. what happens once the developer commits changes to the repository and how CI/CD is realized in Jenkins. We also look at the Master-Agent architecture in Jenkins.\nArchitecture Of Jenkins\nBefore we dive into how does Jenkins works, we must understand the architecture of Jenkins. These are the series of steps that outlines the interaction between different elements in Jenkins:\n\nDevelopers do the necessary modifications in the source code and commit the changes to the repository. A new version of that file will be created in the version control system that is used for maintaining the repository of source code.\nThe repository is continuously checked by the Jenkins CI server for any changes (either in the form of code or libraries) and changes are pulled by the server.\nIn the next step, we ensure that the build with the ‘pulled changes’ is going through or not. The Build server performs a build with the code and an executable is generated if the build process is successful. In case of a build failure, an automated email with a link to build logs and other build artifacts is sent to the developer.\nIn case of a successful build, the built application (or executable) is deployed to the test server. This step helps in realizing continuous testing where the newly built executable goes through a series of automated tests. Developers are alerted in case the changes have caused any breakage in functionality.\nIf there are no build, integration, and testing issues with the checked-in code, the changes and tested application are automatically deployed to the Prod/Production server.\n\nHere is the diagrammatic representation of the Jenkins architecture:\n\nA single Jenkins server might not be sufficient to realize the following requirements:\n\nTesting needs to be performed on different environments (i.e. code written using different languages e.g. Java, Python, C, etc. are committed to the version control system), where a single server might not suffice the requirement.\nA single Jenkins server might not be sufficient to handle the load that comes with large-scale software projects.\n\nIn such scenarios, the distributed (or Master-Agent) architecture of Jenkins is used for continuous integration and testing. Diving deeper into how does Jenkins works, we take a look at the architecture of Jenkins.\nMaster- Agent Architecture In Jenkins\nThe master-agent (or distributed) architecture in Jenkins is used for managing distributed builds. The Master and Agent(s) communicate through the TCP/IP protocol.\nThese are the roles and responsibilities of the Jenkins Master and Agent(s):\nJenkins Master\nThe main server in Jenkins is the Master. Here are the jobs handled by Jenkins Master:\n\nSchedule build jobs\nChoosing the appropriate agent in the master-agent ecosystem for dispatching the builds.\nMonitor agents and take them online/offline as and when required.\nPresenting the build results (and reports) to the developer.\n\nThe Jenkins master can also execute the jobs directly but it is always recommended to select the appropriate agent(s) for build and execution-related tasks.\nJenkins Agent(s)\nA agent is a remote machine that is connected to the Master. Depending on the project and build requirements, you could opt for ‘N’ number of agents. agents can run on different operating systems and depending on the ‘type of build request’, the appropriate Agent is chosen by the Master for build execution and testing.\nHere are the jobs handled by the Jenkins Agent(s):\n\nListen to commands from the Jenkins Master.\nExecute build jobs that are dispatched by the Master.\nDevelopers have the flexibility to run the build and execute tests on a particular agent or a particular type of Agent. The default option is Jenkins Master selecting the best-suited Agent for the job.\n\nHere is a simple diagrammatic representation of how does Jenkins work, with multiple Jenkins Agents connected to the Jenkins Master:\n\nHow Does Jenkins Work In Master-Agent Architecture?\nIn the previous section of the What is Jenkins blog, we touched upon the brief responsibilities of Master and Agent(s) in Jenkins. Let’s look at exactly how does Jenkins works in the Master-Agent (or distributed) architecture:\nIn the Jenkins Master-Agent architecture shown below, there are three Agents, each running on a different operating system (i.e. Windows 10, Linux, and Mac OS).\n\n\nDevelopers check-in their respective code changes in ‘The Remote Source Code Repository’ that is depicted on the left-hand side.\nOnly the Jenkins master is connected to the repository and it checks for code-changes (in the repository) at periodic intervals. All the Jenkins Agents are connected to the Jenkins Master.\nJenkins master dispatches the request (for build and test) to the appropriate Jenkins Agent depending on the environment required for performing the build. This lets you perform builds and execute tests in different environments across the entire architecture.\nThe Agent performs the testing, generates test reports, and sends the same to the Jenkins Master for monitoring.\n\nAs developers keep pushing code, Jenkins Agents can run different builds versions of the code for different platforms. Jenkins Master (or Master Node) controls how the respective builds should operate.\nIn subsequent sections of the What is Jenkins blog, we would look at the steps for setting up Jenkins Master and Agents."},"Jenkins/SonarQube-with-maven":{"title":"SonarQube with maven","links":[],"tags":[],"content":"\nSonar provides a comprehensive code quality and security analysis solution to scan your IaC files in your managed cloud environments to review a wide range of possible issues or security vulnerabilities.\nIntroduction\nSonarQube is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages.\n\nIt can report duplicated code, coding standards, unit tests, code coverage, code complexity and comments.\n\n\nThe only prerequisite for running SonarQube is to have Java (Oracle JRE 11 or OpenJDK 11) installed on your machine.\n\nInstallation\nUsing docker run sonarqube container\ndocker run -d --name sonarqube -p 9000:9000 sonarqube\n\nIn your web browser, connect to http://localhost:9000 to access the SonarQube web interface.\nUse the following default credentials to log in:\nUsername: admin  \nPassword: admin\n\nYou’ll probably prompted to change the default password\n\n\nTo run sonar scan against our locally hosted code select manually option\n\n\nEnter the token name and click the “Generate” button. You will get the sonar token.\n\n\nSave that token and Click “Continue”.\n\n\nSelect Maven and copy the code\n\n\nand open terminal and navigate to your locally hosted code’s directory\nand run that command\nmvn clean verify sonar:sonar \\\n  -Dsonar.projectKey=student \\\n  -Dsonar.projectName=&#039;student&#039; \\\n  -Dsonar.host.url=http://3.137.177.165:32768 \\\n  -Dsonar.token=sqp_3584391716cf8fbc01eeea4e17d34b35b07d5a2f\nand then check sonarqube dashboard for code analysis report."},"Linux/Top-40-Linux-Commands":{"title":"Top 40 Linux Commands","links":[],"tags":[],"content":"Navigation Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription1cdChange directory2pwdPrint working directory3lsList directory contents4cd ..Change to parent directory5cd ~Go to home directory\nFile Management Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription6mkdirMake directory7rmdirRemove directory8cpCopy files or directories9mvMove or rename files or directories10rmRemove files or directories\nFile Inspection Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription11catConcatenate and print files12headPrint the first few lines of a file13tailPrint the last few lines of a file14lessView files page by page15moreView files one page at a time\nText Processing Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription16grepSearch for patterns in files17sedStream editor for text manipulation18awkText processing language for pattern scanning and processing19cutCut out selected columns from a file20pasteMerge lines of files\nSystem Information and Process Management Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription21psProcess list22topDynamic real-time view of system processes23killTerminate a running process24freeDisplay information about system memory usage25dfDisplay disk usage information\nUser management Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription26useraddCreate a new user account27usermodModify an existing user account28userdelDelete a user account29passwdSet or change a user’s password30idDisplay user and group information31groupsList the groups a user belongs to32suSwitch user33sudoExecute commands with superuser privileges34chageModify password aging parameters for a user35pwManage user passwords and shadow files (specific to some distributions)\nFile Permission and Ownership Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.CommandDescription36chmodChange file permissions37chownChange file ownership38chgrpChange file group ownership39umaskSet default file permissions for newly created files and directories40chrootChange the root directory of a process\nAdditional Usecase Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandDescriptionmanDisplay online manual pagesfindSearch for files in a directory hierarchytarArchive and compress filesunzipUnzip compressed fileswgetDownload files from the internetsshSecure Shell for remote loginscpSecure Copy for copying files over SSHtopDisplay system processeshtopInteractive process viewer"},"Linux/Understanding-Vim":{"title":"Understanding Vim","links":[],"tags":[],"content":"Vim Keys Navigation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkMoves cursor upjMoves cursor downhMoves cursor leftlMoves cursor right\nBasic Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:qExit from the file:wqSave and exit:q!Exit without saving:$Move cursor to end of file:wSave changes:set numberTo show line numbers:set no numberHide line numbers:%s/wordabc/wordxyzsubstitute wordabc with worxyz for all occurances\nVim insert modes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naInsert cursor at right of current characterAInsert at the end of lineoInsert mode at new line belowOInsert mode at new line aboverTo change the single characterRStart Replacing everything\nVim Keybinds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyyYank the line (copy)DdDelete the lineDwDelete the wordYwCopy the wordPPaste the copied contentUUndoCtrl + rRedoGgMove the cursor to start of fileGMove the cursor to end of file\nVim Modes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niInsert modeEscEscape mode:Command mode"},"Terraform/EC2-Creation":{"title":"EC2 Creation","links":[],"tags":["terraform","EC2"],"content":"Terraform is IAC i.e. Infrastructure as Code which leverages code to create Infrastructure resource in any of the cloud provider that support terraform.\nWe create two files main.tf and provider.tf\nin main.tf we specify the resources with their configuration to create and in provider.tf we declare information about the cloud provider at we are creating resources.\nHere is the simple Terraform code to create aws_instance resource in aws platform.\nFile Name: main.tf\nresource &quot;aws_instance&quot; &quot;a&quot; {\n  ami           = &quot;ami-0c614dee691cbbf37&quot;\n  instance_type = &quot;t2.micro&quot;\n  key_name = &quot;my-key&quot;\n \n  tags = {\n    Name = &quot;HelloWorld&quot;\n  }\n}\nFile Name: provider.tf\nterraform {\n  required_providers {\n    aws = {\n      source = &quot;hashicorp/aws&quot;\n      version = &quot;5.80.0&quot;\n    }\n  }\n}\n \nprovider &quot;aws&quot; {\n  region = &quot;us-east-1&quot;\n}\nTo apply the execution plan we run\nterraform plan This shows us detailed view of what changes going to take place.\nand with terraform apply we make these changes."},"Terraform/HCL-Basics":{"title":"HCL Basics","links":[],"tags":[],"content":"HCL Syntax:\n\nconsists of a block and argument\n\n&lt;block&gt; &lt;parameter&gt; {\n\tkey1 = value1\n\tkey2 = value2\n}\n\nA block is defined within curly braces and it contains a set of arguments in key value pair format representing the configuration detail.\nA block in terraforms contains information about infrastructure platform and set of resources withing that platform we want to create.\nFor example to create a local file we will use local_file resource and we’ll create local.tf file cotaining following configuration:\nresource &quot;local_file&quot; &quot;pet&quot; {\n\tfilename = &quot;/root/pets.txt&quot;\n\tcontent = &quot;We love pets!&quot;\n}\n\n\nHere is resource is a block name (keyword) which shows the type of block we are using.\nafter the resource declaration we have “resource type” called local_file its fixed value.\nHere “local” is a provider and “file” follwing the underscore _ shows the type of resource which is file\nNext is a Resource name pet which can be anything.\nNext we have Arguments specific to the resource type in a key value pair format. In this case for local_file we have filename and content which are self explanatory."},"Terraform/S3-bucket-Creation":{"title":"S3 bucket Creation","links":[],"tags":["S3","terraform"],"content":"There are two methods to do this:\n\nManually giving buckets names for each buckets\nUsing Increment in Variable to add numbers after specific Unique name\n\nMethod 1 : Manually Adding Bucket Names\nFilename: Main.tf\nresource &quot;aws_s3_bucket&quot; &quot;my-known-buckets&quot; {\n  count     = length(var.bucket_names) \n  bucket    = var.bucket_names[count.index]\n}\nFilename: vars.tf\nvariable &quot;bucket_names&quot; {\n  description = &quot;List of S3 bucket names&quot;\n  type        = list(string)\n  default     = [&quot;chandratsdfsdasfdfaara&quot;, &quot;nayantara123qq3112&quot;, &quot;rocketshopzxys&quot;, &quot;123asdxzsdgheoi&quot;, &quot;asdf21alkj44dsfj&quot;]\n}\nFilename: provider.tf\nterraform {\n  required_providers {\n    aws = {\n      source = &quot;hashicorp/aws&quot;\n      version = &quot;5.80.0&quot;\n    }\n  }\n}\n \nprovider &quot;aws&quot; {\n  # Configuration options\n  region = &quot;us-east-1&quot;\n}\nThis will create 5 buckets with the given names [&quot;chandratsdfsdasfdfaara&quot;, &quot;nayantara123qq3112&quot;, &quot;rocketshopzxys&quot;, &quot;123asdxzsdgheoi&quot;, &quot;asdf21alkj44dsfj&quot;]\nApply it using\nterraform plan\nterraform apply\nYou can list s3 buckets with aws cli commands\naws s3 ls\nMethod 2: Using Increment in Variable to add numbers after specific Unique name\nUse this method when you have too many buckets to create and naming doesnt really matters\nthen this will save your time as you only need to change bucket count in vars.tf file\nFilename: main.tf\nresource &quot;aws_s3_bucket&quot; &quot;c&quot; {\n  count     = var.bucket_count\n  bucket    = &quot;wingardiumleviosamybucketsintheair-${count.index + 1}&quot;\n}\nFilename: vars.tf\nvariable &quot;bucket_count&quot; {\n\tdescription = &quot;number of s3 buckets to create&quot;\n\ttype = number\n\tdefault = 5\n}"},"Terraform/Terraform-Commands":{"title":"Terraform Commands","links":[],"tags":[],"content":"Main Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandDescriptionterraform initTo initialize the Projectterraform validateCheck whether the configuration is validterraform planShows which changes will takes place after applyterraform applyIncorporate changes/ Update or create infrastructureterraform destroyDestroy previously created infrastructure\nOther Commands\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommandDescriptionterraform showShow the current state or a saved planterraform testExecute integration tests for Terraform modulesterraform fmtReformat your configuration in the standard styleterraform providersShow the providers required for this configuration"},"Terraform/VPC-Creation":{"title":"VPC Creation","links":[],"tags":[],"content":"element()\nlength()\nmain.tf\n# vpc\n\nresource &quot;aws_vpc&quot; &quot;main&quot; {\n  cidr_block       = var.vpc_cidr\n\n  tags = {\n    Name = &quot;${var.tag}-vpc&quot;\n  }\n}\n\n# public subnet\n\nresource &quot;aws_subnet&quot; &quot;main&quot; {\n  count = length(var.public_subnet_cidr)\n  vpc_id     = aws_vpc.main.id\n  cidr_block = element( var.public_subnet_cidr, count.index )\n  tags = {\n    Name = &quot;${var.tag}-public-subnet-${element( var.public_subnet_cidr, count.index )}&quot;\n  }\n}\n\n## private subnet\nresource &quot;aws_subnet&quot; &quot;private&quot; {\n    count = length(var.private_subnet_cidr)\n  vpc_id     = aws_vpc.main.id\n  cidr_block = element(var.private_subnet_cidr, count.index)\n\n  tags = {\n    Name = &quot;${var.tag}-private-subnet-${element(var.private_subnet_cidr, count.index)}&quot;\n  }\n}\n\n# public rt\n\nresource &quot;aws_route_table&quot; &quot;main&quot; {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = &quot;0.0.0.0/0&quot;\n    gateway_id = aws_internet_gateway.gw.id\n  }\n\n\n  tags = {\n    Name = &quot;${var.tag}-public-route&quot;\n  }\n}\n\n\n# private rt\n\nresource &quot;aws_route_table&quot; &quot;private&quot; {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = &quot;0.0.0.0/0&quot;\n    gateway_id = aws_nat_gateway.nat.id\n  }\n\n\n  tags = {\n    Name = &quot;${var.tag}-private-route&quot;\n  }\n}\n# eip\nresource &quot;aws_eip&quot; &quot;nat&quot; {\n  domain   = &quot;vpc&quot;\n} \n\n#nat \nresource &quot;aws_nat_gateway&quot; &quot;nat&quot; {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = aws_subnet.main[0].id\n\n  tags = {\n    Name = &quot;${var.tag}-nat&quot;\n  }\n\n  # To ensure proper ordering, it is recommended to add an explicit dependency\n  # on the Internet Gateway for the VPC.\n  depends_on = [aws_internet_gateway.gw]\n}\n\n# igw \nresource &quot;aws_internet_gateway&quot; &quot;gw&quot; {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name = &quot;${var.tag}-igw&quot;\n  }\n}\n\n# subnet association\n\nresource &quot;aws_route_table_association&quot; &quot;a&quot; {\n  count = length(var.public_subnet_cidr)\n  subnet_id = aws_subnet.main[count.index].id\n  route_table_id = aws_route_table.main.id\n}\n\n# private subnet association\n\nresource &quot;aws_route_table_association&quot; &quot;b&quot; {\n  count = length(var.private_subnet_cidr)\n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = aws_route_table.private.id\n}\n\nProvider.tf\nterraform {\n  required_providers {\n    aws = {\n      source = &quot;hashicorp/aws&quot;\n      version = &quot;5.85.0&quot;\n    }\n  }\n}\n\nprovider &quot;aws&quot; {\n  # Configuration options\n  region = &quot;us-east-2&quot;\n\n}\n\nvar.tf\nvariable &quot;vpc_cidr&quot; {\n  type = string\n  default = &quot;10.0.0.0/16&quot;\n  description = &quot;vpc cidr &quot;\n}\n\nvariable &quot;tag&quot; {\n  type = string\n  default = &quot;cdec&quot;\n}\n\nvariable &quot;public_subnet_cidr&quot; {\n  default = [&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;]\n}\n\nvariable &quot;private_subnet_cidr&quot; {\n  default = [&quot;10.0.3.0/24&quot;, &quot;10.0.4.0/24&quot;, &quot;10.0.5.0/24&quot;,&quot;10.0.6.0/24&quot;]\n}\n"},"index":{"title":"index","links":["AWS/Notes/aws-cli-cheatsheet","Jenkins/SonarQube-with-maven","Terraform/HCL-Basics","Jenkins/Jenkins-First-Look","Linux/Cloudblitz-Linux-Session.pdf","Linux/Top-40-Linux-Commands","Linux/Understanding-Vim","AWS/Tasks-Done/☑️-Implement-autoscaling-on-memory-utilization","AWS/Tasks-Done/✅-Host-static-website-on-s3-bucket","AWS/Tasks-Done/✅-Create-5-IAM-users-and-5-S3-buckets-and-attach-them-each-other","AWS/Tasks-Done/✅-Implement-Template-with-Scheduled-Autoscaling","AWS/Tasks-Done/✅-Monitoring-Nginx-logs-with-Cloudwatch","AWS/Tasks-Done/☑️-How-to-store-aws-load-balancer-logs-in-s3-bucket","AWS/Tasks-Done/✅🐈‍⬛Host-tomcat-basesd-web-app-called-Student-app-with-RDS-and-ec2","AWS/Tasks-Done/✅-Get-sns-alert-when-any-user-launches-new-instance","AWS/Tasks-Done/✅-Create-notification-for-s3-bucket-activity-happen-of-put-and-delete","Agile-Development","Agile-Vs-DevOps","Amazon-Web-Services-In-Plain-English","AWS/For-Reading/Authorized-vs-Authonticated","AWS/For-Reading/AWS-Instance-Types","AWS/For-Reading/Difference-between-Load-balancers","Git-Fetch-vs-Git-pull","AWS/For-Reading/How-DNS-Works","AWS/For-Reading/IAM-Policies","AWS/For-Reading/Load-Balancer","AWS/For-Reading/Managed-policies-and-inline-policies","AWS/For-Reading/Network-Protocols","AWS/For-Reading/OSI-Model","AWS/For-Reading/SDLC---Software-Developement-LifeCycle","Simple-Definitions-To-give-in-Interview","AWS/For-Reading/Some-talks-about-git-in-reddit","AWS/For-Reading/ssl-certificate","AWS/For-Reading/Storage-Classes-in-S3","AWS/For-Reading/Subnetting","AWS/For-Reading/Types-of-Autoscaling","Daily-Notes/2024-11-18-EC2-VPC-and-stuffs","Daily-Notes/2024-11-22-s3-task","Daily-Notes/2024-11-26-Cloudwatch-class","Daily-Notes/2024-11-28-Creating-RDS","Daily-Notes/2024-12-11-AWS-Task","Daily-Notes/2024-12-19-Git-Class","Daily-Notes/2024-12-23-docker-day","Daily-Notes/2024-12-24-docker-basics","Daily-Notes/2024-12-31-Dockerfile-Explained","Daily-Notes/2025-01-02-Multistage-Dockerfile","Daily-Notes/2025-01-07-kubernetes","Daily-Notes/2025-01-09-k8s-manifest-files","Daily-Notes/2025-01-10-Service-Types","Daily-Notes/2025-01-13","Daily-Notes/2025-01-21-Creating-PVC","Task-1-Creating-Tomcat-student-ui-container","Task-2-Create-the-Free-css-template-container","Task-3-Create-Mysql-Container","DevOps/Tasks-Done/Task-4-Create-daemon-service-of-tomcat","DevOps/Tasks-Done/Task-5-Create-dockerfiles","DevOps/Tasks-Done/Task-6-How-to-set-permanent-alias","Daily-Notes/2025-01-22-3-Microservice-Tasks","DevOps/K8S-Tasks/K8s-Task-1","DevOps/K8S-Tasks/3-Microservices-Task","DevOps/K8S-Tasks/Tomcat-on-single-node","DevOps/For-Reading/Monolithic-vs-Microservice-Architecture","DevOps/Notes/Git-Theory-and-Practical","DevOps/For-Reading/Reading-Docker-Inspect-output","DevOps/For-Reading/read-about-docker.sock-file","DevOps/Notes/Kubernetes-Architecture","DevOps/For-Reading/Read-about-each-and-every-component-of-kubernetes","DevOps/For-Reading/K8s-Service-Types","DevOps/For-Reading/k8s-Namespace","DevOps/For-Reading/k8s-Volume","Terraform/EC2-Creation","Terraform/S3-bucket-Creation","Terraform/VPC-Creation"],"tags":["homepage"],"content":"\nMy DevOps Notes\nAuthor: Sarang Wandile\nGithub: srngx\nHello, My name is Sarang, in this place I store my DevOps Notes, assignments and Practical. Its not much but I like to keep track of everything I learned.\nThis Place also acts like a Second brain kinda thing where I can always go back in time and recollect my old thoughts and Ideas. Its not perfect and thats the beautify of it. I like to keep it this way original and imperfect.\nIn this Digital Garden you are always welcome to explore and share your notes too if you want or contribute/give suggestion anytime.\nThese notes are made using Obsidian App - A Markdown Notetaking App and Hosted with Cloudflare page.\n\n\n                  \n                  Recent Notes \n                  \n                \n\n\naws-cli cheatsheet\nSonarQube with maven\nHCL Basics\nJenkins First Look\n\n\n\nLinux\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitleNote LinkPDFCloudblitz Linux Session.pdfCommandsTop 40 Linux CommandsVimUnderstanding Vim\nAWS\n\n\n                  \n                  Tasks to perform \n                  \n                \n\n\n☑️ Implement autoscaling on memory utilization\n✅ Host static website on s3 bucket\n✅ Create 5 IAM users and 5 S3 buckets and attach them each other\n✅ Implement Template with Scheduled Autoscaling\n✅ Monitoring Nginx logs with Cloudwatch\n☑️ How to store aws load balancer logs in s3 bucket\n✅🐈‍⬛Host tomcat basesd web app called Student-app with RDS and ec2\n✅ Get sns alert when any user launches new instance\n✅ Create notification for s3 bucket activity happen of put and delete\n\n\n\n\n\n                  \n                  Task assigned for Reading \n                  \n                \n\n\nAgile Development\nAgile Vs DevOps\nAmazon Web Services In Plain English\nAuthorized vs Authonticated\nAWS Instance Types\nDifference between Load balancers\nGit Fetch vs Git pull\nHow DNS Works\nIAM Policies\nLoad Balancer\nManaged policies and inline policies\nNetwork Protocols\nOSI Model\nSDLC - Software Developement LifeCycle\nSimple Definitions To give in Interview\nSome talks about git in reddit\nssl certificate\nStorage Classes in S3\nSubnetting\nTypes of Autoscaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDateSome Importants from Daily Notes2024-11-182024-11-18 EC2 VPC and stuffs2024-11-222024-11-22 s3 task2024-11-262024-11-26 Cloudwatch class2024-22-282024-11-28 Creating RDS2024-12-112024-12-11 AWS Task\nDevOps | Git, Docker, Kubernetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDateSome Importants from Daily Notes2024-12-192024-12-19 Git Class2024-12-232024-12-23 docker-day2024-12-242024-12-24 docker basics2024-12-312024-12-31 Dockerfile Explained2025-01-022025-01-02 Multistage Dockerfile2025-01-072025-01-07 kubernetes2025-01-092025-01-09 k8s manifest files2025-01-102025-01-10 Service Types2025-01-132025-01-13 Config Map and Secrets2025-01-212025-01-21 Creating PVC\n\n\n                  \n                  Tasks to perform \n                  \n                \n\n\nTask 1 Creating Tomcat student-ui container\nTask 2 Create the Free-css template container\nTask 3 Create Mysql Container\nTask 4 Create daemon service of tomcat\nTask 5 Create dockerfiles\nTask 6 How to set permanent alias\nKubernetes 2025-01-22 3-Microservice Tasks\nK8s Task 1\n3 Microservices Task\nTomcat on single node\n\n\n\n\n\n                  \n                  Task assigned for Reading \n                  \n                \n\n\nAgile Development\nAgile Vs DevOps\nGit Fetch vs Git pull\nMonolithic vs Microservice Architecture\nGit Theory and Practical\nReading Docker Inspect output\nread about docker.sock file\nKubernetes Architecture\nRead about each and every component of kubernetes\nK8s Service Types\nk8s Namespace\nk8s Volume\n\n\n\nDevOps | Jenkins, Terraform\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDateNotes2025-01-28Jenkins First Look2025-02-01EC2 Creation2025-02-02S3 bucket Creation2025-02-03VPC Creation\n"}}